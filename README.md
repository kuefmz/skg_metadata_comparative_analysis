# ğŸ§  SKG Metadata Analysis

This repository supports the study presented in:

> **"Are Scientific Tasks and Methods Consistently Represented across Science Knowledge Graphs?"**  
> Jenifer Tabita Ciuciu-Kiss  
> Accepted at **Sci-K @ ISWC 2025**

We investigate whether category annotations (such as tasks and methods) for AI-related publications are consistently represented across multiple **Scientific Knowledge Graphs (SKGs)**, including:
- **Papers with Code**
- **OpenAlex**
- **OpenAIRE**
- **ORKG**

---

## ğŸ“ Project Structure

.
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ data.json # Automatically created metadata (raw SKG annotations)
â”‚ â”œâ”€â”€ all_data.json # Intermediate merged version
â”‚ â”œâ”€â”€ initial_dataset.json # Cleaned and unified dataset (auto-generated)
â”‚ â””â”€â”€ gold_standard.json # Manually curated annotations (ground truth)
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ get_papers_with_code_categories.py
â”‚ â”œâ”€â”€ get_openalex_categories.py
â”‚ â”œâ”€â”€ get_openaire_categories.py
â”‚ â””â”€â”€ integrate_all_3_souces.py
â”‚
â”œâ”€â”€ get_initial_dataset.ipynb # Creates initial_dataset.json from data.json
â”œâ”€â”€ analysis_initial_golden.ipynb # Analysis comparing initial vs. gold-standard
â”œâ”€â”€ analysis_initial_golden.pdf # Final Sci-K 2025 paper
â””â”€â”€ README.md


---

## âš™ï¸ Pipeline Overview

### ğŸ”¹ Step 1: Extract Raw Annotations

Scripts in `src/` pull annotations from each SKG and populate `data/data.json`:

- `get_papers_with_code_categories.py`  
  â†’ Extracts tasks and methods from the Papers with Code JSON dump.

- `get_openalex_categories.py`  
  â†’ Retrieves concept labels assigned by OpenAlex.

- `get_openaire_categories.py`  
  â†’ Maps papers to OpenAIREâ€™s OECD FOS labels using metadata.

- `integrate_all_3_souces.py`  
  â†’ Combines all three outputs into a single paper-centric structure.

### ğŸ”¹ Step 2: Clean and Aggregate

- Run `get_initial_dataset.ipynb`  
  â†’ Cleans terminology, removes duplicates, and produces `initial_dataset.json`  
  (This is your **automatic baseline dataset**.)

### ğŸ”¹ Step 3: Gold Standard Curation

- `gold_standard.json`  
  â†’ Manually reviewed annotations created to validate the auto-extracted results.

### ğŸ”¹ Step 4: Evaluation & Analysis

- Run `analysis_initial_golden.ipynb`  
  â†’ Compares both datasets to assess precision, recall, agreement, and category overlap.

---

## ğŸ“Š Key Results (from the Paper)

Our comparative analysis revealed:

| SKG         | Precision | Recall | F1-score |
|-------------|-----------|--------|----------|
| PwC         | 0.42      | 0.23   | 0.30     |
| OpenAlex    | 0.33      | 0.35   | 0.34     |
| OpenAIRE    | 0.19      | 0.25   | 0.21     |
| ORKG        | 0.38      | 0.15   | 0.21     |

**Notable findings:**
- **PwC** annotations had the best overall precision.
- **OpenAlex** had the highest recall but suffered from label inflation.
- **Overlap between SKGs is low** â€” only 8 categories appeared in all four SKGs.
- **Terminological inconsistency** (e.g., `"NER"` vs `"Named Entity Recognition"`) and **granularity mismatches** were frequent.

> ğŸŸ  These results highlight that existing SKGs diverge significantly in how they annotate scientific tasks/methods, raising challenges for automated classification and semantic interoperability.

---

## ğŸ“ Dataset Description

Each paper entry includes:

```json
{
  "title": "...",
  "doi": "...",
  "abstract": "...",
  "papers_with_code_categories_flat": [...],
  "openalex_categories_flat": [...],
  "openaire_categories_flat": [...],
  "orkg_categories_flat": [...]
}
```


## ğŸ“„ License

This project is licensed under the Apache License 2.0.
You may obtain a copy of the license at:

http://www.apache.org/licenses/

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an â€œAS ISâ€ BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.