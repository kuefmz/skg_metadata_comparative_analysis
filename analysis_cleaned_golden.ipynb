{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "876f7880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9d16e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/all_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "with open(\"data/cleaned_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cleaned_data = json.load(f)\n",
    "\n",
    "with open(\"data/gold_standard.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    golden_standard = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e812d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1fc05c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#papers with OpenAlex cat: 120\n",
      "#papers with OpenAIRE cat: 98\n",
      "#papers with PwC cat: 92\n",
      "#papers with ORKG cat: 93\n",
      "#papers with missing OpenAlex cat: 4.76\n",
      "#papers with missing OpenAIRE cat: 22.22\n",
      "#papers with missing PwC cat: 26.98\n",
      "#papers with missing ORKG cat: 26.19\n"
     ]
    }
   ],
   "source": [
    "has_pwc_cat = 0\n",
    "has_orkg_cat = 0\n",
    "has_openalex_cat = 0\n",
    "has_openaire_cat = 0\n",
    "for paper in all_data:\n",
    "    if paper['openaire_categories_flat']:\n",
    "        has_openaire_cat += 1\n",
    "    if paper['openalex_categories_flat']:\n",
    "        has_openalex_cat += 1\n",
    "    if paper['papers_with_code_categories_flat']:\n",
    "        has_pwc_cat += 1\n",
    "    if paper['orkg_categories_flat']:\n",
    "        has_orkg_cat += 1\n",
    "\n",
    "all_papercount = len(all_data)\n",
    "print(f\"#papers with OpenAlex cat: {has_openalex_cat}\")\n",
    "print(f\"#papers with OpenAIRE cat: {has_openaire_cat}\")\n",
    "print(f\"#papers with PwC cat: {has_pwc_cat}\")\n",
    "print(f\"#papers with ORKG cat: {has_orkg_cat}\")\n",
    "print(f\"#papers with missing OpenAlex cat: {round((1-has_openalex_cat/all_papercount)*100, 2)}\")\n",
    "print(f\"#papers with missing OpenAIRE cat: {round((1-has_openaire_cat/all_papercount)*100, 2)}\")\n",
    "print(f\"#papers with missing PwC cat: {round((1-has_pwc_cat/all_papercount)*100, 2)}\")\n",
    "print(f\"#papers with missing ORKG cat: {round((1-has_orkg_cat/all_papercount)*100, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02a523f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg OpenAlex cats cleaned: 12.39\n",
      "avg OpenAIRE cats cleaned: 7.43\n",
      "avg with PwC cats cleaned: 16.73\n",
      "avg ORKG cat cleaned: 2.83\n"
     ]
    }
   ],
   "source": [
    "pwc_cat_cnt = []\n",
    "orkg_cat_cnt = []\n",
    "openalex_cat_cnt = []\n",
    "openaire_cat_cnt = []\n",
    "for paper in cleaned_data:\n",
    "    if paper['openaire_categories_flat']:\n",
    "        openaire_cat_cnt.append(len(paper['openaire_categories_flat']))\n",
    "    if paper['openalex_categories_flat']:\n",
    "        openalex_cat_cnt.append(len(paper['openalex_categories_flat']))\n",
    "    if paper['papers_with_code_categories_flat']:\n",
    "        pwc_cat_cnt.append(len(paper['papers_with_code_categories_flat']))\n",
    "    if paper['orkg_categories_flat']:\n",
    "        orkg_cat_cnt.append(len(paper['orkg_categories_flat']))\n",
    "\n",
    "print(f\"avg OpenAlex cats cleaned: {round(sum(openalex_cat_cnt)/len(openalex_cat_cnt), 2)}\")\n",
    "print(f\"avg OpenAIRE cats cleaned: {round(sum(openaire_cat_cnt)/len(openaire_cat_cnt), 2)}\")\n",
    "print(f\"avg with PwC cats cleaned: {round(sum(pwc_cat_cnt)/len(pwc_cat_cnt), 2)}\")\n",
    "print(f\"avg ORKG cat cleaned: {round(sum(orkg_cat_cnt)/len(orkg_cat_cnt), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a02c8e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg OpenAlex cats golden: 4.84\n",
      "avg OpenAIRE cats golden: 3.67\n",
      "avg with PwC cats golden: 4.66\n",
      "avg ORKG cat golden: 1.93\n"
     ]
    }
   ],
   "source": [
    "pwc_cat_cnt = []\n",
    "orkg_cat_cnt = []\n",
    "openalex_cat_cnt = []\n",
    "openaire_cat_cnt = []\n",
    "for paper in golden_standard:\n",
    "    if paper['openaire_categories_flat']:\n",
    "        openaire_cat_cnt.append(len(paper['openaire_categories_flat']))\n",
    "    if paper['openalex_categories_flat']:\n",
    "        openalex_cat_cnt.append(len(paper['openalex_categories_flat']))\n",
    "    if paper['papers_with_code_categories_flat']:\n",
    "        pwc_cat_cnt.append(len(paper['papers_with_code_categories_flat']))\n",
    "    if paper['orkg_categories_flat']:\n",
    "        orkg_cat_cnt.append(len(paper['orkg_categories_flat']))\n",
    "\n",
    "print(f\"avg OpenAlex cats golden: {round(sum(openalex_cat_cnt)/len(openalex_cat_cnt), 2)}\")\n",
    "print(f\"avg OpenAIRE cats golden: {round(sum(openaire_cat_cnt)/len(openaire_cat_cnt), 2)}\")\n",
    "print(f\"avg with PwC cats golden: {round(sum(pwc_cat_cnt)/len(pwc_cat_cnt), 2)}\")\n",
    "print(f\"avg ORKG cat golden: {round(sum(orkg_cat_cnt)/len(orkg_cat_cnt), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "66d8c6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#all categories cleaned: 728\n",
      "#ORKG categories cleaned: 133\n",
      "#PwC categories cleaned: 198\n",
      "#OpenAlex categories cleaned: 277\n",
      "#OpenAIRE categories cleaned: 157\n"
     ]
    }
   ],
   "source": [
    "categories = set([])\n",
    "orkg_categories = set([])\n",
    "pwc_categories = set([])\n",
    "openalex_categories = set([])\n",
    "openaire_categories = set([])\n",
    "\n",
    "for paper in cleaned_data:\n",
    "    for cat in paper['openaire_categories_flat']:\n",
    "        categories.add(cat)\n",
    "        openaire_categories.add(cat)\n",
    "    for cat in paper['openalex_categories_flat']:\n",
    "        categories.add(cat)\n",
    "        openalex_categories.add(cat)\n",
    "    for cat in paper['papers_with_code_categories_flat']:\n",
    "        categories.add(cat)\n",
    "        pwc_categories.add(cat)\n",
    "    for cat in paper['orkg_categories_flat']:\n",
    "        categories.add(cat)\n",
    "        orkg_categories.add(cat)\n",
    "\n",
    "print(f\"#all categories cleaned: {len(list(categories))}\")\n",
    "print(f\"#ORKG categories cleaned: {len(list(orkg_categories))}\")\n",
    "print(f\"#PwC categories cleaned: {len(list(pwc_categories))}\")\n",
    "print(f\"#OpenAlex categories cleaned: {len(list(openalex_categories))}\")\n",
    "print(f\"#OpenAIRE categories cleaned: {len(list(openaire_categories))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40eaab5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#all categories golden: 300\n",
      "#ORKG categories golden: 75\n",
      "#PwC categories golden: 119\n",
      "#OpenAlex categories golden: 96\n",
      "#OpenAIRE categories golden: 38\n"
     ]
    }
   ],
   "source": [
    "categories = set([])\n",
    "orkg_categories = set([])\n",
    "pwc_categories = set([])\n",
    "openalex_categories = set([])\n",
    "openaire_categories = set([])\n",
    "\n",
    "for paper in golden_standard:\n",
    "    for cat in paper['openaire_categories_flat']:\n",
    "        categories.add(cat)\n",
    "        openaire_categories.add(cat)\n",
    "    for cat in paper['openalex_categories_flat']:\n",
    "        categories.add(cat)\n",
    "        openalex_categories.add(cat)\n",
    "    for cat in paper['papers_with_code_categories_flat']:\n",
    "        categories.add(cat)\n",
    "        pwc_categories.add(cat)\n",
    "    for cat in paper['orkg_categories_flat']:\n",
    "        categories.add(cat)\n",
    "        orkg_categories.add(cat)\n",
    "\n",
    "print(f\"#all categories golden: {len(list(categories))}\")\n",
    "print(f\"#ORKG categories golden: {len(list(orkg_categories))}\")\n",
    "print(f\"#PwC categories golden: {len(list(pwc_categories))}\")\n",
    "print(f\"#OpenAlex categories golden: {len(list(openalex_categories))}\")\n",
    "print(f\"#OpenAIRE categories golden: {len(list(openaire_categories))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b6e797a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique categories: 728\n",
      "\n",
      "Agreement levels:\n",
      "Categories appearing in 1 SKGs: 695\n",
      "Categories appearing in 2 SKGs: 29\n",
      "Categories appearing in 3 SKGs: 4\n",
      "Categories appearing in 4 SKGs: 0\n"
     ]
    }
   ],
   "source": [
    "# Mapping of category to SKGs it's found in\n",
    "category_to_skgs = defaultdict(set)\n",
    "\n",
    "for paper in cleaned_data:\n",
    "    skg_cats = {\n",
    "        \"orkg\": set(paper[\"orkg_categories_flat\"]),\n",
    "        \"pwc\": set(paper[\"papers_with_code_categories_flat\"]),\n",
    "        \"openalex\": set(paper[\"openalex_categories_flat\"]),\n",
    "        \"openaire\": set(paper[\"openaire_categories_flat\"]),\n",
    "    }\n",
    "\n",
    "    for skg, cats in skg_cats.items():\n",
    "        for cat in cats:\n",
    "            category_to_skgs[cat].add(skg)\n",
    "\n",
    "# Count how many categories appear in 1, 2, 3, or 4 SKGs\n",
    "agreement_counter = Counter()\n",
    "for cat, skgs in category_to_skgs.items():\n",
    "    agreement_counter[len(skgs)] += 1\n",
    "\n",
    "# Total unique categories\n",
    "total_unique_cats = len(category_to_skgs)\n",
    "\n",
    "print(f\"\\nTotal unique categories: {total_unique_cats}\")\n",
    "print(\"\\nAgreement levels:\")\n",
    "for k in range(1, 5):\n",
    "    print(f\"Categories appearing in {k} SKGs: {agreement_counter[k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bff57506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique categories: 300\n",
      "\n",
      "Agreement levels:\n",
      "Categories appearing in 1 SKGs: 277\n",
      "Categories appearing in 2 SKGs: 18\n",
      "Categories appearing in 3 SKGs: 5\n",
      "Categories appearing in 4 SKGs: 0\n"
     ]
    }
   ],
   "source": [
    "# Mapping of category to SKGs it's found in\n",
    "category_to_skgs = defaultdict(set)\n",
    "\n",
    "for paper in golden_standard:\n",
    "    skg_cats = {\n",
    "        \"orkg\": set(paper[\"orkg_categories_flat\"]),\n",
    "        \"pwc\": set(paper[\"papers_with_code_categories_flat\"]),\n",
    "        \"openalex\": set(paper[\"openalex_categories_flat\"]),\n",
    "        \"openaire\": set(paper[\"openaire_categories_flat\"]),\n",
    "    }\n",
    "\n",
    "    for skg, cats in skg_cats.items():\n",
    "        for cat in cats:\n",
    "            category_to_skgs[cat].add(skg)\n",
    "\n",
    "# Count how many categories appear in 1, 2, 3, or 4 SKGs\n",
    "agreement_counter = Counter()\n",
    "for cat, skgs in category_to_skgs.items():\n",
    "    agreement_counter[len(skgs)] += 1\n",
    "\n",
    "# Total unique categories\n",
    "total_unique_cats = len(category_to_skgs)\n",
    "\n",
    "print(f\"\\nTotal unique categories: {total_unique_cats}\")\n",
    "print(\"\\nAgreement levels:\")\n",
    "for k in range(1, 5):\n",
    "    print(f\"Categories appearing in {k} SKGs: {agreement_counter[k]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de5b8d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers with at least one overlapping category in:\n",
      "- 2 SKGs: 43\n",
      "- 3 SKGs: 1\n",
      "- 4 SKGs: 0\n"
     ]
    }
   ],
   "source": [
    "paper_overlap_counter = Counter()\n",
    "\n",
    "for paper in cleaned_data:\n",
    "    skg_cats = {\n",
    "        \"orkg\": set(paper[\"orkg_categories_flat\"]),\n",
    "        \"pwc\": set(paper[\"papers_with_code_categories_flat\"]),\n",
    "        \"openalex\": set(paper[\"openalex_categories_flat\"]),\n",
    "        \"openaire\": set(paper[\"openaire_categories_flat\"]),\n",
    "    }\n",
    "\n",
    "    # Build reverse map: category → list of SKGs\n",
    "    category_skg_map = {}\n",
    "    for skg, cats in skg_cats.items():\n",
    "        for cat in cats:\n",
    "            category_skg_map.setdefault(cat, set()).add(skg)\n",
    "\n",
    "    # Count how many categories appear in how many SKGs\n",
    "    overlap_levels = Counter()\n",
    "    for skgs in category_skg_map.values():\n",
    "        overlap_levels[len(skgs)] += 1\n",
    "\n",
    "    # Add to overall paper-level count\n",
    "    for level in [2, 3, 4]:\n",
    "        if overlap_levels[level] > 0:\n",
    "            paper_overlap_counter[level] += 1\n",
    "\n",
    "# Print results\n",
    "print(\"Number of papers with at least one overlapping category in:\")\n",
    "print(f\"- 2 SKGs: {paper_overlap_counter[2]}\")\n",
    "print(f\"- 3 SKGs: {paper_overlap_counter[3]}\")\n",
    "print(f\"- 4 SKGs: {paper_overlap_counter[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb9c3ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers with at least one overlapping category in:\n",
      "- 2 SKGs: 65\n",
      "- 3 SKGs: 2\n",
      "- 4 SKGs: 0\n"
     ]
    }
   ],
   "source": [
    "paper_overlap_counter = Counter()\n",
    "\n",
    "for paper in golden_standard:\n",
    "    skg_cats = {\n",
    "        \"orkg\": set(paper[\"orkg_categories_flat\"]),\n",
    "        \"pwc\": set(paper[\"papers_with_code_categories_flat\"]),\n",
    "        \"openalex\": set(paper[\"openalex_categories_flat\"]),\n",
    "        \"openaire\": set(paper[\"openaire_categories_flat\"]),\n",
    "    }\n",
    "\n",
    "    # Build reverse map: category → list of SKGs\n",
    "    category_skg_map = {}\n",
    "    for skg, cats in skg_cats.items():\n",
    "        for cat in cats:\n",
    "            category_skg_map.setdefault(cat, set()).add(skg)\n",
    "\n",
    "    # Count how many categories appear in how many SKGs\n",
    "    overlap_levels = Counter()\n",
    "    for skgs in category_skg_map.values():\n",
    "        overlap_levels[len(skgs)] += 1\n",
    "\n",
    "    # Add to overall paper-level count\n",
    "    for level in [2, 3, 4]:\n",
    "        if overlap_levels[level] > 0:\n",
    "            paper_overlap_counter[level] += 1\n",
    "\n",
    "# Print results\n",
    "print(\"Number of papers with at least one overlapping category in:\")\n",
    "print(f\"- 2 SKGs: {paper_overlap_counter[2]}\")\n",
    "print(f\"- 3 SKGs: {paper_overlap_counter[3]}\")\n",
    "print(f\"- 4 SKGs: {paper_overlap_counter[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "028e0011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Papers with at least one category in 2 SKGs (43 papers):\n",
      "- MiniCPM: Unveiling the Potential of Small Language Models with Scalable\n",
      "  Training Strategies\n",
      "- Enhancing text-based knowledge graph completion with zero-shot large language models: A focus on semantic enhancement\n",
      "- COCONut: Modernizing COCO Segmentation\n",
      "- Annotation Errors and NER: A Study with OntoNotes 5.0\n",
      "- Understanding and Tackling Label Errors in Individual-Level Nature\n",
      "  Language Understanding\n",
      "- Human Evaluation of Procedural Knowledge Graph Extraction from Text with Large Language Models\n",
      "- TinyLlama: An Open-Source Small Language Model\n",
      "- Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives\n",
      "- Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks\n",
      "- The Power of Noise: Redefining Retrieval for RAG Systems\n",
      "- Retrieval meets Long Context Large Language Models\n",
      "- Corrective Retrieval Augmented Generation\n",
      "- UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems\n",
      "- FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction\n",
      "- Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning\n",
      "- G-Retriever: Retrieval-Augmented Generation for Textual Graph\n",
      "  Understanding and Question Answering\n",
      "- Generating Benchmarks for Factuality Evaluation of Language Models\n",
      "- PURPLE: Making a Large Language Model a Better SQL Writer\n",
      "- ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\n",
      "- Automating psychological hypothesis generation with AI: when large language models meet causal graph\n",
      "- Compact Language Models via Pruning and Knowledge Distillation\n",
      "- CYCLE: Learning to Self-Refine the Code Generation\n",
      "- Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving\n",
      "- GWQ: Gradient-Aware Weight Quantization for Large Language Models\n",
      "- SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers\n",
      "- Artificial intelligence for literature reviews: opportunities and challenges\n",
      "- SciDQA: A Deep Reading Comprehension Dataset over Scientific Papers\n",
      "- Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling\n",
      "- UL2: Unifying Language Learning Paradigms\n",
      "- Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models\n",
      "- ChemCrow: Augmenting large-language models with chemistry tools\n",
      "- StarCoder: may the source be with you!\n",
      "- WizardLM: Empowering Large Language Models to Follow Complex Instructions\n",
      "- WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct\n",
      "- Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models\n",
      "- Orca: Progressive Learning from Complex Explanation Traces of GPT-4\n",
      "- MEGA: Multilingual Evaluation of Generative AI\n",
      "- ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning\n",
      "- M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models\n",
      "- Measuring Massive Multitask Chinese Understanding\n",
      "- Evaluating language models for mathematics through interactions\n",
      "- How well do Large Language Models perform in Arithmetic tasks?\n",
      "- MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models\n",
      "\n",
      "Papers with at least one category in 3 SKGs (1 papers):\n",
      "- Annotation Errors and NER: A Study with OntoNotes 5.0\n",
      "\n",
      "Papers with at least one category in 4 SKGs (0 papers):\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to hold paper titles per overlap level\n",
    "papers_with_overlap = {\n",
    "    2: [],\n",
    "    3: [],\n",
    "    4: []\n",
    "}\n",
    "\n",
    "for paper in cleaned_data:\n",
    "    skg_cats = {\n",
    "        \"orkg\": set(paper[\"orkg_categories_flat\"]),\n",
    "        \"pwc\": set(paper[\"papers_with_code_categories_flat\"]),\n",
    "        \"openalex\": set(paper[\"openalex_categories_flat\"]),\n",
    "        \"openaire\": set(paper[\"openaire_categories_flat\"]),\n",
    "    }\n",
    "\n",
    "    # Build reverse map: category → set of SKGs it appears in\n",
    "    category_skg_map = {}\n",
    "    for skg, cats in skg_cats.items():\n",
    "        for cat in cats:\n",
    "            category_skg_map.setdefault(cat, set()).add(skg)\n",
    "\n",
    "    # Count categories by their SKG overlap level\n",
    "    overlap_levels = {k: 0 for k in [2, 3, 4]}\n",
    "    for skgs in category_skg_map.values():\n",
    "        if 2 <= len(skgs) <= 4:\n",
    "            overlap_levels[len(skgs)] += 1\n",
    "\n",
    "    # Save paper title if there's at least one category for that level\n",
    "    for level in [2, 3, 4]:\n",
    "        if overlap_levels[level] > 0:\n",
    "            papers_with_overlap[level].append(paper[\"title\"])\n",
    "\n",
    "# Print results\n",
    "for level in [2, 3, 4]:\n",
    "    print(f\"\\nPapers with at least one category in {level} SKGs ({len(papers_with_overlap[level])} papers):\")\n",
    "    for title in papers_with_overlap[level]:\n",
    "        print(f\"- {title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "46359d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Papers with at least one category in 2 SKGs (65 papers):\n",
      "- MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies\n",
      "- OLMo: Accelerating the Science of Language Models\n",
      "- Enhancing text-based knowledge graph completion with zero-shot large language models: A focus on semantic enhancement\n",
      "- COCONut: Modernizing COCO Segmentation\n",
      "- Annotation Errors and NER: A Study with OntoNotes 5.0\n",
      "- Understanding and Tackling Label Errors in Individual-Level Nature Language Understanding\n",
      "- Human Evaluation of Procedural Knowledge Graph Extraction from Text with Large Language Models\n",
      "- Structure Guided Large Language Model for SQL Generation\n",
      "- TinyLlama: An Open-Source Small Language Model\n",
      "- Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone\n",
      "- Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\n",
      "- Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives\n",
      "- Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models\n",
      "- Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement\n",
      "- Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks\n",
      "- The Power of Noise: Redefining Retrieval for RAG Systems\n",
      "- Retrieval meets Long Context Large Language Models\n",
      "- RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
      "- Corrective Retrieval Augmented Generation\n",
      "- UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems\n",
      "- FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction\n",
      "- Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning\n",
      "- G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering\n",
      "- Generating Benchmarks for Factuality Evaluation of Language Models\n",
      "- BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models\n",
      "- PURPLE: Making a Large Language Model a Better SQL Writer\n",
      "- Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments\n",
      "- Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM\n",
      "- Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL\n",
      "- Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm\n",
      "- ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\n",
      "- CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution\n",
      "- Gemma 2: Improving Open Language Models at a Practical Size\n",
      "- MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases\n",
      "- Compact Language Models via Pruning and Knowledge Distillation\n",
      "- Self-Refinement of Language Models from External Proxy Metrics Feedback\n",
      "- CYCLE: Learning to Self-Refine the Code Generation\n",
      "- Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving\n",
      "- MobileQuant: Mobile-friendly Quantization for On-device Language Models\n",
      "- GWQ: Gradient-Aware Weight Quantization for Large Language Models\n",
      "- SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers\n",
      "- Artificial intelligence for literature reviews: opportunities and challenges\n",
      "- SciDQA: A Deep Reading Comprehension Dataset over Scientific Papers\n",
      "- Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling\n",
      "- UL2: Unifying Language Learning Paradigms\n",
      "- Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "- Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models\n",
      "- WizardCoder: Empowering Code Large Language Models with Evol-Instruct\n",
      "- StarCoder: may the source be with you!\n",
      "- WizardLM: Empowering Large Language Models to Follow Complex Instructions\n",
      "- WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct\n",
      "- Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models\n",
      "- Orca: Progressive Learning from Complex Explanation Traces of GPT-4\n",
      "- CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?\n",
      "- MEGA: Multilingual Evaluation of Generative AI\n",
      "- ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning\n",
      "- M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models\n",
      "- Measuring Massive Multitask Chinese Understanding\n",
      "- Evaluating language models for mathematics through interactions\n",
      "- How well do Large Language Models perform in Arithmetic tasks?\n",
      "- StructGPT: A General Framework for Large Language Model to Reason over Structured Data\n",
      "- MMBench: Is Your Multi-modal Model an All-Around Player?\n",
      "- MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models\n",
      "- Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation\n",
      "- C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models\n",
      "\n",
      "Papers with at least one category in 3 SKGs (2 papers):\n",
      "- Annotation Errors and NER: A Study with OntoNotes 5.0\n",
      "- SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers\n",
      "\n",
      "Papers with at least one category in 4 SKGs (0 papers):\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to hold paper titles per overlap level\n",
    "papers_with_overlap = {\n",
    "    2: [],\n",
    "    3: [],\n",
    "    4: []\n",
    "}\n",
    "\n",
    "for paper in golden_standard:\n",
    "    skg_cats = {\n",
    "        \"orkg\": set(paper[\"orkg_categories_flat\"]),\n",
    "        \"pwc\": set(paper[\"papers_with_code_categories_flat\"]),\n",
    "        \"openalex\": set(paper[\"openalex_categories_flat\"]),\n",
    "        \"openaire\": set(paper[\"openaire_categories_flat\"]),\n",
    "    }\n",
    "\n",
    "    # Build reverse map: category → set of SKGs it appears in\n",
    "    category_skg_map = {}\n",
    "    for skg, cats in skg_cats.items():\n",
    "        for cat in cats:\n",
    "            category_skg_map.setdefault(cat, set()).add(skg)\n",
    "\n",
    "    # Count categories by their SKG overlap level\n",
    "    overlap_levels = {k: 0 for k in [2, 3, 4]}\n",
    "    for skgs in category_skg_map.values():\n",
    "        if 2 <= len(skgs) <= 4:\n",
    "            overlap_levels[len(skgs)] += 1\n",
    "\n",
    "    # Save paper title if there's at least one category for that level\n",
    "    for level in [2, 3, 4]:\n",
    "        if overlap_levels[level] > 0:\n",
    "            papers_with_overlap[level].append(paper[\"title\"])\n",
    "\n",
    "# Print results\n",
    "for level in [2, 3, 4]:\n",
    "    print(f\"\\nPapers with at least one category in {level} SKGs ({len(papers_with_overlap[level])} papers):\")\n",
    "    for title in papers_with_overlap[level]:\n",
    "        print(f\"- {title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bb5dd83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PWC: Precision=0.27, Recall=0.99, F1-score=0.42\n",
      "OPENALEX: Precision=0.39, Recall=1.0, F1-score=0.56\n",
      "OPENAIRE: Precision=0.35, Recall=0.72, F1-score=0.47\n",
      "ORKG: Precision=0.66, Recall=0.98, F1-score=0.79\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "skg_keys = {\n",
    "    \"pwc\": \"papers_with_code_categories_flat\",\n",
    "    \"openalex\": \"openalex_categories_flat\",\n",
    "    \"openaire\": \"openaire_categories_flat\",\n",
    "    \"orkg\": \"orkg_categories_flat\"\n",
    "}\n",
    "\n",
    "# Compute metrics per SKG\n",
    "results = {}\n",
    "\n",
    "for skg, key in skg_keys.items():\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "\n",
    "    for paper_clean, paper_gold in zip(cleaned_data, golden_standard):\n",
    "        gold_cats = set(cat.lower() for cat in paper_gold.get(key, []))\n",
    "        pred_cats = set(cat.lower() for cat in paper_clean.get(key, []))\n",
    "\n",
    "        all_cats = sorted(gold_cats | pred_cats)\n",
    "        y_true = [1 if c in gold_cats else 0 for c in all_cats]\n",
    "        y_pred = [1 if c in pred_cats else 0 for c in all_cats]\n",
    "\n",
    "        y_true_all.extend(y_true)\n",
    "        y_pred_all.extend(y_pred)\n",
    "\n",
    "    precision = precision_score(y_true_all, y_pred_all, zero_division=0)\n",
    "    recall = recall_score(y_true_all, y_pred_all, zero_division=0)\n",
    "    f1 = f1_score(y_true_all, y_pred_all, zero_division=0)\n",
    "\n",
    "    results[skg] = {\n",
    "        \"precision\": round(precision, 2),\n",
    "        \"recall\": round(recall, 2),\n",
    "        \"f1_score\": round(f1, 2)\n",
    "    }\n",
    "\n",
    "# Output results\n",
    "for skg, metrics in results.items():\n",
    "    print(f\"{skg.upper()}: Precision={metrics['precision']}, Recall={metrics['recall']}, F1-score={metrics['f1_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "32546b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inconsistency counts per SKG:\n",
      "PWC: Coverage=0, Incorrect=54\n",
      "ORKG: Coverage=0, Incorrect=30\n",
      "OPENALEX: Coverage=0, Incorrect=60\n",
      "OPENAIRE: Coverage=0, Incorrect=61\n"
     ]
    }
   ],
   "source": [
    "gold_index = {paper[\"title\"].lower(): paper for paper in golden_standard}\n",
    "\n",
    "# SKGs and inconsistency counters\n",
    "skg_keys = {\n",
    "    \"pwc\": \"papers_with_code_categories_flat\",\n",
    "    \"orkg\": \"orkg_categories_flat\",\n",
    "    \"openalex\": \"openalex_categories_flat\",\n",
    "    \"openaire\": \"openaire_categories_flat\"\n",
    "}\n",
    "\n",
    "inconsistencies = {\n",
    "    skg: {\n",
    "        \"coverage_inconsistency\": 0,\n",
    "        \"incorrect_assignment\": 0,\n",
    "    } for skg in skg_keys\n",
    "}\n",
    "\n",
    "# Go through each paper in cleaned data that is also in gold\n",
    "for paper in cleaned_data:\n",
    "    title = paper[\"title\"].lower()\n",
    "    if title not in gold_index:\n",
    "        continue\n",
    "    gold_paper = gold_index[title]\n",
    "\n",
    "    for skg, skg_field in skg_keys.items():\n",
    "        skg_labels = set(paper[skg_field])\n",
    "        gold_labels = set(gold_paper[skg_field])\n",
    "\n",
    "        # Coverage inconsistency\n",
    "        if len(skg_labels) <= 1 and len(gold_labels) >= 3:\n",
    "            inconsistencies[skg][\"coverage_inconsistency\"] += 1\n",
    "\n",
    "        # Incorrect assignment (labels not in gold)\n",
    "        if len(skg_labels - gold_labels) > 0:\n",
    "            inconsistencies[skg][\"incorrect_assignment\"] += 1\n",
    "\n",
    "# Output results\n",
    "print(\"Inconsistency counts per SKG:\")\n",
    "for skg in skg_keys:\n",
    "    print(f\"{skg.upper()}: Coverage={inconsistencies[skg]['coverage_inconsistency']}, \"\n",
    "          f\"Incorrect={inconsistencies[skg]['incorrect_assignment']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "601a29b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKG - Initial - Gold - Incorrect - AvgIncorrect\n",
      "PWC - 1018 - 243 - 779 - 12.56\n",
      "ORKG - 184 - 123 - 64 - 1.03\n",
      "OPENALEX - 801 - 311 - 490 - 7.90\n",
      "OPENAIRE - 479 - 228 - 317 - 5.11\n"
     ]
    }
   ],
   "source": [
    "gold_index = {paper[\"title\"].lower(): paper for paper in golden_standard}\n",
    "skg_keys = {\n",
    "    \"pwc\": \"papers_with_code_categories_flat\",\n",
    "    \"orkg\": \"orkg_categories_flat\",\n",
    "    \"openalex\": \"openalex_categories_flat\",\n",
    "    \"openaire\": \"openaire_categories_flat\"\n",
    "}\n",
    "\n",
    "extra_counts = defaultdict(int)\n",
    "paper_counts = defaultdict(int)\n",
    "initial_total = defaultdict(int)\n",
    "gold_total = defaultdict(int)\n",
    "\n",
    "for paper in cleaned_data:\n",
    "    title = paper[\"title\"].lower()\n",
    "    if title not in gold_index:\n",
    "        continue\n",
    "    gold_paper = gold_index[title]\n",
    "\n",
    "    for skg, field in skg_keys.items():\n",
    "        cleaned_labels = set(paper[field])\n",
    "        gold_labels = set(gold_paper[field])\n",
    "        extras = cleaned_labels - gold_labels\n",
    "\n",
    "        extra_counts[skg] += len(extras)\n",
    "        paper_counts[skg] += 1\n",
    "        initial_total[skg] += len(cleaned_labels)\n",
    "        gold_total[skg] += len(gold_labels)\n",
    "\n",
    "# Print results\n",
    "print(\"SKG - Initial - Gold - Incorrect - AvgIncorrect\")\n",
    "for skg in skg_keys:\n",
    "    total_init = initial_total[skg]\n",
    "    total_gold = gold_total[skg]\n",
    "    total_extra = extra_counts[skg]\n",
    "    avg_extra = total_extra / paper_counts[skg]\n",
    "    print(f\"{skg.upper()} - {total_init} - {total_gold} - {total_extra} - {avg_extra:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
