{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "328ff3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fbad9046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'MiniCPM: Unveiling the Potential of Small Language Models with Scalable\\n  Training Strategies',\n",
       " 'doi': '10.48550/arxiv.2404.06395',\n",
       " 'abstract': \"The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .\",\n",
       " 'orkg categories': {'domains': ['Generic'],\n",
       "  'methods': [],\n",
       "  'research problems': ['Small Language Models (SLMs) survey'],\n",
       "  'tasks': []},\n",
       " 'papers with code categories': {'tasks': ['Domain Adaptation'],\n",
       "  'methods': ['Chinchilla'],\n",
       "  'main_collection_name': ['Language Models'],\n",
       "  'main_collection_area': ['Natural Language Processing']},\n",
       " 'openalex categories': {'primary topics': ['Topic Modeling'],\n",
       "  'topics': ['Topic Modeling', 'Natural Language Processing Techniques'],\n",
       "  'concepts': ['Training (meteorology)',\n",
       "   'Scalability',\n",
       "   'Computer science',\n",
       "   'Natural language processing',\n",
       "   'Artificial intelligence',\n",
       "   'Geography',\n",
       "   'Database',\n",
       "   'Meteorology']},\n",
       " 'openaire categories': {'subjects': ['FOS: Computer and information sciences',\n",
       "   'Computer Science - Machine Learning',\n",
       "   'Machine Learning (cs.LG)',\n",
       "   'Computation and Language (cs.CL)',\n",
       "   'Computer Science - Computation and Language']},\n",
       " 'crossref categories': {'subjects': []}}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9f7c03fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_fields = [\n",
    "    \"orkg categories\",\n",
    "    \"papers with code categories\",\n",
    "    \"openalex categories\",\n",
    "    \"openaire categories\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07482997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f56ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid papers: 70\n"
     ]
    }
   ],
   "source": [
    "count_all_present = 0\n",
    "valid_papers = []\n",
    "paper_count = 0\n",
    "for paper in data:\n",
    "    paper_count += 1\n",
    "    # print(paper_count)\n",
    "    all_field_count = 0\n",
    "    for field in required_fields:\n",
    "        is_any_subfield_not_empty = False\n",
    "        for subfield_key in paper[field].keys():\n",
    "            if  len(paper[field][subfield_key]) > 0:\n",
    "                is_any_subfield_not_empty = True\n",
    "        if is_any_subfield_not_empty:\n",
    "            all_field_count += 1\n",
    "    if all_field_count >= 4:\n",
    "        count_all_present += 1\n",
    "        valid_papers.append(paper)\n",
    "print(f'Valid papers: {count_all_present}')\n",
    "# for paper in valid_papers:\n",
    "#     print(paper[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c403f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_fields = {\n",
    "    \"orkg categories\": [\"domains\", \"methods\", \"research problems\", \"tasks\"],\n",
    "    \"papers with code categories\": [\"tasks\", \"methods\", \"main_collection_name\", \"main_collection_area\"],\n",
    "    \"openalex categories\": [\"primary topics\", \"topics\", \"concepts\"],\n",
    "    \"openaire categories\": [\"subjects\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2ddeb6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in valid_papers:\n",
    "    entry.pop(\"crossref categories\", None)\n",
    "    for field, subfields in category_fields.items():\n",
    "        flat_list = []\n",
    "        if field in entry and isinstance(entry[field], dict):\n",
    "            for subfield in subfields:\n",
    "                if subfield in entry[field] and isinstance(entry[field][subfield], list):\n",
    "                    cleaned = [item.lower() for item in entry[field][subfield] if isinstance(item, str)]\n",
    "                    entry[field][subfield] = cleaned\n",
    "                    flat_list.extend(cleaned)\n",
    "        flat_key = field.replace(\" \", \"_\") + \"_flat\"\n",
    "        entry[flat_key] = list(set(flat_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "01f6c63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'MiniCPM: Unveiling the Potential of Small Language Models with Scalable\\n  Training Strategies',\n",
       " 'doi': '10.48550/arxiv.2404.06395',\n",
       " 'abstract': \"The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .\",\n",
       " 'orkg categories': {'domains': ['generic'],\n",
       "  'methods': [],\n",
       "  'research problems': ['small language models (slms) survey'],\n",
       "  'tasks': []},\n",
       " 'papers with code categories': {'tasks': ['domain adaptation'],\n",
       "  'methods': ['chinchilla'],\n",
       "  'main_collection_name': ['language models'],\n",
       "  'main_collection_area': ['natural language processing']},\n",
       " 'openalex categories': {'primary topics': ['topic modeling'],\n",
       "  'topics': ['topic modeling', 'natural language processing techniques'],\n",
       "  'concepts': ['training (meteorology)',\n",
       "   'scalability',\n",
       "   'computer science',\n",
       "   'natural language processing',\n",
       "   'artificial intelligence',\n",
       "   'geography',\n",
       "   'database',\n",
       "   'meteorology']},\n",
       " 'openaire categories': {'subjects': ['fos: computer and information sciences',\n",
       "   'computer science - machine learning',\n",
       "   'machine learning (cs.lg)',\n",
       "   'computation and language (cs.cl)',\n",
       "   'computer science - computation and language']},\n",
       " 'orkg_categories_flat': ['small language models (slms) survey', 'generic'],\n",
       " 'papers_with_code_categories_flat': ['domain adaptation',\n",
       "  'chinchilla',\n",
       "  'natural language processing',\n",
       "  'language models'],\n",
       " 'openalex_categories_flat': ['artificial intelligence',\n",
       "  'scalability',\n",
       "  'meteorology',\n",
       "  'topic modeling',\n",
       "  'natural language processing techniques',\n",
       "  'computer science',\n",
       "  'geography',\n",
       "  'database',\n",
       "  'training (meteorology)',\n",
       "  'natural language processing'],\n",
       " 'openaire_categories_flat': ['fos: computer and information sciences',\n",
       "  'computer science - computation and language',\n",
       "  'computation and language (cs.cl)',\n",
       "  'computer science - machine learning',\n",
       "  'machine learning (cs.lg)']}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for entry in data:\n",
    "    entry.pop(\"crossref categories\", None)\n",
    "    for field, subfields in category_fields.items():\n",
    "        flat_list = []\n",
    "        if field in entry and isinstance(entry[field], dict):\n",
    "            for subfield in subfields:\n",
    "                if subfield in entry[field] and isinstance(entry[field][subfield], list):\n",
    "                    cleaned = [item.lower() for item in entry[field][subfield] if isinstance(item, str)]\n",
    "                    entry[field][subfield] = cleaned\n",
    "                    flat_list.extend(cleaned)\n",
    "        flat_key = field.replace(\" \", \"_\") + \"_flat\"\n",
    "        entry[flat_key] = list(set(flat_list))\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895108c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7a459a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "all_data = []\n",
    "\n",
    "for entry in valid_papers:\n",
    "    cleaned_entry = {\n",
    "        \"title\": entry.get(\"title\", \"\"),\n",
    "        \"doi\": entry.get(\"doi\", \"\"),\n",
    "        \"abstract\": entry.get(\"abstract\", \"\"),\n",
    "        \"orkg_categories_flat\": entry.get(\"orkg_categories_flat\", []),\n",
    "        \"papers_with_code_categories_flat\": sorted(entry.get(\"papers_with_code_categories_flat\", [])),\n",
    "        \"openalex_categories_flat\": sorted(entry.get(\"openalex_categories_flat\", [])),\n",
    "        \"openaire_categories_flat\": sorted(entry.get(\"openaire_categories_flat\", []))\n",
    "    }\n",
    "    cleaned_data.append(cleaned_entry)\n",
    "\n",
    "for entry in data:\n",
    "    cleaned_entry = {\n",
    "        \"title\": entry.get(\"title\", \"\"),\n",
    "        \"doi\": entry.get(\"doi\", \"\"),\n",
    "        \"abstract\": entry.get(\"abstract\", \"\"),\n",
    "        \"orkg_categories_flat\": entry.get(\"orkg_categories_flat\", []),\n",
    "        \"papers_with_code_categories_flat\": sorted(entry.get(\"papers_with_code_categories_flat\", [])),\n",
    "        \"openalex_categories_flat\": sorted(entry.get(\"openalex_categories_flat\", [])),\n",
    "        \"openaire_categories_flat\": sorted(entry.get(\"openaire_categories_flat\", []))\n",
    "    }\n",
    "    all_data.append(cleaned_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8c535fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'MiniCPM: Unveiling the Potential of Small Language Models with Scalable\\n  Training Strategies',\n",
       " 'doi': '10.48550/arxiv.2404.06395',\n",
       " 'abstract': \"The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .\",\n",
       " 'orkg_categories_flat': ['small language models (slms) survey', 'generic'],\n",
       " 'papers_with_code_categories_flat': ['chinchilla',\n",
       "  'domain adaptation',\n",
       "  'language models',\n",
       "  'natural language processing'],\n",
       " 'openalex_categories_flat': ['artificial intelligence',\n",
       "  'computer science',\n",
       "  'database',\n",
       "  'geography',\n",
       "  'meteorology',\n",
       "  'natural language processing',\n",
       "  'natural language processing techniques',\n",
       "  'scalability',\n",
       "  'topic modeling',\n",
       "  'training (meteorology)'],\n",
       " 'openaire_categories_flat': ['computation and language (cs.cl)',\n",
       "  'computer science - computation and language',\n",
       "  'computer science - machine learning',\n",
       "  'fos: computer and information sciences',\n",
       "  'machine learning (cs.lg)']}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3522d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_arxiv_taxonomy():\n",
    "    url = \"https://arxiv.org/category_taxonomy\"\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "\n",
    "    # Extract descriptors and category codes\n",
    "    pattern = r'<span class=\"descriptor\">([^<]+)</span>\\s+\\(([^)]+)\\)'\n",
    "    matches = re.findall(pattern, html)\n",
    "    return {code.lower(): desc.lower() for desc, code in matches}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5ffbe903",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_map = fetch_arxiv_taxonomy()\n",
    "acm_ccs_map = {\n",
    "    \"i.4\": \"image processing and computer vision\",\n",
    "    \"h.2.8\": \"database applications\",\n",
    "    \"h.2\": \"database management\",\n",
    "    \"k.4\": \"computers and society\",\n",
    "    \"d.2\": \"software engineering\",\n",
    "    \"c.2\": \"computer-communication networks\"\n",
    "    # You can expand this manually or scrape from dl.acm.org/ccs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "62a81a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_openaire_categories(categories, arxiv_map, acm_map):\n",
    "    decoded_labels = set()\n",
    "    cleaned = set()\n",
    "\n",
    "    for cat in categories:\n",
    "        cat = cat.strip().lower()\n",
    "\n",
    "        # Step 1: Remove FOS prefix\n",
    "        if cat.startswith(\"fos:\"):\n",
    "            cat = cat.replace(\"fos:\", \"\").strip()\n",
    "\n",
    "        # Step 2: Decode arXiv/ACM codes from parentheses\n",
    "        code_matches = re.findall(r\"\\(([\\w\\.\\-]+)\\)\", cat)\n",
    "        for code in code_matches:\n",
    "            if code in arxiv_map:\n",
    "                decoded_labels.add(arxiv_map[code])\n",
    "            elif code in acm_map:\n",
    "                decoded_labels.add(acm_map[code])\n",
    "\n",
    "        # Remove the (code) part, keep rest\n",
    "        cat = re.sub(r\"\\([\\w\\.\\-]+\\)\", \"\", cat).strip()\n",
    "\n",
    "        # Step 3: Decode raw codes directly (skip storing them)\n",
    "        if cat in arxiv_map:\n",
    "            decoded_labels.add(arxiv_map[cat])\n",
    "            continue\n",
    "        if cat in acm_map:\n",
    "            decoded_labels.add(acm_map[cat])\n",
    "            continue\n",
    "        if re.fullmatch(r\"[a-z]\\.\\d+(\\.\\d+)?\", cat) or re.fullmatch(r\"cs\\.[a-z]+\", cat):\n",
    "            continue\n",
    "\n",
    "        # Step 4: Split by 'and' and '-'\n",
    "        cleaned.add(cat)\n",
    "\n",
    "    final = cleaned.union(decoded_labels)\n",
    "    return sorted(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a841f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in cleaned_data:\n",
    "    if \"openaire_categories_flat\" in entry:\n",
    "        entry[\"openaire_categories_flat\"] = clean_openaire_categories(\n",
    "            entry[\"openaire_categories_flat\"], arxiv_map, acm_ccs_map\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7359c116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'OLMo: Accelerating the Science of Language Models',\n",
       " 'doi': '10.18653/v1/2024.acl-long.841',\n",
       " 'abstract': 'Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.',\n",
       " 'orkg_categories_flat': ['small language models (slms) survey', 'generic'],\n",
       " 'papers_with_code_categories_flat': ['language modeling',\n",
       "  'language modelling'],\n",
       " 'openalex_categories_flat': ['computer science', 'topic modeling'],\n",
       " 'openaire_categories_flat': ['computation and language',\n",
       "  'computer and information sciences',\n",
       "  'computer science - computation and language']}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6a49332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in all_data:\n",
    "    if \"openaire_categories_flat\" in entry:\n",
    "        entry[\"openaire_categories_flat\"] = clean_openaire_categories(\n",
    "            entry[\"openaire_categories_flat\"], arxiv_map, acm_ccs_map\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4be69845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a40d2d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cleaned_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a321e",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ae4c1d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#papers with OpenAlex cat: 120\n",
      "#papers with OpenAIRE cat: 98\n",
      "#papers with PwC cat: 92\n",
      "#papers with ORKG cat: 93\n",
      "#papers with missing OpenAlex cat: 4.76\n",
      "#papers with missing OpenAIRE cat: 22.22\n",
      "#papers with missing PwC cat: 26.98\n",
      "#papers with missing ORKG cat: 26.19\n"
     ]
    }
   ],
   "source": [
    "has_pwc_cat = 0\n",
    "has_orkg_cat = 0\n",
    "has_openalex_cat = 0\n",
    "has_openaire_cat = 0\n",
    "for paper in all_data:\n",
    "    if paper['openaire_categories_flat']:\n",
    "        has_openaire_cat += 1\n",
    "    if paper['openalex_categories_flat']:\n",
    "        has_openalex_cat += 1\n",
    "    if paper['papers_with_code_categories_flat']:\n",
    "        has_pwc_cat += 1\n",
    "    if paper['orkg_categories_flat']:\n",
    "        has_orkg_cat += 1\n",
    "\n",
    "all_papercount = len(all_data)\n",
    "print(f\"#papers with OpenAlex cat: {has_openalex_cat}\")\n",
    "print(f\"#papers with OpenAIRE cat: {has_openaire_cat}\")\n",
    "print(f\"#papers with PwC cat: {has_pwc_cat}\")\n",
    "print(f\"#papers with ORKG cat: {has_orkg_cat}\")\n",
    "print(f\"#papers with missing OpenAlex cat: {round((1-has_openalex_cat/all_papercount)*100, 2)}\")\n",
    "print(f\"#papers with missing OpenAIRE cat: {round((1-has_openaire_cat/all_papercount)*100, 2)}\")\n",
    "print(f\"#papers with missing PwC cat: {round((1-has_pwc_cat/all_papercount)*100, 2)}\")\n",
    "print(f\"#papers with missing ORKG cat: {round((1-has_orkg_cat/all_papercount)*100, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a4e10e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg OpenAlex cats: 12.39\n",
      "avg OpenAIRE cats: 7.51\n",
      "avg with PwC cats: 16.73\n",
      "avg ORKG cat: 2.83\n"
     ]
    }
   ],
   "source": [
    "pwc_cat_cnt = []\n",
    "orkg_cat_cnt = []\n",
    "openalex_cat_cnt = []\n",
    "openaire_cat_cnt = []\n",
    "for paper in cleaned_data:\n",
    "    if paper['openaire_categories_flat']:\n",
    "        openaire_cat_cnt.append(len(paper['openaire_categories_flat']))\n",
    "    if paper['openalex_categories_flat']:\n",
    "        openalex_cat_cnt.append(len(paper['openalex_categories_flat']))\n",
    "    if paper['papers_with_code_categories_flat']:\n",
    "        pwc_cat_cnt.append(len(paper['papers_with_code_categories_flat']))\n",
    "    if paper['orkg_categories_flat']:\n",
    "        orkg_cat_cnt.append(len(paper['orkg_categories_flat']))\n",
    "\n",
    "print(f\"avg OpenAlex cats: {round(sum(openalex_cat_cnt)/len(openalex_cat_cnt), 2)}\")\n",
    "print(f\"avg OpenAIRE cats: {round(sum(openaire_cat_cnt)/len(openaire_cat_cnt), 2)}\")\n",
    "print(f\"avg with PwC cats: {round(sum(pwc_cat_cnt)/len(pwc_cat_cnt), 2)}\")\n",
    "print(f\"avg ORKG cat: {round(sum(orkg_cat_cnt)/len(orkg_cat_cnt), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fff4b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_global_term_stats_and_mappings(data, min_support=2, similarity_threshold=0.8):\n",
    "    # Step 1: Determine category fields (ending in \"_flat\")\n",
    "    category_fields = [key for key in data[0].keys() if key.endswith(\"_flat\")]\n",
    "    print(f\"Using fields: {category_fields}\")\n",
    "\n",
    "    # Step 2: Count term frequency and store SKG sources\n",
    "    term_occurrence = Counter()\n",
    "    term_sources = defaultdict(set)\n",
    "\n",
    "    for paper in data:\n",
    "        for field in category_fields:\n",
    "            for term in paper.get(field, []):\n",
    "                clean_term = term.strip().lower()\n",
    "                term_occurrence[clean_term] += 1\n",
    "                term_sources[clean_term].add(field)\n",
    "\n",
    "    # Step 3: Filter terms by minimum support\n",
    "    valid_terms = {term for term, count in term_occurrence.items() if count >= min_support}\n",
    "    terms_list = sorted(valid_terms)\n",
    "\n",
    "    # Step 4: TF-IDF and cosine similarity\n",
    "    tfidf = TfidfVectorizer(analyzer='word', lowercase=True).fit_transform(terms_list)\n",
    "    cosine_sim = cosine_similarity(tfidf)\n",
    "\n",
    "    # Step 5: Create final mappings\n",
    "    mappings = []\n",
    "    for i, term1 in enumerate(terms_list):\n",
    "        for j in range(i + 1, len(terms_list)):\n",
    "            term2 = terms_list[j]\n",
    "            sim = cosine_sim[i, j]\n",
    "            if sim >= similarity_threshold:\n",
    "                mappings.append({\n",
    "                    \"term_1\": term1,\n",
    "                    \"term_2\": term2,\n",
    "                    \"similarity\": round(float(sim), 3),\n",
    "                    \"term_1_sources\": sorted(term_sources[term1]),\n",
    "                    \"term_2_sources\": sorted(term_sources[term2]),\n",
    "                    \"term_1_support\": term_occurrence[term1],\n",
    "                    \"term_2_support\": term_occurrence[term2]\n",
    "                })\n",
    "\n",
    "    # Step 6: Term stats dictionary\n",
    "    term_stats = {\n",
    "        term: {\n",
    "            \"support\": term_occurrence[term],\n",
    "            \"sources\": sorted(term_sources[term])\n",
    "        } for term in sorted(valid_terms)\n",
    "    }\n",
    "\n",
    "    return term_stats, mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2b68a92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fields: ['orkg_categories_flat', 'papers_with_code_categories_flat', 'openalex_categories_flat', 'openaire_categories_flat']\n"
     ]
    }
   ],
   "source": [
    "term_stats, term_mappings = build_global_term_stats_and_mappings(\n",
    "        data=cleaned_data,\n",
    "        min_support=2,\n",
    "        similarity_threshold=0.8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e879dc7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(term_mappings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
