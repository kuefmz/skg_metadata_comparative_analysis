[
    {
        "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable\n  Training Strategies",
        "doi": "10.48550/arxiv.2404.06395",
        "abstract": "The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .",
        "orkg categories": {
            "domains": [
                "Generic"
            ],
            "methods": [],
            "research problems": [
                "Small Language Models (SLMs) survey"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Domain Adaptation"
            ],
            "methods": [
                "Chinchilla"
            ],
            "main_collection_name": [
                "Language Models"
            ],
            "main_collection_area": [
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Training (meteorology)",
                "Scalability",
                "Computer science",
                "Natural language processing",
                "Artificial intelligence",
                "Geography",
                "Database",
                "Meteorology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computer Science - Machine Learning",
                "Machine Learning (cs.LG)",
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "OLMo: Accelerating the Science of Language Models",
        "doi": "10.18653/v1/2024.acl-long.841",
        "abstract": "Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.",
        "orkg categories": {
            "domains": [
                "Generic"
            ],
            "methods": [],
            "research problems": [
                "Small Language Models (SLMs) survey"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Language Modeling",
                "Language Modelling"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling"
            ],
            "concepts": [
                "Computer science"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computer Science - Computation and Language",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Enhancing text-based knowledge graph completion with zero-shot large language models: A focus on semantic enhancement",
        "doi": "10.1016/j.knosys.2024.112155",
        "abstract": "The design and development of text-based knowledge graph completion (KGC) methods leveraging textual entity descriptions are at the forefront of research. These methods involve advanced optimization techniques such as soft prompts and contrastive learning to enhance KGC models. The effectiveness of text-based methods largely hinges on the quality and richness of the training data. Large language models (LLMs) can utilize straightforward prompts to alter text data, thereby enabling data augmentation for KGC. Nevertheless, LLMs typically demand substantial computational resources. To address these issues, we introduce a framework termed constrained prompts for KGC (CP-KGC). This CP-KGC framework designs prompts that adapt to different datasets to enhance semantic richness. Additionally, CP-KGC employs a context constraint strategy to effectively identify polysemous entities within KGC datasets. Through extensive experimentation, we have verified the effectiveness of this framework. Even after quantization, the LLM (Qwen-7B-Chat-int4) still enhances the performance of text-based KGC methods \\footnote{Code and datasets are available at \\href{https://github.com/sjlmg/CP-KGC}{https://github.com/sjlmg/CP-KGC}}. This study extends the performance limits of existing models and promotes further integration of KGC with LLMs.",
        "orkg categories": {
            "domains": [],
            "methods": [
                "use of quantized and full-scale LLMs (e.g., Qwen-7B, GPT-3.5)",
                "semantic expansion",
                "semantic compression",
                "contrastive learning",
                "soft prompts",
                "context constraint strategy",
                "framework termed constrained prompts for KGC (CP-KGC)"
            ],
            "research problems": [
                "effective prompt design for LLM-driven data augmentation",
                "data truncation in PLMs",
                "polysemy in textual KGC inputs",
                "semantic quality of entity descriptions",
                "text-based knowledge graph completion"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Contrastive Learning",
                "Data Augmentation",
                "Graph Embedding",
                "Hallucination",
                "Knowledge Graph Completion",
                "Knowledge Graphs",
                "Quantization",
                "Text Generation"
            ],
            "methods": [
                "Contrastive Learning"
            ],
            "main_collection_name": [
                "Graph Representation Learning"
            ],
            "main_collection_area": [
                "Graphs"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Advanced Graph Neural Networks",
                "Data Quality and Management"
            ],
            "concepts": [
                "Computer science",
                "Context (archaeology)",
                "Graph",
                "Constraint (computer-aided design)",
                "Theoretical computer science",
                "Mechanical engineering",
                "Paleontology",
                "Engineering",
                "Biology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Artificial Intelligence",
                "Artificial Intelligence (cs.AI)",
                "FOS: Computer and information sciences",
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "COCONut: Modernizing COCO Segmentation",
        "doi": "10.1109/cvpr52733.2024.02065",
        "abstract": "Accepted at CVPR2024, data available at https://xdeng7.github.io/coconut.github.io/",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": [
                "Universal Segmentation",
                "Object detection"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Panoptic Segmentation",
                "Segmentation",
                "Universal Segmentation"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Advanced Neural Network Applications"
            ],
            "topics": [
                "Advanced Neural Network Applications",
                "Advanced Image and Video Retrieval Techniques",
                "Industrial Vision Systems and Defect Detection"
            ],
            "concepts": [
                "Coco",
                "Computer science",
                "Artificial intelligence"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computer Vision and Pattern Recognition (cs.CV)",
                "Computer Science - Computer Vision and Pattern Recognition"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Annotation Errors and NER: A Study with OntoNotes 5.0",
        "doi": "10.48550/arxiv.2406.19172",
        "abstract": "Named Entity Recognition (NER) is a well-studied problem in NLP. However, there is much less focus on studying NER datasets, compared to developing new NER models. In this paper, we employed three simple techniques to detect annotation errors in the OntoNotes 5.0 corpus for English NER, which is the largest available NER corpus for English. Our techniques corrected ~10% of the sentences in train/dev/test data. In terms of entity mentions, we corrected the span and/or type of ~8% of mentions in the dataset, while adding/deleting/splitting/merging a few more. These are large numbers of changes, considering the size of OntoNotes. We used three NER libraries to train, evaluate and compare the models trained with the original and the re-annotated datasets, which showed an average improvement of 1.23% in overall F-scores, with large (&gt;10%) improvements for some of the entity types. While our annotation error detection methods are not exhaustive and there is some manual annotation effort involved, they are largely language agnostic and can be employed with other NER datasets, and other sequence labelling tasks.",
        "orkg categories": {
            "domains": [
                "Natural Language Processing"
            ],
            "methods": [],
            "research problems": [],
            "tasks": [
                "Named Entity Recognition"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "named-entity-recognition",
                "Named Entity Recognition",
                "Named Entity Recognition (NER)",
                "NER"
            ],
            "methods": [
                "Focus"
            ],
            "main_collection_name": [
                "Transformers"
            ],
            "main_collection_area": [
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Natural Language Processing Techniques"
            ],
            "topics": [
                "Natural Language Processing Techniques",
                "Semantic Web and Ontologies"
            ],
            "concepts": [
                "Annotation",
                "Computer science",
                "Natural language processing",
                "Artificial intelligence"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language",
                "FOS: Computer and information sciences"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Understanding and Tackling Label Errors in Individual-Level Nature\n  Language Understanding",
        "doi": "10.48550/arxiv.2502.13297",
        "abstract": "Natural language understanding (NLU) is a task that enables machines to understand human language. Some tasks, such as stance detection and sentiment analysis, are closely related to individual subjective perspectives, thus termed individual-level NLU. Previously, these tasks are often simplified to text-level NLU tasks, ignoring individual factors. This not only makes inference difficult and unexplainable but often results in a large number of label errors when creating datasets. To address the above limitations, we propose a new NLU annotation guideline based on individual-level factors. Specifically, we incorporate other posts by the same individual and then annotate individual subjective perspectives after considering all individual posts. We use this guideline to expand and re-annotate the stance detection and topic-based sentiment analysis datasets. We find that error rates in the samples were as high as 31.7\\% and 23.3\\%. We further use large language models to conduct experiments on the re-annotation datasets and find that the large language models perform well on both datasets after adding individual factors. Both GPT-4o and Llama3-70B can achieve an accuracy greater than 87\\% on the re-annotation datasets. We also verify the effectiveness of individual factors through ablation studies. We call on future researchers to add individual factors when creating such datasets. Our re-annotation dataset can be found at https://github.com/24yearsoldstudent/Individual-NLU",
        "orkg categories": {
            "domains": [
                "Natural Language Processing"
            ],
            "methods": [],
            "research problems": [],
            "tasks": [
                "Stance detection",
                "Sentiment Analysis"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Natural Language Understanding",
                "Sentiment Analysis",
                "Stance Detection"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Child and Animal Learning Development"
            ],
            "topics": [
                "Child and Animal Learning Development",
                "Design Education and Practice",
                "Multi-Criteria Decision Making"
            ],
            "concepts": [
                "Psychology",
                "Computer science",
                "Natural language processing",
                "Cognitive psychology",
                "Linguistics",
                "Philosophy"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Artificial Intelligence",
                "Computer Science - Computation and Language",
                "Computation and Language (cs.CL)",
                "FOS: Computer and information sciences"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "BenchIE^FL: A Manually Re-Annotated Fact-Based Open Information Extraction Benchmark",
        "doi": "10.18653/v1/2024.findings-acl.496",
        "abstract": "Open Information Extraction (OIE) is a field of natural language processing that aims to present textual information in a format that allows it to be organized, analyzed and reflected upon. Numerous OIE systems are developed, claiming ever-increasing performance, marking the need for objective benchmarks. BenchIE is the latest reference we know of. Despite being very well thought out, we noticed a number of issues we believe are limiting. Therefore, we propose $\\textit{BenchIE}^{FL}$, a new OIE benchmark which fully enforces the principles of BenchIE while containing fewer errors, omissions and shortcomings when candidate facts are matched towards reference ones. $\\textit{BenchIE}^{FL}$ allows insightful conclusions to be drawn on the actual performance of OIE extractors.",
        "orkg categories": {
            "domains": [
                "Natural Language Processing"
            ],
            "methods": [],
            "research problems": [],
            "tasks": [
                "Open information extraction"
            ]
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Natural Language Processing Techniques"
            ],
            "topics": [
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Computer science",
                "Benchmark (surveying)",
                "Information extraction",
                "Extraction (chemistry)",
                "Artificial intelligence",
                "Information retrieval",
                "Geography",
                "Cartography",
                "Chemistry",
                "Chromatography"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computer Science - Computation and Language",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Human Evaluation of Procedural Knowledge Graph Extraction from Text with Large Language Models",
        "doi": "10.1007/978-3-031-77792-9_26",
        "abstract": "Presentation given by Valentina Carriero about the paper with the same title at the 24th International Conference on Knowledge Engineering and Knowledge Management - Amsterdam, Netherlands - November 28, 2024.",
        "orkg categories": {
            "methods": [
                "LLMs",
                "Prompt engineering"
            ],
            "research problems": [
                "procedural knowledge extraction"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Language Modeling",
                "Language Modelling",
                "Large Language Model",
                "Prompt Engineering"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Advanced Text Analysis Techniques"
            ],
            "concepts": [
                "Computer science",
                "Knowledge graph",
                "Graph",
                "Artificial intelligence",
                "Natural language processing",
                "Information extraction",
                "Theoretical computer science"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "Computer Science - Artificial Intelligence",
                "Computation and Language (cs.CL)",
                "FOS: Computer and information sciences",
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Human-Computer Interaction",
                "Human-Computer Interaction (cs.HC)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small\n  LLMs",
        "doi": "10.48550/arxiv.2412.13337",
        "abstract": "The rise of large language models (LLMs) has created a significant disparity: industrial research labs with their computational resources, expert teams, and advanced infrastructures, can effectively fine-tune LLMs, while individual developers and small organizations face barriers due to limited resources. In this paper, we aim to bridge this gap by presenting a comprehensive study on supervised fine-tuning of LLMs using instruction-tuning datasets spanning diverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B parameters) for their cost-efficiency and accessibility. We explore various training configurations and strategies across four open-source pre-trained models. We provide detailed documentation of these configurations, revealing findings that challenge several common training practices, including hyperparameter recommendations from TULU and phased training recommended by Orca. Key insights from our work include: (i) larger batch sizes paired with lower learning rates lead to improved model performance on benchmarks such as MMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics, such as lower gradient norms and higher loss values, are strong indicators of better final model performance, enabling early termination of sub-optimal runs and significant computational savings; (iii) through a thorough exploration of hyperparameters like warmup steps and learning rate schedules, we provide guidance for practitioners and find that certain simplifications do not compromise performance; and (iv) we observed no significant difference in performance between phased and stacked training strategies, but stacked training is simpler and more sample efficient. With these findings holding robustly across datasets and models, we hope this study serves as a guide for practitioners fine-tuning small LLMs and promotes a more inclusive environment for LLM research.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "MMLU"
            ],
            "methods": [
                "Focus"
            ],
            "main_collection_name": [
                "Transformers"
            ],
            "main_collection_area": [
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Digital Rights Management and Security"
            ],
            "topics": [
                "Digital Rights Management and Security"
            ],
            "concepts": [
                "Recipe",
                "Computer science",
                "Fine-tuning",
                "History",
                "Archaeology",
                "Quantum mechanics",
                "Physics"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computer Science - Artificial Intelligence",
                "Computer Science - Machine Learning",
                "Artificial Intelligence (cs.AI)",
                "Machine Learning (cs.LG)",
                "Statistics - Machine Learning",
                "I.2.7; I.2.6; I.2.4",
                "Machine Learning (stat.ML)",
                "I.2.6",
                "I.2.7",
                "I.2.4",
                "53-04"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n  Reinforcement Learning",
        "doi": "10.48550/arxiv.2501.12948",
        "abstract": "Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existing methods either design a fixed CoT tailored for a specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans and supervised fine-tuning (SFT) prone to overfitting, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), a novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoning-based translation to broader MT scenarios (e.g., multilingual MT, domain MT) unseen in the training phase; (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like context-aware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery through RL. Both human and automatic evaluation results indicate a steady translation performance improvement in a total of 10+ languages and 40+ translation directions on Flores-101 test set and four domain-specific MT tasks, especially on the languages unseen from training.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Mathematical Reasoning",
                "Multi-task Language Understanding",
                "Question Answering",
                "Reinforcement Learning (RL)"
            ],
            "methods": [
                "Adam",
                "1-bit Adam"
            ],
            "main_collection_name": [
                "Stochastic Optimization"
            ],
            "main_collection_area": [
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Software Engineering Research"
            ],
            "topics": [
                "Software Engineering Research",
                "Imbalanced Data Classification Techniques",
                "Statistical and Computational Modeling"
            ],
            "concepts": [
                "Reinforcement learning",
                "Reinforcement",
                "Computer science",
                "Cognitive science",
                "Artificial intelligence",
                "Psychology",
                "Social psychology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computation and Language (cs.CL)",
                "Computer Science - Machine Learning",
                "FOS: Computer and information sciences",
                "Machine Learning (cs.LG)",
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Computation and Language",
                "Computer Science - Artificial Intelligence"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
        "doi": "10.48550/arxiv.2501.12599",
        "abstract": "25 pages",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Math",
                "reinforcement-learning",
                "Reinforcement Learning",
                "Reinforcement Learning (RL)"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Robot Manipulation and Learning"
            ],
            "topics": [
                "Robot Manipulation and Learning",
                "Machine Learning and Data Classification",
                "Machine Learning and Algorithms"
            ],
            "concepts": [
                "Reinforcement",
                "Reinforcement learning",
                "Scaling",
                "Computer science",
                "Psychology",
                "Artificial intelligence",
                "Social psychology",
                "Mathematics",
                "Geometry"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Machine Learning",
                "Machine Learning (cs.LG)",
                "Artificial Intelligence (cs.AI)",
                "FOS: Computer and information sciences",
                "Computer Science - Artificial Intelligence"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Structure Guided Large Language Model for SQL Generation",
        "doi": "10.48550/arxiv.2402.13284",
        "abstract": "The 42nd International Conference on Machine Learning",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Development And Evaluation of a SOTA LLM-based NL-To-SQL translation interface"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Language Modeling",
                "Language Modelling",
                "Large Language Model",
                "model",
                "Natural Language Queries",
                "Text to SQL",
                "Text-To-SQL"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Distributed and Parallel Computing Systems"
            ],
            "topics": [
                "Distributed and Parallel Computing Systems",
                "Advanced Computational Techniques and Applications",
                "Scientific Computing and Data Management"
            ],
            "concepts": [
                "Computer science",
                "Programming language",
                "SQL",
                "SQL/PSM",
                "Data definition language",
                "PL/SQL",
                "Natural language processing",
                "Query by Example",
                "World Wide Web",
                "Web search query",
                "Search engine"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computation and Language (cs.CL)",
                "Computer Science - Software Engineering",
                "Databases (cs.DB)",
                "Software Engineering (cs.SE)",
                "FOS: Computer and information sciences",
                "Computer Science - Databases",
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Computation and Language",
                "Computer Science - Artificial Intelligence"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "TinyLlama: An Open-Source Small Language Model",
        "doi": "10.48550/arxiv.2401.02385",
        "abstract": "We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention and Lit-GPT), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama.",
        "orkg categories": {
            "domains": [
                "Generic"
            ],
            "methods": [],
            "research problems": [
                "Small Language Models (SLMs) survey"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Computational Efficiency",
                "Language Modeling",
                "Language Modelling",
                "model",
                "Small Language Model"
            ],
            "methods": [
                "FlashAttention"
            ],
            "main_collection_name": [
                "Attention"
            ],
            "main_collection_area": [
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Natural Language Processing Techniques"
            ],
            "topics": [
                "Natural Language Processing Techniques",
                "Topic Modeling",
                "Algorithms and Data Compression"
            ],
            "concepts": [
                "Computer science",
                "Open source",
                "Source code",
                "Language model",
                "Code (set theory)",
                "Architecture",
                "Downstream (manufacturing)",
                "Programming language",
                "World Wide Web",
                "Artificial intelligence",
                "Software",
                "Set (abstract data type)",
                "Archaeology",
                "Operations management",
                "Economics",
                "History"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computer Science - Machine Learning",
                "Artificial Intelligence (cs.AI)",
                "Computation and Language (cs.CL)",
                "Computer Science - Artificial Intelligence",
                "Computer Science - Computation and Language",
                "Machine Learning (cs.LG)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your\n  Phone",
        "doi": "10.48550/arxiv.2404.14219",
        "abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.",
        "orkg categories": {
            "domains": [
                "Generic"
            ],
            "methods": [],
            "research problems": [
                "Small Language Model (SLM) Survey Comparison"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Language Modeling",
                "Language Modelling",
                "Math",
                "MMLU",
                "MMR total"
            ],
            "methods": [
                "Fixed Factorized Attention",
                "BPE",
                "Multi-Head Attention",
                "Adam",
                "Softmax",
                "Cosine Annealing",
                "GPT-3",
                "LLaMA",
                "Dropout",
                "Attention Dropout",
                "Linear Layer",
                "Linear Warmup With Cosine Annealing",
                "MoE",
                "Dense Connections",
                "Layer Normalization",
                "Residual Connection",
                "GELU",
                "Attention",
                "Weight Decay",
                "15 Ways to Contact How can i speak to someone at Delta Airlines"
            ],
            "main_collection_name": [
                "Attention Patterns",
                "Regularization",
                "Stochastic Optimization",
                "Normalization",
                "Transformers",
                "Skip Connections",
                "Attention Modules",
                "Output Functions",
                "Learning Rate Schedules",
                "Ensembling",
                "Attention Mechanisms",
                "Subword Segmentation",
                "Activation Functions",
                "Language Models",
                "Feedforward Networks"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Computational Physics and Python Applications"
            ],
            "topics": [
                "Computational Physics and Python Applications"
            ],
            "concepts": [
                "Phone",
                "Computer science",
                "Linguistics",
                "Philosophy"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "Computer Science - Artificial Intelligence",
                "Artificial Intelligence (cs.AI)",
                "Computation and Language (cs.CL)",
                "FOS: Computer and information sciences"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency",
        "doi": "10.18653/v1/2023.findings-emnlp.1032",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploration Of LLM Hallucination Benchmarks"
            ],
            "tasks": [
                "TypeDetection",
                "InputQuestion",
                "LabelAnswer",
                "MetricAUROC"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Hallucination",
                "Question Answering"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Adversarial Robustness in Machine Learning"
            ],
            "concepts": [
                "Computer science",
                "Consistency (knowledge bases)",
                "Causal consistency",
                "Black box",
                "Natural language processing",
                "Artificial intelligence",
                "Consistency model",
                "Sequential consistency",
                "Data consistency",
                "Operating system"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens\n  of context",
        "doi": "10.48550/arxiv.2403.05530",
        "abstract": "In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (&gt;99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Finding Pre-trained Large Language Model"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "1 Image, 2*2 Stitching",
                "Code Generation",
                "FS-MEVQA",
                "Image Retrieval",
                "Long-Context Understanding",
                "Math Word Problem Solving",
                "Question Answering",
                "Retrieval",
                "Temporal Relation Extraction",
                "Video Question Answering",
                "Visual Question Answering",
                "Zero-Shot Video Question Answer"
            ],
            "methods": [
                "Linear Layer",
                "Layer Normalization",
                "Multi-Head Attention",
                "Label Smoothing",
                "GPT-4",
                "Residual Connection",
                "BPE",
                "Softmax",
                "Transformer",
                "Dense Connections",
                "Position-Wise Feed-Forward Layer",
                "Adam",
                "Dropout",
                "SET",
                "Attention",
                "Absolute Position Encodings"
            ],
            "main_collection_name": [
                "Feedforward Networks",
                "Output Functions",
                "Language Models",
                "Attention Mechanisms",
                "Attention Modules",
                "Normalization",
                "Position Embeddings",
                "Stochastic Optimization",
                "Skip Connections",
                "Regularization",
                "Subword Segmentation",
                "Sparsity",
                "Transformers"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Semantic Web and Ontologies"
            ],
            "topics": [
                "Semantic Web and Ontologies"
            ],
            "concepts": [
                "Context (archaeology)",
                "Business",
                "Computer science",
                "Internet privacy",
                "Geography",
                "Archaeology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computation and Language (cs.CL)",
                "Computer Science - Artificial Intelligence",
                "FOS: Computer and information sciences",
                "Computer Science - Computation and Language",
                "Artificial Intelligence (cs.AI)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Jamba-1.5: Hybrid Transformer-Mamba Models at Scale",
        "doi": "10.48550/arxiv.2408.12570",
        "abstract": "Webpage: https://www.ai21.com/jamba",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Chatbot",
                "Instruction Following",
                "Mamba",
                "Mixture-of-Experts",
                "Quantization"
            ],
            "methods": [
                "Multi-Head Attention",
                "Absolute Position Encodings",
                "Dropout",
                "Layer Normalization",
                "Transformer",
                "Dense Connections",
                "Position-Wise Feed-Forward Layer",
                "Residual Connection",
                "Attention",
                "BPE",
                "Adam",
                "Linear Layer",
                "Softmax",
                "Label Smoothing"
            ],
            "main_collection_name": [
                "Feedforward Networks",
                "Normalization",
                "Attention Modules",
                "Transformers",
                "Regularization",
                "Subword Segmentation",
                "Position Embeddings",
                "Stochastic Optimization",
                "Output Functions",
                "Skip Connections",
                "Attention Mechanisms"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Fluid Dynamics and Vibration Analysis"
            ],
            "topics": [
                "Fluid Dynamics and Vibration Analysis",
                "High voltage insulation and dielectric phenomena",
                "Power Transformer Diagnostics and Insulation"
            ],
            "concepts": [
                "Transformer",
                "Scale (ratio)",
                "Computer science",
                "Engineering",
                "Electrical engineering",
                "Geography",
                "Voltage",
                "Cartography"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computation and Language (cs.CL)",
                "Computer Science - Machine Learning",
                "Machine Learning (cs.LG)",
                "FOS: Computer and information sciences",
                "Computer Science - Computation and Language"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives",
        "doi": "10.18653/v1/2024.acl-long.197",
        "abstract": "The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties that LLM often overlooks. Reflecting upon these can catalyze more accurate and stable reflection. Experiments conducted on a series of reasoning and translation tasks with different LLMs serve to underscore the effectiveness and generality of our strategy.",
        "orkg categories": {
            "domains": [],
            "methods": [
                "Self-Contrast"
            ],
            "research problems": [
                "Exploring Self-Refine Methods in Large Language Models"
            ],
            "tasks": [
                "CommonMT",
                "SVAMP",
                "GSM8K"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Language Modeling",
                "Language Modelling",
                "Large Language Model"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Education and Critical Thinking Development"
            ],
            "topics": [
                "Education and Critical Thinking Development"
            ],
            "concepts": [
                "Contrast (vision)",
                "Reflection (computer programming)",
                "Computer science",
                "Artificial intelligence",
                "Programming language"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Artificial Intelligence",
                "Computer Science - Computation and Language",
                "FOS: Computer and information sciences",
                "Artificial Intelligence (cs.AI)",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models",
        "doi": "10.48550/arxiv.2405.00402",
        "abstract": "The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches deliver more performant models, they do not show sufficiently strong generalization ability as the training only relies on the provided demonstrations.  In this paper, we propose the Self-refine Instruction-tuning method that elicits Smaller Language Models to self-refine their abilities. Our approach is based on a two-stage process, where reasoning abilities are first transferred between LLMs and Small Language Models (SLMs) via Instruction-tuning on demonstrations provided by LLMs, and then the instructed models Self-refine their abilities through preference optimization strategies. In particular, the second phase operates refinement heuristics based on the Direct Preference Optimization algorithm, where the SLMs are elicited to deliver a series of reasoning paths by automatically sampling the generated responses and providing rewards using ground truths from the LLMs. Results obtained on commonsense and math reasoning tasks show that this approach significantly outperforms Instruction-tuning in both in-domain and out-domain scenarios, aligning the reasoning abilities of Smaller and Larger Language Models.",
        "orkg categories": {
            "domains": [],
            "methods": [
                "SRIT"
            ],
            "research problems": [
                "Exploring Self-Refine Methods in Large Language Models"
            ],
            "tasks": [
                "Open Book Question Answering",
                "Common Sense Question Answering",
                "Physical Interaction Question Answering",
                "Social Interaction Question Answering"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Math"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Natural Language Processing Techniques"
            ],
            "topics": [
                "Natural Language Processing Techniques",
                "Topic Modeling",
                "Speech and dialogue systems"
            ],
            "concepts": [
                "Computer science",
                "Language model",
                "Natural language processing",
                "Artificial intelligence"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "FOS: Computer and information sciences",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Improving LLM-based Machine Translation with Systematic Self-Correction",
        "doi": "10.48550/arxiv.2402.16379",
        "abstract": "Our code and data are available at https://github.com/fzp0424/self_correct_mt",
        "orkg categories": {
            "domains": [],
            "methods": [
                "TEaR"
            ],
            "research problems": [
                "Exploring Self-Refine Methods in Large Language Models"
            ],
            "tasks": [
                "WMT23",
                "WMT22"
            ]
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Natural Language Processing Techniques"
            ],
            "topics": [
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Machine translation",
                "Translation (biology)",
                "Computer science",
                "Artificial intelligence",
                "Natural language processing",
                "Chemistry",
                "Biochemistry",
                "Messenger RNA",
                "Gene"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "Computer Science - Artificial Intelligence",
                "FOS: Computer and information sciences",
                "Computation and Language (cs.CL)",
                "Artificial Intelligence (cs.AI)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
        "doi": "10.18653/v1/2024.acl-long.826",
        "abstract": "Recent studies show that large language models (LLMs) improve their performance through self-feedback on certain tasks while degrade on others. We discovered that such a contrary is due to LLM's bias in evaluating their own output. In this paper, we formally define LLM's self-bias - the tendency to favor its own generation - using two statistics. We analyze six LLMs (GPT-4, GPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks. The code and data are released at https://github.com/xu1998hz/llm_self_bias.",
        "orkg categories": {
            "domains": [],
            "methods": [
                "Self-Bias"
            ],
            "research problems": [
                "Exploring Self-Refine Methods in Large Language Models"
            ],
            "tasks": [
                "Flores-200",
                "MQM"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Mathematical Reasoning",
                "Text Generation"
            ],
            "methods": [
                "Dropout",
                "Attention Dropout",
                "Multi-Head Attention",
                "Softmax",
                "Linear Warmup With Cosine Annealing",
                "GELU",
                "Residual Connection",
                "Weight Decay",
                "Linear Layer",
                "Dense Connections",
                "15 Ways to Contact How can i speak to someone at Delta Airlines",
                "Adam",
                "BPE",
                "Fixed Factorized Attention",
                "Layer Normalization",
                "GPT-3",
                "Cosine Annealing",
                "Attention"
            ],
            "main_collection_name": [
                "Subword Segmentation",
                "Attention Patterns",
                "Transformers",
                "Normalization",
                "Attention Modules",
                "Regularization",
                "Learning Rate Schedules",
                "Skip Connections",
                "Stochastic Optimization",
                "Feedforward Networks",
                "Attention Mechanisms",
                "Activation Functions",
                "Output Functions"
            ],
            "main_collection_area": [
                "General",
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Law, Economics, and Judicial Systems"
            ],
            "topics": [
                "Law, Economics, and Judicial Systems",
                "European and International Contract Law",
                "Legal principles and applications"
            ],
            "concepts": [
                "Pride",
                "Prejudice (legal term)",
                "Computer science",
                "Social psychology",
                "Psychology",
                "Political science",
                "Law"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computation and Language (cs.CL)",
                "Computer Science - Artificial Intelligence",
                "FOS: Computer and information sciences",
                "Computer Science - Computation and Language",
                "Artificial Intelligence (cs.AI)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs",
        "doi": "10.18653/v1/2023.emnlp-main.565",
        "abstract": "Large language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-independent quantization method for LLMs to guarantee its generalization performance? In this work, we propose EasyQuant, a training-free and data-independent weight-only quantization algorithm for LLMs. Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error. Therefore, in EasyQuant, we leave the outliers (less than 1%) unchanged and optimize the quantization range to reduce the reconstruction error. With these methods, we surprisingly find that EasyQuant achieves comparable performance to the original model. Since EasyQuant does not depend on any training data, the generalization performance of quantized LLMs is safely guaranteed. Moreover, EasyQuant can be implemented in parallel so that the quantized model could be attained in a few minutes even for LLMs over 100B. To our best knowledge, we are the first work that achieves almost lossless quantization performance for LLMs under a data-independent setting and our algorithm runs over 10 times faster than the data-dependent methods.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Data Free Quantization",
                "Quantization"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Multimodal Machine Learning Applications"
            ],
            "concepts": [
                "Quantization (signal processing)",
                "Linde–Buzo–Gray algorithm",
                "Computer science",
                "Algorithm",
                "Outlier",
                "Artificial intelligence"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Machine Learning",
                "Computer Science - Artificial Intelligence"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks",
        "doi": "10.1145/3589334.3645363",
        "abstract": "Accepted by WWW 2024",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Retrieval Augmented Generation (RAG) Method"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Fact Checking",
                "Information Retrieval",
                "Language Modelling",
                "Large Language Model",
                "Long Form Question Answering",
                "Multi-hop Question Answering",
                "Question Answering",
                "Retrieval",
                "Retrieval-augmented Generation",
                "slot-filling",
                "Slot Filling"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Speech and dialogue systems",
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Computer science",
                "Chain (unit)",
                "Natural language processing",
                "Artificial intelligence",
                "Human–computer interaction",
                "Programming language",
                "Information retrieval",
                "Physics",
                "Astronomy"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "Computation and Language (cs.CL)",
                "FOS: Computer and information sciences"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "The Power of Noise: Redefining Retrieval for RAG Systems",
        "doi": "10.1145/3626772.3657834",
        "abstract": "Retrieval-Augmented Generation (RAG) has recently emerged as a method to extend beyond the pre-trained knowledge of Large Language Models by augmenting the original prompt with relevant passages or documents retrieved by an Information Retrieval (IR) system. RAG has become increasingly important for Generative AI solutions, especially in enterprise settings or in any domain in which knowledge is constantly refreshed and cannot be memorized in the LLM. We argue here that the retrieval component of RAG systems, be it dense or sparse, deserves increased attention from the research community, and accordingly, we conduct the first comprehensive and systematic examination of the retrieval strategy of RAG systems. We focus, in particular, on the type of passages IR systems within a RAG solution should retrieve. Our analysis considers multiple factors, such as the relevance of the passages included in the prompt context, their position, and their number. One counter-intuitive finding of this work is that the retriever's highest-scoring documents that are not directly relevant to the query (e.g., do not contain the answer) negatively impact the effectiveness of the LLM. Even more surprising, we discovered that adding random documents in the prompt improves the LLM accuracy by up to 35%. These results highlight the need to investigate the appropriate strategies when integrating retrieval with LLMs, thereby laying the groundwork for future research in this area.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Retrieval Augmented Generation (RAG) Method"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Information Retrieval",
                "RAG",
                "Retrieval",
                "Retrieval-augmented Generation",
                "Text Generation"
            ],
            "methods": [
                "Linear Warmup With Linear Decay",
                "RAG",
                "GELU",
                "BERT",
                "Layer Normalization",
                "Linear Layer",
                "Adam",
                "BART",
                "Dropout",
                "Multi-Head Attention",
                "Softmax",
                "Weight Decay",
                "WordPiece",
                "Dense Connections",
                "BPE",
                "Attention Dropout",
                "Residual Connection",
                "Attention"
            ],
            "main_collection_name": [
                "Learning Rate Schedules",
                "Regularization",
                "Activation Functions",
                "Skip Connections",
                "Attention Modules",
                "Feedforward Networks",
                "Attention Mechanisms",
                "Subword Segmentation",
                "Transformers",
                "Language Models",
                "Normalization",
                "Output Functions",
                "Stochastic Optimization"
            ],
            "main_collection_area": [
                "General",
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Natural Language Processing Techniques"
            ],
            "topics": [
                "Natural Language Processing Techniques",
                "Topic Modeling",
                "Music and Audio Processing"
            ],
            "concepts": [
                "Computer science",
                "Noise (video)",
                "Power (physics)",
                "Artificial intelligence",
                "Physics",
                "Quantum mechanics",
                "Image (mathematics)"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "FOS: Computer and information sciences",
                "information retrieval; llm; rag",
                "Information Retrieval (cs.IR)",
                "0404 agricultural biotechnology",
                "02 engineering and technology",
                "RAG; LLM; Information Retrieval",
                "04 agricultural and veterinary sciences",
                "Computation and Language (cs.CL)",
                "Computer Science - Information Retrieval",
                "0202 electrical engineering, electronic engineering, information engineering"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation",
        "doi": "10.48550/arxiv.2312.11361",
        "abstract": "Retrieval-Augmented Generation (RAG) grounds Large Language Model (LLM) output by leveraging external knowledge sources to reduce factual hallucinations. However, prior work lacks a comprehensive evaluation of different language families, making it challenging to evaluate LLM robustness against errors in external retrieved knowledge. To overcome this, we establish NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across 18 typologically diverse languages. NoMIRACL includes both a non-relevant and a relevant subset. Queries in the non-relevant subset contain passages judged as non-relevant, whereas queries in the relevant subset include at least a single judged relevant passage. We measure relevance assessment using: (i) hallucination rate, measuring model tendency to hallucinate, when the answer is not present in passages in the non-relevant subset, and (ii) error rate, measuring model inaccuracy to recognize relevant passages in the relevant subset.In our work, we observe that most models struggle to balance the two capacities. Models such as LLAMA-2 and Orca-2 achieve over 88% hallucination rate on the non-relevant subset. Mistral and LLAMA-3 hallucinate less but can achieve up to a 74.9% error rate on the relevant subset. Overall, GPT-4 is observed to provide the best tradeoff on both subsets, highlighting future work necessary to improve LLM robustness. NoMIRACL dataset and evaluation code are available at: https://github.com/project-miracl/nomiracl.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Retrieval Augmented Generation (RAG) Method"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Domain Adaptation and Few-Shot Learning"
            ],
            "concepts": [
                "Computer science",
                "Information retrieval",
                "World Wide Web"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language",
                "Computer Science - Information Retrieval",
                "FOS: Computer and information sciences",
                "Information Retrieval (cs.IR)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Retrieval meets Long Context Large Language Models",
        "doi": "10.48550/arxiv.2310.03025",
        "abstract": "Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Retrieval Augmented Generation (RAG) Method"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "16k",
                "4k",
                "Few-Shot Learning",
                "Natural Questions",
                "Question Answering",
                "Retrieval"
            ],
            "methods": [
                "Weight Decay",
                "Dropout",
                "Discriminative Fine-Tuning",
                "GPT",
                "Cosine Annealing",
                "Attention",
                "Linear Layer",
                "BPE",
                "Attention Dropout",
                "Dense Connections",
                "15 Ways to Contact How can i speak to someone at Delta Airlines",
                "Residual Connection",
                "Linear Warmup With Cosine Annealing",
                "Softmax",
                "Multi-Head Attention",
                "Layer Normalization",
                "Adam",
                "GELU",
                "Fixed Factorized Attention",
                "GPT-3"
            ],
            "main_collection_name": [
                "Normalization",
                "Skip Connections",
                "Output Functions",
                "Activation Functions",
                "Fine-Tuning",
                "Regularization",
                "Feedforward Networks",
                "Attention Modules",
                "Transformers",
                "Stochastic Optimization",
                "Attention Mechanisms",
                "Learning Rate Schedules",
                "Attention Patterns",
                "Subword Segmentation"
            ],
            "main_collection_area": [
                "General",
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Multimodal Machine Learning Applications"
            ],
            "concepts": [
                "Computer science",
                "Context (archaeology)",
                "Automatic summarization",
                "Margin (machine learning)",
                "Window (computing)",
                "Information retrieval",
                "Artificial intelligence",
                "Machine learning",
                "World Wide Web",
                "Paleontology",
                "Biology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "Computer Science - Artificial Intelligence",
                "Computation and Language (cs.CL)",
                "Computer Science - Information Retrieval",
                "Computer Science - Computer Vision and Pattern Recognition",
                "Computer Vision and Pattern Recognition (cs.CV)",
                "Information Retrieval (cs.IR)",
                "Machine Learning (cs.LG)",
                "FOS: Computer and information sciences",
                "Computer Science - Machine Learning",
                "Artificial Intelligence (cs.AI)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval",
        "doi": "10.48550/arxiv.2401.18059",
        "abstract": "Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20% in absolute accuracy.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Retrieval Augmented Generation (RAG) Method"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Question Answering",
                "Retrieval"
            ],
            "methods": [
                "Dropout",
                "Position-Wise Feed-Forward Layer",
                "Residual Connection",
                "Multi-Head Attention",
                "Adam",
                "Linear Layer",
                "BPE",
                "Transformer",
                "GPT-4",
                "Softmax",
                "Label Smoothing",
                "Dense Connections",
                "Absolute Position Encodings",
                "Attention",
                "Layer Normalization"
            ],
            "main_collection_name": [
                "Subword Segmentation",
                "Stochastic Optimization",
                "Normalization",
                "Feedforward Networks",
                "Regularization",
                "Skip Connections",
                "Output Functions",
                "Attention Mechanisms",
                "Attention Modules",
                "Transformers",
                "Position Embeddings",
                "Language Models"
            ],
            "main_collection_area": [
                "General",
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Semantic Web and Ontologies"
            ],
            "topics": [
                "Semantic Web and Ontologies",
                "Data Management and Algorithms",
                "Topic Modeling"
            ],
            "concepts": [
                "Computer science",
                "Tree (set theory)",
                "Information retrieval",
                "Artificial intelligence",
                "Mathematics",
                "Combinatorics"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "Computer Science - Machine Learning"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Corrective Retrieval Augmented Generation",
        "doi": "10.48550/arxiv.2401.15884",
        "abstract": "Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return sub-optimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-then-recompose algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based approaches. Experiments on four datasets covering short- and long-form generation tasks show that CRAG can significantly improve the performance of RAG-based approaches.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Retrieval Augmented Generation (RAG) Method"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "RAG",
                "Retrieval",
                "Retrieval-augmented Generation"
            ],
            "methods": [
                "Focus"
            ],
            "main_collection_name": [
                "Transformers"
            ],
            "main_collection_area": [
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Speech and dialogue systems"
            ],
            "topics": [
                "Speech and dialogue systems"
            ],
            "concepts": [
                "Computer science",
                "Information retrieval"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Humans",
                "Security Test Automation",
                "Large Language Models",
                "SARS-CoV-2",
                "COVID-19",
                "Information Storage and Retrieval",
                "RA1-1270",
                "Artificial Intelligence and Robotics",
                "Cyber Security",
                "Machine Learning (cs.LG)",
                "Retrieval-Augmented Generation",
                "Software Security",
                "Artificial Intelligence (cs.AI)",
                "FOS: Computer and information sciences",
                "Computation and Language (cs.CL)",
                "Public aspects of medicine",
                "Computer Science - Computation and Language",
                "R858-859.7",
                "Software Engineering (cs.SE)",
                "Computer Science - Artificial Intelligence",
                "Computer applications to medicine. Medical informatics",
                "Security Test Case Generation",
                "Cybersecurity",
                "Computer Science - Machine Learning",
                "Computer Science - Software Engineering",
                "004"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems",
        "doi": "10.48550/arxiv.2401.13256",
        "abstract": "Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their behavior to diverse task requirements. Meanwhile, evaluation tokens gauge the relevance score between the dialogue context and the retrieved evidence. In addition, we carefully design a self-refinement mechanism to iteratively refine the generated response considering 1) the consistency scores between the generated response and retrieved evidence; and 2) the relevance scores. Experiments on two personalized datasets (DuLeMon and KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge source selection and response generation task with itself as a retriever in a unified manner. Extensive analyses and discussions are provided for shedding some new perspectives for personalized dialogue systems.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Retrieval Augmented Generation (RAG) Method"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "RAG",
                "Response Generation",
                "Retrieval",
                "Retrieval-augmented Generation"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Speech and dialogue systems"
            ],
            "concepts": [
                "Computer science",
                "Relevance (law)",
                "Consistency (knowledge bases)",
                "Personalization",
                "Language model",
                "Context (archaeology)",
                "Selection (genetic algorithm)",
                "Task (project management)",
                "Information retrieval",
                "Artificial intelligence",
                "Natural language processing",
                "World Wide Web",
                "Paleontology",
                "Management",
                "Political science",
                "Law",
                "Economics",
                "Biology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Artificial Intelligence",
                "Computation and Language (cs.CL)",
                "FOS: Computer and information sciences",
                "Artificial Intelligence (cs.AI)",
                "02 engineering and technology",
                "04 agricultural and veterinary sciences",
                "0202 electrical engineering, electronic engineering, information engineering",
                "Computer Science - Computation and Language",
                "0404 agricultural biotechnology"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction",
        "doi": "10.1145/3625007.3627505",
        "abstract": "Narrative construction is the process of representing disparate event information into a logical plot structure that models an end to end story. Intelligence analysis is an example of a domain that can benefit tremendously from narrative construction techniques, particularly in aiding analysts during the largely manual and costly process of synthesizing event information into comprehensive intelligence reports. Manual intelligence report generation is often prone to challenges such as integrating dynamic event information, writing fine-grained queries, and closing information gaps. This motivates the development of a system that retrieves and represents critical aspects of events in a form that aids in automatic generation of intelligence reports.  We introduce a Retrieval Augmented Generation (RAG) approach to augment prompting of an autoregressive decoder by retrieving structured information asserted in a knowledge graph to generate targeted information based on a narrative plot model. We apply our approach to the problem of neural intelligence report generation and introduce FABULA, framework to augment intelligence analysis workflows using RAG. An analyst can use FABULA to query an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be used to augment prompting of a Large Language Model (LLM) during intelligence report generation. Our evaluation studies show that the plot points included in the generated intelligence reports have high semantic relevance, high coherency, and low data redundancy.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Retrieval Augmented Generation (RAG) Method"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Decoder",
                "Language Modelling",
                "Large Language Model",
                "RAG",
                "Retrieval",
                "Retrieval-augmented Generation"
            ],
            "methods": [
                "WordPiece",
                "Linear Layer",
                "Multi-Head Attention",
                "Attention",
                "BPE",
                "Softmax",
                "Linear Warmup With Linear Decay",
                "Layer Normalization",
                "Dense Connections",
                "Dropout",
                "Attention Dropout",
                "Residual Connection",
                "Weight Decay",
                "BART",
                "RAG",
                "Adam",
                "BERT",
                "GELU"
            ],
            "main_collection_name": [
                "Learning Rate Schedules",
                "Language Models",
                "Attention Modules",
                "Regularization",
                "Normalization",
                "Stochastic Optimization",
                "Transformers",
                "Feedforward Networks",
                "Activation Functions",
                "Subword Segmentation",
                "Output Functions",
                "Skip Connections",
                "Attention Mechanisms"
            ],
            "main_collection_area": [
                "General",
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Scientific Computing and Data Management",
                "Web Data Mining and Analysis"
            ],
            "concepts": [
                "Computer science",
                "Plot (graphics)",
                "Narrative",
                "Graph",
                "Event (particle physics)",
                "Artificial intelligence",
                "Natural language processing",
                "Information retrieval",
                "Workflow",
                "Redundancy (engineering)",
                "Data science",
                "Theoretical computer science",
                "Database",
                "Linguistics",
                "Statistics",
                "Physics",
                "Mathematics",
                "Quantum mechanics",
                "Operating system",
                "Philosophy"
            ]
        },
        "openaire categories": {
            "subjects": [
                "0501 psychology and cognitive sciences",
                "05 social sciences",
                "0509 other social sciences",
                "Computer Science - Information Retrieval",
                "Information Retrieval (cs.IR)",
                "FOS: Computer and information sciences"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning",
        "doi": "10.48550/arxiv.2310.01061",
        "abstract": "Accepted by ICLR 2024",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Retrieval Augmented Generation (RAG) Method"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Knowledge Graphs",
                "Language Modeling",
                "Language Modelling",
                "Large Language Model",
                "Retrieval",
                "valid"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Advanced Graph Neural Networks"
            ],
            "concepts": [
                "Inference",
                "Analytic reasoning",
                "Reasoning system",
                "Computer science",
                "Model-based reasoning",
                "Case-based reasoning",
                "Benchmark (surveying)",
                "Deductive reasoning",
                "Automated reasoning",
                "Verbal reasoning",
                "Artificial intelligence",
                "Opportunistic reasoning",
                "Natural language processing",
                "Cognitive science",
                "Knowledge representation and reasoning",
                "Psychology",
                "Cognition",
                "Geodesy",
                "Neuroscience",
                "Geography"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Artificial Intelligence (cs.AI)",
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language",
                "Computer Science - Artificial Intelligence",
                "FOS: Computer and information sciences"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph\n  Understanding and Question Answering",
        "doi": "10.48550/arxiv.2402.07630",
        "abstract": "Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.~\\footnote{Our codes and datasets are available at: \\url{https://github.com/XiaoxinHe/G-Retriever}}",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Retrieval Augmented Generation (RAG) Method"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Common Sense Reasoning",
                "Graph Classification",
                "Graph Question Answering",
                "Hallucination",
                "Question Answering",
                "RAG",
                "Retrieval",
                "Retrieval-augmented Generation"
            ],
            "methods": [
                "Residual Connection",
                "Weight Decay",
                "RAG",
                "Linear Layer",
                "WordPiece",
                "BPE",
                "Layer Normalization",
                "BERT",
                "BART",
                "Attention",
                "Dropout",
                "Focus",
                "GELU",
                "Linear Warmup With Linear Decay",
                "Adam",
                "Multi-Head Attention",
                "Softmax",
                "Attention Dropout",
                "Dense Connections"
            ],
            "main_collection_name": [
                "Feedforward Networks",
                "Regularization",
                "Output Functions",
                "Attention Mechanisms",
                "Transformers",
                "Skip Connections",
                "Attention Modules",
                "Normalization",
                "Learning Rate Schedules",
                "Stochastic Optimization",
                "Language Models",
                "Activation Functions",
                "Subword Segmentation"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Advanced Graph Neural Networks",
                "Advanced Text Analysis Techniques"
            ],
            "concepts": [
                "Question answering",
                "Information retrieval",
                "Graph",
                "Labrador Retriever",
                "Computer science",
                "Natural language processing",
                "Medicine",
                "Theoretical computer science",
                "Pathology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Machine Learning",
                "FOS: Computer and information sciences",
                "Machine Learning (cs.LG)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI",
        "doi": "10.1016/j.inffus.2019.12.012",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Explainable Artificial Intelligence (XAI)"
            ],
            "topics": [
                "Explainable Artificial Intelligence (XAI)",
                "Adversarial Robustness in Machine Learning",
                "Artificial Intelligence in Healthcare and Education"
            ],
            "concepts": [
                "Computer science",
                "Artificial intelligence",
                "Taxonomy (biology)",
                "Field (mathematics)",
                "Software deployment",
                "Data science",
                "Deep learning",
                "Machine learning",
                "Management science",
                "Software engineering",
                "Engineering",
                "Botany",
                "Mathematics",
                "Pure mathematics",
                "Biology"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Generating Benchmarks for Factuality Evaluation of Language Models",
        "doi": "10.48550/arxiv.2307.06908",
        "abstract": "Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing methods for factuality evaluation of LLM generation focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent domain specific or rare facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create three benchmarks: Wiki-FACTOR, News-FACTOR and Expert-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score and perplexity do not always agree on model ranking; (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators. We make our data and code publicly available in https://github.com/AI21Labs/factor.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploration Of LLM Hallucination Benchmarks"
            ],
            "tasks": [
                "Task TypeMulti-Choice QA",
                "ask InputQuestion",
                "Task LabelAnswer",
                "Task MetricLikelihood"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Language Modeling",
                "Language Modelling",
                "Retrieval"
            ],
            "methods": [
                "Focus"
            ],
            "main_collection_name": [
                "Transformers"
            ],
            "main_collection_area": [
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Computer science",
                "Natural language processing",
                "Linguistics",
                "Programming language",
                "Philosophy"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Vision and Pattern Recognition (cs.CV)",
                "Computer Science - Information Retrieval",
                "Computation and Language (cs.CL)",
                "FOS: Computer and information sciences",
                "Computer Science - Computer Vision and Pattern Recognition",
                "Computer Science - Artificial Intelligence",
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Computation and Language",
                "Information Retrieval (cs.IR)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models",
        "doi": "10.48550/arxiv.2309.13345",
        "abstract": "Large language models (LLMs) have achieved dramatic proficiency over NLP tasks with normal length. Recently, multiple studies have committed to extending the context length and enhancing the long text modeling capabilities of LLMs. To comprehensively evaluate the long context ability of LLMs, we propose BAMBOO, a multi-task long context benchmark. BAMBOO has been designed with four principles: comprehensive capacity evaluation, avoidance of data contamination, accurate automatic evaluation, and different length levels. It consists of 10 datasets from 5 different long text understanding tasks, i.e. question answering, hallucination detection, text sorting, language modeling, and code completion, to cover core capacities and various domains of LLMs. We conduct experiments with five long context models on BAMBOO and further discuss four key research questions of long text. We also qualitatively analyze current long context models and point out future directions for enhancing long text modeling capacities. We release our data, prompts, and code at https://github.com/RUCAIBox/BAMBOO.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploration Of LLM Hallucination Benchmarks"
            ],
            "tasks": [
                "Task TypeDetection",
                "Task Inputpaper",
                "Task LabelSummary",
                "Task MetricPrecision"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Code Completion",
                "Hallucination",
                "Language Modeling",
                "Language Modelling",
                "Question Answering"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Context (archaeology)",
                "Benchmark (surveying)",
                "Computer science",
                "Code (set theory)",
                "Language model",
                "Task (project management)",
                "Sorting",
                "Natural language processing",
                "Point (geometry)",
                "Bamboo",
                "Context model",
                "Artificial intelligence",
                "Data science",
                "Geography",
                "Engineering",
                "Cartography",
                "Programming language",
                "Ecology",
                "Geometry",
                "Mathematics",
                "Archaeology",
                "Set (abstract data type)",
                "Systems engineering",
                "Object (grammar)",
                "Biology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency",
        "doi": "10.18653/v1/2023.findings-emnlp.1032",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploration Of LLM Hallucination Benchmarks"
            ],
            "tasks": [
                "Task TypeDetection",
                "Task InputQuestion",
                "Task LabelAnswer",
                "Task MetricAUROC"
            ]
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Adversarial Robustness in Machine Learning"
            ],
            "concepts": [
                "Computer science",
                "Consistency (knowledge bases)",
                "Causal consistency",
                "Black box",
                "Natural language processing",
                "Artificial intelligence",
                "Consistency model",
                "Sequential consistency",
                "Data consistency",
                "Operating system"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency",
        "doi": "",
        "abstract": "Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking. Our SAC3 approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploration Of LLM Hallucination Benchmarks"
            ],
            "tasks": [
                "Task TypeDetection",
                "Task InputQuestion",
                "Task LabelAnswer",
                "Task MetricAUROC"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Hallucination",
                "Question Answering"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [],
            "topics": [],
            "concepts": []
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "PURPLE: Making a Large Language Model a Better SQL Writer",
        "doi": "10.1109/icde60146.2024.00009",
        "abstract": "Large Language Model (LLM) techniques play an increasingly important role in Natural Language to SQL (NL2SQL) translation. LLMs trained by extensive corpora have strong natural language understanding and basic SQL generation abilities without additional tuning specific to NL2SQL tasks. Existing LLMs-based NL2SQL approaches try to improve the translation by enhancing the LLMs with an emphasis on user intention understanding. However, LLMs sometimes fail to generate appropriate SQL due to their lack of knowledge in organizing complex logical operator composition. A promising method is to input the LLMs with demonstrations, which include known NL2SQL translations from various databases. LLMs can learn to organize operator compositions from the input demonstrations for the given task. In this paper, we propose PURPLE (Pre-trained models Utilized to Retrieve Prompts for Logical Enhancement), which improves accuracy by retrieving demonstrations containing the requisite logical operator composition for the NL2SQL task on hand, thereby guiding LLMs to produce better SQL translation. PURPLE achieves a new state-of-the-art performance of 80.5% exact-set match accuracy and 87.8% execution match accuracy on the validation set of the popular NL2SQL benchmark Spider. PURPLE maintains high accuracy across diverse benchmarks, budgetary constraints, and various LLMs, showing robustness and cost-effectiveness.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Development And Evaluation of a SOTA LLM-based NL-To-SQL translation interface"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Language Modeling",
                "Language Modelling",
                "Large Language Model",
                "Natural Language Understanding",
                "Translation"
            ],
            "methods": [
                "SET"
            ],
            "main_collection_name": [
                "Sparsity"
            ],
            "main_collection_area": [
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Scientific Computing and Data Management"
            ],
            "topics": [
                "Scientific Computing and Data Management",
                "Computational Physics and Python Applications",
                "Distributed and Parallel Computing Systems"
            ],
            "concepts": [
                "Computer science",
                "SQL",
                "Programming language",
                "Natural language processing",
                "Artificial intelligence"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Databases (cs.DB)",
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Artificial Intelligence",
                "Computation and Language (cs.CL)",
                "FOS: Computer and information sciences",
                "Computer Science - Databases",
                "Computer Science - Computation and Language"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "PET-SQL: A Prompt-Enhanced Two-Round Refinement of Text-to-SQL with Cross-consistency",
        "doi": "",
        "abstract": "Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the prompt's schema information and instruct the LLM to produce the final SQL. Finally, as the post-refinement module, we propose using cross-consistency across different LLMs rather than self-consistency within a particular LLM. Our methods achieve new SOTA results on the Spider benchmark, with an execution accuracy of 87.6%.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Development And Evaluation of a SOTA LLM-based NL-To-SQL translation interface"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "In-Context Learning",
                "Text to SQL",
                "Text-To-SQL"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [],
            "topics": [],
            "concepts": []
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Metasql: A Generate-Then-Rank Framework for Natural Language to SQL Translation",
        "doi": "10.1109/icde60146.2024.00143",
        "abstract": "The Natural Language Interface to Databases (NLIDB) empowers non-technical users with database access through intuitive natural language (NL) interactions. Advanced approaches, utilizing neural sequence-to-sequence models or large-scale language models, typically employ auto-regressive decoding to generate unique SQL queries sequentially. While these translation models have greatly improved the overall translation accuracy, surpassing 70% on NLIDB benchmarks, the use of auto-regressive decoding to generate single SQL queries may result in sub-optimal outputs, potentially leading to erroneous translations. In this paper, we propose Metasql, a unified generate-then-rank framework that can be flexibly incorporated with existing NLIDBs to consistently improve their translation accuracy. Metasql introduces query metadata to control the generation of better SQL query candidates and uses learning-to-rank algorithms to retrieve globally optimized queries. Specifically, Metasql first breaks down the meaning of the given NL query into a set of possible query metadata, representing the basic concepts of the semantics. These metadata are then used as language constraints to steer the underlying translation model toward generating a set of candidate SQL queries. Finally, Metasql ranks the candidates to identify the best matching one for the given NL query. Extensive experiments are performed to study Metasql on two public NLIDB benchmarks. The results show that the performance of the translation models can be effectively improved using Metasql.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Development And Evaluation of a SOTA LLM-based NL-To-SQL translation interface"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Learning-To-Rank",
                "Translation"
            ],
            "methods": [
                "SET"
            ],
            "main_collection_name": [
                "Sparsity"
            ],
            "main_collection_area": [
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Scientific Computing and Data Management"
            ],
            "topics": [
                "Scientific Computing and Data Management",
                "Advanced Database Systems and Queries",
                "Mathematics, Computing, and Information Processing"
            ],
            "concepts": [
                "Computer science",
                "Rank (graph theory)",
                "SQL",
                "Programming language",
                "Natural language processing",
                "Translation (biology)",
                "Natural language",
                "Machine translation",
                "Natural (archaeology)",
                "Artificial intelligence",
                "Mathematics",
                "History",
                "Biochemistry",
                "Chemistry",
                "Archaeology",
                "Combinatorics",
                "Messenger RNA",
                "Gene"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Databases",
                "Computer Science - Artificial Intelligence"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments",
        "doi": "10.18653/v1/2024.emnlp-main.436",
        "abstract": "EMNLP'2024; 18 pages, 8 figures, 8 tables",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Development And Evaluation of a SOTA LLM-based NL-To-SQL translation interface"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": [
                "Linear Layer",
                "Layer Normalization",
                "Dense Connections",
                "Residual Connection",
                "Adam",
                "Position-Wise Feed-Forward Layer",
                "GPT-4",
                "Absolute Position Encodings",
                "BPE",
                "Dropout",
                "Label Smoothing",
                "Attention",
                "Softmax",
                "Multi-Head Attention",
                "Transformer"
            ],
            "main_collection_name": [
                "Attention Mechanisms",
                "Subword Segmentation",
                "Normalization",
                "Transformers",
                "Output Functions",
                "Position Embeddings",
                "Feedforward Networks",
                "Stochastic Optimization",
                "Attention Modules",
                "Regularization",
                "Skip Connections",
                "Language Models"
            ],
            "main_collection_area": [
                "General",
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Multi-Agent Systems and Negotiation"
            ],
            "topics": [
                "Multi-Agent Systems and Negotiation",
                "Natural Language Processing Techniques",
                "Semantic Web and Ontologies"
            ],
            "concepts": [
                "Middleware (distributed applications)",
                "Computer science",
                "Computer security",
                "Distributed computing"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Artificial Intelligence",
                "I.2.7",
                "Computer Science - Computation and Language",
                "Artificial Intelligence (cs.AI)",
                "FOS: Computer and information sciences",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "SQL-CRAFT: Text-to-SQL through Interactive Refinement and Enhanced\n  Reasoning",
        "doi": "10.48550/arxiv.2402.14851",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Development And Evaluation of a SOTA LLM-based NL-To-SQL translation interface"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Advanced Database Systems and Queries"
            ],
            "topics": [
                "Advanced Database Systems and Queries",
                "Semantic Web and Ontologies"
            ],
            "concepts": [
                "SQL",
                "Computer science",
                "Craft",
                "SQL/PSM",
                "Programming language",
                "Stored procedure",
                "Natural language processing",
                "Artificial intelligence",
                "Query by Example",
                "Information retrieval",
                "Art",
                "Search engine",
                "Visual arts",
                "Web search query"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM",
        "doi": "10.18653/v1/2024.findings-acl.653",
        "abstract": "Generating accurate SQL queries for user questions (text-to-SQL) has been a long-standing challenge since it requires a deep understanding of both the user's question and the corresponding database schema in order to retrieve the desired content accurately. Existing methods rely on the comprehensive capability of large language models (LLMs) to generate the SQL. However, some necessary knowledge is not explicitly included in the database schema and user question or has been learned by LLMs. Thus, the generated SQL of the knowledge-insufficient questions may be inaccurate, negatively influencing the text-to-SQL models' performance and robustness. To address this challenge, we propose the Knowledge-to-SQL framework, which employs tailored Data Expert LLM (DELLM) to provide helpful knowledge for all text-to-SQL models. Specifically, we introduce the detailed implementation of DELLM regarding table reading and the basic fine-tuning process. We further propose a Preference Learning via Database Feedback (PLDBF) strategy, refining the DELLM to generate more helpful knowledge for LLMs. Extensive experiments verify that DELLM can enhance the state-of-the-art approaches for text-to-SQL tasks. The corresponding code of DELLM is released for further research.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Development And Evaluation of a SOTA LLM-based NL-To-SQL translation interface"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Text to SQL",
                "Text-To-SQL"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Scientific Computing and Data Management"
            ],
            "topics": [
                "Scientific Computing and Data Management",
                "Advanced Database Systems and Queries",
                "Semantic Web and Ontologies"
            ],
            "concepts": [
                "Computer science",
                "SQL/PSM",
                "SQL",
                "PL/SQL",
                "Data definition language",
                "Stored procedure",
                "Database",
                "Data Transformation Services",
                "Query by Example",
                "World Wide Web",
                "Search engine",
                "Web search query"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL",
        "doi": "10.18653/v1/2024.findings-emnlp.65",
        "abstract": "Currently, the in-context learning method based on large language models (LLMs) has become the mainstream of text-to-SQL research. Previous works have discussed how to select demonstrations related to the user question from a human-labeled demonstration pool. However, human labeling suffers from the limitations of insufficient diversity and high labeling overhead. Therefore, in this paper, we discuss how to measure and improve the diversity of the demonstrations for text-to-SQL. We present a metric to measure the diversity of the demonstrations and analyze the insufficient of the existing labeled data by experiments. Based on the above discovery, we propose fusing iteratively for demonstrations (Fused) to build a high-diversity demonstration pool through human-free multiple-iteration synthesis, improving diversity and lowering label cost. Our method achieves an average improvement of 3.2% and 5.0% with and without human labeling on several mainstream datasets, which proves the effectiveness of Fused.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Development And Evaluation of a SOTA LLM-based NL-To-SQL translation interface"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Diversity",
                "In-Context Learning",
                "Text to SQL",
                "Text-To-SQL"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Advanced Database Systems and Queries"
            ],
            "topics": [
                "Advanced Database Systems and Queries",
                "Cloud Computing and Resource Management",
                "Scientific Computing and Data Management"
            ],
            "concepts": [
                "Computer science",
                "Diversity (politics)",
                "SQL",
                "Artificial intelligence",
                "Natural language processing",
                "Programming language",
                "Sociology",
                "Anthropology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computer Science - Computation and Language",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm",
        "doi": "10.18653/v1/2024.findings-acl.641",
        "abstract": "In-context learning of large-language models (LLMs) has achieved remarkable success in the field of natural language processing, while extensive case studies reveal that the single-step chain-of-thought prompting approach faces challenges such as attention diffusion and inadequate performance in complex tasks like text-to-SQL. To improve the contextual learning capabilities of LLMs in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the attention and problem-solving scope of LLMs through decomposition. Specifically, the information determination module for eliminating redundant information and the brand-new prompt structure based on problem classification greatly enhance the model's attention. Additionally, the inclusion of self-correction and active learning modules greatly expands the problem-solving scope of LLMs, hence improving the upper limit of LLM-based approaches. Extensive experiments conducted on three datasets demonstrate that our approach outperforms other methods by a significant margin. About 2-3 percentage point improvements compared to the existing baseline on the Spider Dev, Spider-Realistic, and Bird Dev datasets and new SOTA results on the Spider Test dataset are achieved. Our code is available on GitHub: \\url{https://github.com/FlyingFeather/DEA-SQL}.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Development And Evaluation of a SOTA LLM-based NL-To-SQL translation interface"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Active Learning",
                "In-Context Learning",
                "Text to SQL",
                "Text-To-SQL"
            ],
            "methods": [
                "Diffusion"
            ],
            "main_collection_name": [
                "Image Generation Models"
            ],
            "main_collection_area": [
                "Computer Vision"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Scientific Computing and Data Management"
            ],
            "topics": [
                "Scientific Computing and Data Management",
                "Advanced Computational Techniques and Applications",
                "Distributed and Parallel Computing Systems"
            ],
            "concepts": [
                "Computer science",
                "Workflow",
                "SQL",
                "Decomposition",
                "Software engineering",
                "Database",
                "Chemistry",
                "Organic chemistry"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computer Science - Computation and Language",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Introduction to Linked Data and Its Lifecycle on the Web",
        "doi": "10.1007/978-3-642-39784-4_1",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Semantic Web and Ontologies"
            ],
            "topics": [
                "Semantic Web and Ontologies",
                "Advanced Database Systems and Queries",
                "Biomedical Text Mining and Ontologies"
            ],
            "concepts": [
                "Linked data",
                "Computer science",
                "Semantic Web",
                "World Wide Web",
                "Set (abstract data type)",
                "Data quality",
                "Social Semantic Web",
                "Web standards",
                "Data science",
                "Information retrieval",
                "Web service",
                "Service (business)",
                "Economy",
                "Economics",
                "Programming language"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL (extended)",
        "doi": "",
        "abstract": "Text-to-SQL, the process of translating natural language into Structured Query Language (SQL), represents a transformative application of large language models (LLMs), potentially revolutionizing how humans interact with data. This paper introduces the SQL-PaLM framework, a comprehensive solution for understanding and enhancing Text-to-SQL using LLMs, using in the learning regimes of few-shot prompting and instruction fine-tuning. With few-shot prompting, we explore the effectiveness of consistency decoding with execution-based error filtering. With instruction fine-tuning, we delve deep in understanding the critical paradigms that influence the performance of tuned LLMs. In particular, we investigate how performance can be improved through expanded training data coverage and diversity, synthetic data augmentation, and integrating query-specific database content. We propose a test-time selection method to further refine accuracy by integrating SQL outputs from multiple paradigms with execution feedback as guidance. Additionally, we tackle the practical challenge of navigating intricate databases with a significant number of tables and columns, proposing efficient techniques for accurately selecting relevant database elements to enhance Text-to-SQL performance. Our holistic approach yields substantial advancements in Text-to-SQL, as demonstrated on two key public benchmarks, Spider and BIRD. Through comprehensive ablations and error analyses, we shed light on the strengths and weaknesses of our framework, offering valuable insights into Text-to-SQL's future work.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Development And Evaluation of a SOTA LLM-based NL-To-SQL translation interface"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Data Augmentation",
                "In-Context Learning",
                "Language Modeling",
                "Language Modelling",
                "Large Language Model",
                "Text to SQL",
                "Text-To-SQL"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [],
            "topics": [],
            "concepts": []
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems",
        "doi": "10.18653/v1/2024.naacl-long.20",
        "abstract": "Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.",
        "orkg categories": {
            "domains": [],
            "methods": [
                "ARES"
            ],
            "research problems": [
                "Exploration Of Fine-Tuning Methods in Single-LLM Evaluation in LLM-as-judges"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "RAG",
                "Retrieval",
                "Retrieval-augmented Generation"
            ],
            "methods": [
                "Dropout",
                "Dense Connections",
                "SET",
                "GELU",
                "Attention Dropout",
                "Linear Warmup With Linear Decay",
                "Residual Connection",
                "BART",
                "Attention",
                "WordPiece",
                "BERT",
                "Adam",
                "Linear Layer",
                "BPE",
                "Softmax",
                "Weight Decay",
                "RAG",
                "Multi-Head Attention",
                "Layer Normalization"
            ],
            "main_collection_name": [
                "Attention Mechanisms",
                "Learning Rate Schedules",
                "Feedforward Networks",
                "Language Models",
                "Activation Functions",
                "Regularization",
                "Normalization",
                "Subword Segmentation",
                "Stochastic Optimization",
                "Output Functions",
                "Transformers",
                "Sparsity",
                "Attention Modules",
                "Skip Connections"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Power Systems and Technologies"
            ],
            "topics": [
                "Power Systems and Technologies"
            ],
            "concepts": [
                "Computer science",
                "Information retrieval",
                "Computer architecture"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language",
                "Computer Science - Information Retrieval",
                "Computer Science - Artificial Intelligence",
                "Artificial Intelligence (cs.AI)",
                "Information Retrieval (cs.IR)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and\n  Evolution",
        "doi": "10.48550/arxiv.2410.16256",
        "abstract": "Technical Report, Code and Models: https://github.com/open-compass/CompassJudger",
        "orkg categories": {
            "domains": [],
            "methods": [
                "CompassJudger-1"
            ],
            "research problems": [
                "Exploration Of Fine-Tuning Methods in Single-LLM Evaluation in LLM-as-judges"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "All",
                "model"
            ],
            "methods": [
                "Softmax",
                "Attention"
            ],
            "main_collection_name": [
                "Output Functions",
                "Attention Mechanisms"
            ],
            "main_collection_area": [
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Simulation Techniques and Applications"
            ],
            "topics": [
                "Simulation Techniques and Applications",
                "Scientific Computing and Data Management",
                "Evolutionary Algorithms and Applications"
            ],
            "concepts": [
                "Computer science"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language",
                "FOS: Computer and information sciences",
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Artificial Intelligence"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Beyond Scalar Reward Model: Learning Generative Judge from Preference\n  Data",
        "doi": "10.48550/arxiv.2410.03742",
        "abstract": "Learning from preference feedback is a common practice for aligning large language models~(LLMs) with human value. Conventionally, preference data is learned and encoded into a scalar reward model that connects a value head with an LLM to produce a scalar score as preference or reward. However, scalar models lack interpretability and are known to be susceptible to biases in datasets. This paper investigates leveraging the generation capability of LLMs to address both limitations in one shot. Specifically, we prompt the pre-trained LLM to generate positive and negative judgments, both supported with rationales in natural language form. The self-generated contrastive judgment pairs are used to train the generative judge with Direct Preference Optimization (DPO). This proposal of training the generative Judge using self-generated Contrastive judgments (Con-J) ensures natural interpretability due to the generated rationales together with the judgments, as well as high robustness against bias without the need for an additional reward head. Experimental results show that the performance of Con-J is comparable to the scalar reward model trained on the same collection of preference data, and demonstrate its superior interpretability and robustness in encoding human preferences.",
        "orkg categories": {
            "domains": [],
            "methods": [
                "Con-J"
            ],
            "research problems": [
                "Exploration Of Fine-Tuning Methods in Single-LLM Evaluation in LLM-as-judges"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Bayesian Modeling and Causal Inference"
            ],
            "topics": [
                "Bayesian Modeling and Causal Inference",
                "Game Theory and Voting Systems",
                "Multi-Criteria Decision Making"
            ],
            "concepts": [
                "Generative grammar",
                "Preference",
                "Generative model",
                "Psychology",
                "Preference learning",
                "Artificial intelligence",
                "Computer science",
                "Economics",
                "Microeconomics"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Artificial Intelligence",
                "Computer Science - Computation and Language",
                "Computation and Language (cs.CL)",
                "Machine Learning (cs.LG)",
                "Computer Science - Machine Learning",
                "Artificial Intelligence (cs.AI)",
                "FOS: Computer and information sciences"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Défi TextMine 2025 : Utilisation des Grands Modèles de Langue pour l'Extraction de Relations dans les Rapports de Renseignement",
        "doi": "10.48550/arxiv.2407.21783",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Natural Language Processing Techniques"
            ],
            "topics": [
                "Natural Language Processing Techniques",
                "Topic Modeling",
                "Text Readability and Simplification"
            ],
            "concepts": [
                "Computer science",
                "Transformer",
                "Guard (computer science)",
                "Artificial intelligence",
                "Natural language processing",
                "Engineering",
                "Programming language",
                "Voltage",
                "Electrical engineering"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "The Llama 3 Herd of Models",
        "doi": "",
        "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Finding Pre-trained Large Language Model"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "answerability prediction",
                "Language Modeling",
                "Language Modelling",
                "Multi-task Language Understanding",
                "Question Answering",
                "speech-recognition",
                "Speech Recognition"
            ],
            "methods": [
                "Position-Wise Feed-Forward Layer",
                "Adam",
                "Residual Connection",
                "Layer Normalization",
                "Dropout",
                "Softmax",
                "Dense Connections",
                "BPE",
                "LLaMA",
                "Attention",
                "Multi-Head Attention",
                "Absolute Position Encodings",
                "Label Smoothing",
                "Transformer",
                "Linear Layer",
                "SET",
                "GPT-4"
            ],
            "main_collection_name": [
                "Stochastic Optimization",
                "Sparsity",
                "Attention Modules",
                "Transformers",
                "Language Models",
                "Position Embeddings",
                "Attention Mechanisms",
                "Output Functions",
                "Skip Connections",
                "Normalization",
                "Feedforward Networks",
                "Subword Segmentation",
                "Regularization"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [],
            "topics": [],
            "concepts": []
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and\n  Diversity of LLM Generated Ideas",
        "doi": "10.48550/arxiv.2410.14255",
        "abstract": "Scientific innovation is pivotal for humanity, and harnessing large language models (LLMs) to generate research ideas could transform discovery. However, existing LLMs often produce simplistic and repetitive suggestions due to their limited ability in acquiring external knowledge for innovation. To address this problem, we introduce an enhanced planning and search methodology designed to boost the creative potential of LLM-based systems. Our approach involves an iterative process to purposely plan the retrieval of external knowledge, progressively enriching the idea generation with broader and deeper insights. Validation through automated and human assessments indicates that our framework substantially elevates the quality of generated ideas, particularly in novelty and diversity. The number of unique novel ideas produced by our framework is 3.4 times higher than without it. Moreover, our method outperforms the current state-of-the-art, generating at least 2.5 times more top-rated ideas based on 170 seed papers in a Swiss Tournament evaluation.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Diversity"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Biomedical and Engineering Education"
            ],
            "topics": [
                "Biomedical and Engineering Education"
            ],
            "concepts": [
                "Novelty",
                "Diversity (politics)",
                "Computer science",
                "Psychology",
                "Sociology",
                "Social psychology",
                "Anthropology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language",
                "Computer Science - Artificial Intelligence",
                "Artificial Intelligence (cs.AI)",
                "FOS: Computer and information sciences"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Automating psychological hypothesis generation with AI: when large language models meet causal graph",
        "doi": "10.1057/s41599-024-03407-5",
        "abstract": "<jats:title>Abstract</jats:title><jats:p>Leveraging the synergy between causal knowledge graphs and a large language model (LLM), our study introduces a groundbreaking approach for computational hypothesis generation in psychology. We analyzed 43,312 psychology articles using a LLM to extract causal relation pairs. This analysis produced a specialized causal graph for psychology. Applying link prediction algorithms, we generated 130 potential psychological hypotheses focusing on “well-being”, then compared them against research ideas conceived by doctoral scholars and those produced solely by the LLM. Interestingly, our combined approach of a LLM and causal graphs mirrored the expert-level insights in terms of novelty, clearly surpassing the LLM-only hypotheses (<jats:italic>t</jats:italic>(59) = 3.34,<jats:italic>p</jats:italic> = 0.007 and<jats:italic>t</jats:italic>(59) = 4.32,<jats:italic>p</jats:italic> &lt; 0.001, respectively). This alignment was further corroborated using deep semantic analysis. Our results show that combining LLM with machine learning techniques such as causal knowledge graphs can revolutionize automated discovery in psychology, extracting novel insights from the extensive literature. This work stands at the crossroads of psychology and artificial intelligence, championing a new enriched paradigm for data-driven hypothesis generation in psychological research.</jats:p>",
        "orkg categories": {
            "domains": [],
            "methods": [
                "Causal pair extraction",
                "Combining causal KG with LLM",
                "semantic analysis",
                "Machine learning"
            ],
            "research problems": [
                "computational hypothesis generation",
                "psychological hypothesis generation"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Articles",
                "Knowledge Graphs",
                "Language Modeling",
                "Language Modelling",
                "Large Language Model",
                "Link Prediction"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Cognitive Science and Mapping"
            ],
            "topics": [
                "Cognitive Science and Mapping",
                "Mental Health Research Topics",
                "Cognitive Abilities and Testing"
            ],
            "concepts": [
                "Computer science",
                "Natural language processing",
                "Graph",
                "Artificial intelligence",
                "Psychology",
                "Theoretical computer science"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Artificial Intelligence",
                "Computer Science - Computers and Society",
                "Artificial Intelligence (cs.AI)",
                "History of scholarship and learning. The humanities",
                "Computers and Society (cs.CY)",
                "Social Sciences",
                "FOS: Computer and information sciences",
                "AZ20-999",
                "H"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Gemma: Open Models Based on Gemini Research and Technology",
        "doi": "10.48550/arxiv.2403.08295",
        "abstract": "This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.",
        "orkg categories": {
            "domains": [
                "Generic"
            ],
            "methods": [],
            "research problems": [
                "Small Language Models (SLMs) survey"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Multi-Agent Systems and Negotiation"
            ],
            "topics": [
                "Multi-Agent Systems and Negotiation"
            ],
            "concepts": [
                "Gemma",
                "Computer science",
                "Biology",
                "Botany"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computer Science - Computation and Language",
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Artificial Intelligence",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Gemma 2: Improving Open Language Models at a Practical Size",
        "doi": "10.48550/arxiv.2408.00118",
        "abstract": "In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3 times bigger. We release all our models to the community.",
        "orkg categories": {
            "domains": [
                "Generic"
            ],
            "methods": [],
            "research problems": [
                "Small Language Models (SLMs) survey"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Knowledge Distillation"
            ],
            "methods": [
                "BPE",
                "Label Smoothing",
                "Knowledge Distillation",
                "Absolute Position Encodings",
                "Dropout",
                "Linear Layer",
                "Adam",
                "Multi-Head Attention",
                "Dense Connections",
                "Residual Connection",
                "Attention",
                "Transformer",
                "Position-Wise Feed-Forward Layer",
                "Softmax",
                "Layer Normalization"
            ],
            "main_collection_name": [
                "Feedforward Networks",
                "Subword Segmentation",
                "Output Functions",
                "Regularization",
                "Attention Modules",
                "Normalization",
                "Knowledge Distillation",
                "Attention Mechanisms",
                "Transformers",
                "Stochastic Optimization",
                "Position Embeddings",
                "Skip Connections"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Natural Language Processing Techniques"
            ],
            "topics": [
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Gemma",
                "Computer science",
                "Biology",
                "Botany"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "Evaluación de procesos en Fisioterapia",
                "FOS: Computer and information sciences",
                "Calidad de la Atención de Salud",
                "Health Care Outcome Evaluation",
                "Outcome Evaluation in Health Care",
                "Computer Science - Artificial Intelligence",
                "Evaluación de Resultado en la Atención de Salud",
                "Health Care Quality Indicators",
                "Computation and Language (cs.CL)",
                "Farmacología & terapéutica",
                "Process evaluation in Physiotherapy",
                "Process Evaluation",
                "Modalidades de Fisioterapia",
                "Physical Therapy Modalities",
                "Indicators for the evaluation of the quality of care in physiotherapy",
                "Indicadores para la evaluación de la calidad de la atención en fisioterapia",
                "Artificial Intelligence (cs.AI)",
                "Indicadores de Calidad de la Atención de Salud"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable\n  Training Strategies",
        "doi": "10.48550/arxiv.2404.06395",
        "abstract": "",
        "orkg categories": {
            "domains": [
                "Generic"
            ],
            "methods": [],
            "research problems": [
                "Small Language Models (SLMs) survey"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Training (meteorology)",
                "Scalability",
                "Computer science",
                "Natural language processing",
                "Artificial intelligence",
                "Geography",
                "Database",
                "Meteorology"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your\n  Phone",
        "doi": "10.48550/arxiv.2404.14219",
        "abstract": "",
        "orkg categories": {
            "domains": [
                "Generic"
            ],
            "methods": [],
            "research problems": [
                "Small Language Models (SLMs) survey"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Computational Physics and Python Applications"
            ],
            "topics": [
                "Computational Physics and Python Applications"
            ],
            "concepts": [
                "Phone",
                "Computer science",
                "Linguistics",
                "Philosophy"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "OpenELM: An Efficient Language Model Family with Open-source Training\n  and Inference Framework",
        "doi": "10.48550/arxiv.2404.14619",
        "abstract": "The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring $2\\times$ fewer pre-training tokens. Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors. Our source code along with pre-trained model weights and training recipes is available at \\url{https://github.com/apple/corenet}. Additionally, \\model models can be found on HuggingFace at: \\url{https://huggingface.co/apple/OpenELM}.",
        "orkg categories": {
            "domains": [
                "Generic"
            ],
            "methods": [],
            "research problems": [
                "Small Language Models (SLMs) survey"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Language Modeling",
                "Language Modelling"
            ],
            "methods": [
                "Library"
            ],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling"
            ],
            "concepts": [
                "Computer science",
                "Inference",
                "Open source",
                "Training (meteorology)",
                "Natural language processing",
                "Artificial intelligence",
                "Programming language",
                "Geography",
                "Software",
                "Meteorology"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT",
        "doi": "10.48550/arxiv.2402.16840",
        "abstract": "Comment: Code available at : https://github.com/mbzuai-oryx/MobiLlama",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Advanced Data Storage Technologies"
            ],
            "topics": [
                "Advanced Data Storage Technologies",
                "Parallel Computing and Optimization Techniques"
            ],
            "concepts": [
                "Computer science"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "MobileLLM: Optimizing Sub-billion Parameter Language Models for\n  On-Device Use Cases",
        "doi": "10.48550/arxiv.2402.14905",
        "abstract": "ICML 2024. Code is available at https://github.com/facebookresearch/MobileLLM",
        "orkg categories": {
            "domains": [
                "Generic"
            ],
            "methods": [],
            "research problems": [
                "Small Language Models (SLMs) survey"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": [
                "Attention",
                "Feedforward Network",
                "Focus",
                "Softmax",
                "Grouped-query attention",
                "Dense Connections"
            ],
            "main_collection_name": [
                "Output Functions",
                "Attention",
                "Feedforward Networks",
                "Attention Mechanisms",
                "Transformers"
            ],
            "main_collection_area": [
                "General",
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Context-Aware Activity Recognition Systems"
            ],
            "topics": [
                "Context-Aware Activity Recognition Systems",
                "Multimedia Communication and Technology",
                "Green IT and Sustainability"
            ],
            "concepts": [
                "Computer science",
                "Econometrics",
                "Economics"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Machine Learning",
                "Computer Science - Computation and Language",
                "Computation and Language (cs.CL)",
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Artificial Intelligence",
                "Machine Learning (cs.LG)",
                "FOS: Computer and information sciences"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Compact Language Models via Pruning and Knowledge Distillation",
        "doi": "10.48550/arxiv.2407.14679",
        "abstract": "Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction (&lt;3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, we develop a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using our approach requires up to 40x fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8x for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. We have open-sourced Minitron model weights on Huggingface, with corresponding supplementary material including example code available on GitHub.",
        "orkg categories": {
            "domains": [
                "Generic"
            ],
            "methods": [],
            "research problems": [
                "Small Language Models (SLMs) survey"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Knowledge Distillation",
                "Language Modeling",
                "Language Modelling",
                "MMLU"
            ],
            "methods": [
                "Attention",
                "Pruning",
                "Softmax",
                "SET"
            ],
            "main_collection_name": [
                "Sparsity",
                "Model Compression",
                "Output Functions",
                "Attention Mechanisms"
            ],
            "main_collection_area": [
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Natural Language Processing Techniques"
            ],
            "topics": [
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Pruning",
                "Distillation",
                "Computer science",
                "Natural language processing",
                "Artificial intelligence",
                "Mathematics",
                "Chemistry",
                "Chromatography",
                "Botany",
                "Biology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Artificial Intelligence",
                "Computer Science - Machine Learning",
                "Computation and Language (cs.CL)",
                "Computer Vision and Pattern Recognition (cs.CV)",
                "02 engineering and technology",
                "Machine Learning (cs.LG)",
                "Computer Science - Computer Vision and Pattern Recognition",
                "Artificial Intelligence (cs.AI)",
                "0202 electrical engineering, electronic engineering, information engineering",
                "Computer Science - Computation and Language",
                "FOS: Computer and information sciences"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "doi": "10.48550/arxiv.2401.02954",
        "abstract": "The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": [
                "Linear Layer",
                "Attention Dropout",
                "Dropout",
                "Layer Normalization",
                "Linear Warmup With Cosine Annealing",
                "Softmax",
                "Cosine Annealing",
                "BASE",
                "Multi-Head Attention",
                "15 Ways to Contact How can i speak to someone at Delta Airlines",
                "Dense Connections",
                "Fixed Factorized Attention",
                "GPT-3",
                "Attention",
                "Residual Connection",
                "GELU",
                "Adam",
                "BPE",
                "Weight Decay"
            ],
            "main_collection_name": [
                "Normalization",
                "Active Learning",
                "Attention Patterns",
                "Activation Functions",
                "Transformers",
                "Learning Rate Schedules",
                "Attention Mechanisms",
                "Regularization",
                "Output Functions",
                "Stochastic Optimization",
                "Feedforward Networks",
                "Subword Segmentation",
                "Attention Modules",
                "Skip Connections"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Natural Language Processing Techniques"
            ],
            "topics": [
                "Natural Language Processing Techniques",
                "Topic Modeling",
                "Text Readability and Simplification"
            ],
            "concepts": [
                "Computer science",
                "Open source",
                "Scaling",
                "Perspective (graphical)",
                "Scaling law",
                "Code (set theory)",
                "Preference",
                "Key (lock)",
                "Data science",
                "Artificial intelligence",
                "Programming language",
                "Computer security",
                "Software",
                "Mathematics",
                "Statistics",
                "Geometry",
                "Set (abstract data type)"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "Computer Science - Artificial Intelligence",
                "Computer Science - Machine Learning"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts\n  Language Model",
        "doi": "10.48550/arxiv.2405.04434",
        "abstract": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Language Modeling",
                "Language Modelling",
                "Mixture-of-Experts",
                "Reinforcement Learning (RL)"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Expert finding and Q&A systems"
            ],
            "topics": [
                "Expert finding and Q&A systems",
                "Topic Modeling",
                "Speech and dialogue systems"
            ],
            "concepts": [
                "Computer science",
                "Natural language processing",
                "Linguistics",
                "Philosophy"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computation and Language (cs.CL)",
                "FOS: Computer and information sciences",
                "Computer Science - Artificial Intelligence",
                "Computer Science - Computation and Language",
                "Artificial Intelligence (cs.AI)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens\n  of context",
        "doi": "10.48550/arxiv.2403.05530",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Finding Pre-trained Large Language Model"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Semantic Web and Ontologies"
            ],
            "topics": [
                "Semantic Web and Ontologies"
            ],
            "concepts": [
                "Context (archaeology)",
                "Business",
                "Computer science",
                "Internet privacy",
                "Geography",
                "Archaeology"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All\n  Tools",
        "doi": "10.48550/arxiv.2406.12793",
        "abstract": "We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through https://github.com/THUDM and https://huggingface.co/THUDM.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "All",
                "GSM8K",
                "HumanEval",
                "Instruction Following",
                "Math",
                "MMLU"
            ],
            "methods": [
                "Attention",
                "BPE",
                "Absolute Position Encodings",
                "Dense Connections",
                "Softmax",
                "Layer Normalization",
                "Adam",
                "Multi-Head Attention",
                "GPT-4",
                "SET",
                "Linear Layer",
                "Residual Connection",
                "Label Smoothing",
                "Position-Wise Feed-Forward Layer",
                "Dropout",
                "Transformer"
            ],
            "main_collection_name": [
                "Attention Modules",
                "Subword Segmentation",
                "Skip Connections",
                "Transformers",
                "Output Functions",
                "Stochastic Optimization",
                "Language Models",
                "Feedforward Networks",
                "Regularization",
                "Normalization",
                "Sparsity",
                "Attention Mechanisms",
                "Position Embeddings"
            ],
            "main_collection_area": [
                "General",
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling"
            ],
            "concepts": [
                "Generalized linear model",
                "Mathematics",
                "Computer science",
                "Econometrics",
                "Statistics"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computer Science - Computation and Language",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Automatic structuring of radiology reports with on-premise open-source large language models",
        "doi": "10.1007/s00330-024-11074-y",
        "abstract": "<jats:title>Abstract</jats:title>           <jats:sec>             <jats:title>Objectives</jats:title>             <jats:p>Structured reporting enhances comparability, readability, and content detail. Large language models (LLMs) could convert free text into structured data without disrupting radiologists’ reporting workflow. This study evaluated an on-premise, privacy-preserving LLM for automatically structuring free-text radiology reports.</jats:p>           </jats:sec>           <jats:sec>             <jats:title>Materials and methods</jats:title>             <jats:p>We developed an approach to controlling the LLM output, ensuring the validity and completeness of structured reports produced by a locally hosted Llama-2-70B-chat model. A dataset with de-identified narrative chest radiograph (CXR) reports was compiled retrospectively. It included 202 English reports from a publicly available MIMIC-CXR dataset and 197 German reports from our university hospital. Senior radiologist prepared a detailed, fully structured reporting template with 48 question-answer pairs. All reports were independently structured by the LLM and two human readers. Bayesian inference (Markov chain Monte Carlo sampling) was used to estimate the distributions of Matthews correlation coefficient (MCC), with [−0.05, 0.05] as the region of practical equivalence (ROPE).</jats:p>           </jats:sec>           <jats:sec>             <jats:title>Results</jats:title>             <jats:p>The LLM generated valid structured reports in all cases, achieving an average MCC of 0.75 (94% HDI: 0.70–0.80) and F1 score of 0.70 (0.70–0.80) for English, and 0.66 (0.62–0.70) and 0.68 (0.64–0.72) for German reports, respectively. The MCC differences between LLM and humans were within ROPE for both languages: 0.01 (−0.05 to 0.07), 0.01 (−0.05 to 0.07) for English, and −0.01 (−0.07 to 0.05), 0.00 (−0.06 to 0.06) for German, indicating approximately comparable performance.</jats:p>           </jats:sec>           <jats:sec>             <jats:title>Conclusion</jats:title>             <jats:p>Locally hosted, open-source LLMs can automatically structure free-text radiology reports with approximately human accuracy. However, the understanding of semantics varied across languages and imaging findings.</jats:p>           </jats:sec>           <jats:sec>             <jats:title>Key Points</jats:title>             <jats:p>               <jats:bold>                 <jats:italic>Question</jats:italic>               </jats:bold>               <jats:italic>Why has structured reporting not been widely adopted in radiology despite clear benefits and how can we improve this?</jats:italic>             </jats:p>             <jats:p>               <jats:bold>                 <jats:italic>Findings</jats:italic>               </jats:bold>               <jats:italic>A locally hosted large language model successfully structured narrative reports, showing variation between languages and findings.</jats:italic>             </jats:p>             <jats:p>               <jats:bold>                 <jats:italic>Critical relevance</jats:italic>               </jats:bold>               <jats:italic>Structured reporting provides many benefits, but its integration into the clinical routine is limited. Automating the extraction of structured information from radiology reports enables the capture of structured data while allowing the radiologist to maintain their reporting workflow.</jats:italic>             </jats:p>           </jats:sec>",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Radiomics and Machine Learning in Medical Imaging",
                "Biomedical Text Mining and Ontologies"
            ],
            "concepts": [
                "Structuring",
                "Medicine",
                "Interventional radiology",
                "Neuroradiology",
                "Radiology",
                "Premise",
                "Medical physics",
                "Linguistics",
                "Philosophy",
                "Finance",
                "Neurology",
                "Psychiatry",
                "Economics"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Natural Language Processing",
                "Radiology",
                "Imaging Informatics and Artificial Intelligence",
                "Radiology Information Systems",
                "Radiography, Thoracic",
                "Humans",
                "Retrospective Studies"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
        "doi": "10.48550/arxiv.2403.00827",
        "abstract": "It is often desirable for Large Language Models (LLMs) to capture multiple objectives when providing a response. In document-grounded response generation, for example, agent responses are expected to be relevant to a user's query while also being grounded in a given document. In this paper, we introduce Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response. ProMiSe leverages feedback on response quality through principle-specific proxy metrics, and iteratively refines its response one principle at a time. We apply ProMiSe to open source language models Flan-T5-XXL and Llama-2-13B-Chat, to evaluate its performance on document-grounded question answering datasets, MultiDoc2Dial and QuAC, demonstrating that self-refinement improves response quality. We further show that fine-tuning Llama-2-13B-Chat on the synthetic dialogue data generated by ProMiSe yields significant performance improvements over the zero-shot baseline as well as a supervised fine-tuned model on human annotated data.",
        "orkg categories": {
            "domains": [],
            "methods": [
                "ProMiSe"
            ],
            "research problems": [
                "Exploring Self-Refine Methods in Large Language Models"
            ],
            "tasks": [
                "QuAC",
                "MultiDoc2Dia"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Question Answering",
                "Response Generation"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Natural Language Processing Techniques"
            ],
            "topics": [
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Proxy (statistics)",
                "Computer science",
                "Econometrics",
                "Economics",
                "Machine learning"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Artificial Intelligence",
                "Computer Science - Machine Learning",
                "Computer Science - Computation and Language"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "CYCLE: Learning to Self-Refine the Code Generation",
        "doi": "10.1145/3649825",
        "abstract": "<jats:p>Pre-trained code language models have achieved promising performance in code generation and improved the programming efficiency of human developers. However, their self-refinement capability is typically overlooked by the existing evaluations of code LMs, which focus only on the accuracy of the one-time prediction. For the cases when code LMs fail to implement the correct program, developers actually find it hard to debug and fix the faulty prediction since it is not written by the developers themselves. Unfortunately, our study reveals that code LMs cannot efficiently self-refine their faulty generations as well.</jats:p>           <jats:p>In this paper, we propose CYCLE framework, learning to self-refine the faulty generation according to the available feedback, such as the execution results reported by the test suites. We evaluate CYCLE on three popular code generation benchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE successfully maintains, sometimes improves, the quality of one-time code generation, while significantly improving the self-refinement capability of code LMs. We implement four variants of CYCLE with varied numbers of parameters across 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently boosts the code generation performance, by up to 63.5</jats:p>",
        "orkg categories": {
            "domains": [],
            "methods": [
                "CYCLE"
            ],
            "research problems": [
                "Exploring Self-Refine Methods in Large Language Models"
            ],
            "tasks": [
                "Apps",
                "MBPP-S",
                "HumanEval"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Code Generation",
                "HumanEval",
                "mbpp"
            ],
            "methods": [
                "Focus"
            ],
            "main_collection_name": [
                "Transformers"
            ],
            "main_collection_area": [
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Model-Driven Software Engineering Techniques"
            ],
            "topics": [
                "Model-Driven Software Engineering Techniques",
                "Software Testing and Debugging Techniques",
                "Software Engineering Research"
            ],
            "concepts": [
                "Code (set theory)",
                "Computer science",
                "Programming language",
                "Set (abstract data type)"
            ]
        },
        "openaire categories": {
            "subjects": [
                "0202 electrical engineering, electronic engineering, information engineering",
                "FOS: Computer and information sciences",
                "Computer Science - Software Engineering",
                "02 engineering and technology",
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language",
                "Software Engineering (cs.SE)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving",
        "doi": "10.18653/v1/2024.emnlp-main.172",
        "abstract": "Camera-ready for EMNLP 2024",
        "orkg categories": {
            "domains": [],
            "methods": [
                "Exp-Refiner"
            ],
            "research problems": [
                "Exploring Self-Refine Methods in Large Language Models"
            ],
            "tasks": [
                "WorldTree",
                "QASC",
                "e-SNLI"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Automated Theorem Proving",
                "Natural Language Inference"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Natural Language Processing Techniques"
            ],
            "topics": [
                "Natural Language Processing Techniques",
                "Semantic Web and Ontologies"
            ],
            "concepts": [
                "Computer science",
                "Natural language",
                "Automated theorem proving",
                "Programming language",
                "Natural (archaeology)",
                "Algebra over a field",
                "Calculus (dental)",
                "Mathematics",
                "Natural language processing",
                "Pure mathematics",
                "Medicine",
                "Archaeology",
                "Dentistry",
                "History"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "FOS: Computer and information sciences",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "DAQ: Density-Aware Post-Training Weight-Only Quantization For LLMs",
        "doi": "10.48550/arxiv.2410.12187",
        "abstract": "Large language models (LLMs) excel in various tasks but face deployment challenges due to hardware constraints. We propose density-aware post-training weight-only quantization (DAQ), which has two stages: 1) density-centric alignment, which identifies the center of high-density weights and centers the dynamic range on this point to align high-density weight regions with floating-point high-precision regions; 2) learnable dynamic range adjustment, which adjusts the dynamic range by optimizing quantization parameters (i.e., scale and zero-point) based on the impact of weights on the model output. Experiments on LLaMA and LLaMA-2 show that DAQ consistently outperforms the best baseline method, reducing perplexity loss by an average of 22.8% on LLaMA and 19.6% on LLaMA-2. Our code is available at https://github.com/LuoYingSong/DAQ.",
        "orkg categories": {
            "domains": [],
            "methods": [
                "DAQ"
            ],
            "research problems": [
                "Weight-only Quantization Methods for LLMs"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Quantization"
            ],
            "methods": [
                "LLaMA",
                "ALIGN"
            ],
            "main_collection_name": [
                "Vision and Language Pre-Trained Models",
                "Language Models"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "Computer Vision"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Medical Image Segmentation Techniques"
            ],
            "topics": [
                "Medical Image Segmentation Techniques",
                "Advanced Image and Video Retrieval Techniques",
                "Medical Imaging Techniques and Applications"
            ],
            "concepts": [
                "Quantization (signal processing)",
                "Training (meteorology)",
                "Computer science",
                "Geography",
                "Meteorology",
                "Algorithm"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Machine Learning",
                "Artificial Intelligence (cs.AI)",
                "FOS: Computer and information sciences",
                "Machine Learning (cs.LG)",
                "Computer Science - Artificial Intelligence"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "MobileQuant: Mobile-friendly Quantization for On-device Language Models",
        "doi": "10.48550/arxiv.2408.13933",
        "abstract": "Large language models (LLMs) have revolutionized language processing, delivering outstanding results across multiple applications. However, deploying LLMs on edge devices poses several challenges with respect to memory, energy, and compute costs, limiting their widespread use in devices such as mobile phones. A promising solution is to reduce the number of bits used to represent weights and activations. While existing works have found partial success at quantizing LLMs to lower bitwidths, e.g. 4-bit weights, quantizing activations beyond 16 bits often leads to large computational overheads due to poor on-device quantization support, or a considerable accuracy drop. Yet, 8-bit activations are very attractive for on-device deployment as they would enable LLMs to fully exploit mobile-friendly hardware, e.g. Neural Processing Units (NPUs). In this work, we make a first attempt to facilitate the on-device deployment of LLMs using integer-only quantization. We first investigate the limitations of existing quantization methods for on-device deployment, with a special focus on activation quantization. We then address these limitations by introducing a simple post-training quantization method, named MobileQuant, that extends previous weight equivalent transformation works by jointly optimizing the weight transformation and activation range parameters in an end-to-end manner. MobileQuant demonstrates superior capabilities over existing methods by 1) achieving near-lossless quantization on a wide range of LLM benchmarks, 2) reducing latency and energy consumption by 20\\%-50\\% compared to current on-device quantization strategies, 3) requiring limited compute budget, 4) being compatible with mobile-friendly compute units, e.g. NPU.",
        "orkg categories": {
            "domains": [],
            "methods": [
                "MobileQuant"
            ],
            "research problems": [
                "Weight-only Quantization Methods for LLMs"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Quantization"
            ],
            "methods": [
                "Focus"
            ],
            "main_collection_name": [
                "Transformers"
            ],
            "main_collection_area": [
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Speech and dialogue systems"
            ],
            "topics": [
                "Speech and dialogue systems",
                "Recommender Systems and Techniques",
                "Topic Modeling"
            ],
            "concepts": [
                "Quantization (signal processing)",
                "Computer science",
                "Environmentally friendly",
                "Mobile device",
                "World Wide Web",
                "Computer vision",
                "Biology",
                "Ecology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language",
                "FOS: Computer and information sciences"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "GWQ: Gradient-Aware Weight Quantization for Large Language Models",
        "doi": "10.48550/arxiv.2411.00850",
        "abstract": "Large language models (LLMs) show impressive performance in solving complex language tasks. However, its large number of parameters presents significant challenges for the deployment. So, compressing LLMs to low bits can enable to deploy on resource-constrained devices. To address this problem, we propose gradient-aware weight quantization (GWQ), the first quantization approach for low-bit weight quantization that leverages gradients to localize outliers, requiring only a minimal amount of calibration data for outlier detection. GWQ retains the top 1\\% outliers preferentially at FP16 precision, while the remaining non-outlier weights are stored in a low-bit. We widely evaluate GWQ on different task include language modeling, grounding detection, massive multitask language understanding and vision-language question and answering. Results show that models quantified by GWQ performs better than other quantization method. During quantization process, GWQ only need one calibration set to realize effective quant. Also, GWQ achieves 1.2x inference speedup in comparison to the original model and effectively reduces the inference memory.",
        "orkg categories": {
            "domains": [],
            "methods": [
                "GWQ"
            ],
            "research problems": [
                "Weight-only Quantization Methods for LLMs"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Outlier Detection",
                "Quantization"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Quantization (signal processing)",
                "Computer science",
                "Artificial intelligence",
                "Algorithm"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computer Science - Machine Learning",
                "Artificial Intelligence (cs.AI)",
                "Machine Learning (cs.LG)",
                "Computer Science - Artificial Intelligence",
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective\n  Weight-Activation Quantization",
        "doi": "10.48550/arxiv.2407.08044",
        "abstract": "Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient Fine-Tuning (PEFT)method, significantly enhances the training efficiency by updating only a small portion of the weights in Large Language Models (LLMs). Recently, weight-only quantization techniques have also been applied to LoRA methods to reduce the memory footprint of fine-tuning. However, applying weight-activation quantization to the LoRA pipeline is under-explored, and we observe substantial performance degradation primarily due to the presence of activation outliers. In this work, we propose RoLoRA, the first LoRA-based scheme for effective weight-activation quantization. RoLoRA utilizes rotation for outlier elimination and proposes rotation-aware fine-tuning to preserve the outlier-free characteristics in rotated LLMs. Experimental results show RoLoRA consistently improves low-bit LoRA convergence and post-training quantization robustness in weight-activation settings. We evaluate RoLoRA across LLaMA2-7B/13B, LLaMA3-8B models, achieving up to 29.5% absolute accuracy gain of 4-bit weight-activation quantized LLaMA2- 13B on commonsense reasoning tasks compared to LoRA baseline. We further demonstrate its effectiveness on Large Multimodal Models (LLaVA-1.5-7B). Codes are available at https://github.com/HuangOwen/RoLoRA",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "parameter-efficient fine-tuning",
                "Quantization"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "CCD and CMOS Imaging Sensors"
            ],
            "topics": [
                "CCD and CMOS Imaging Sensors",
                "Analytical Chemistry and Sensors",
                "Advanced MRI Techniques and Applications"
            ],
            "concepts": [
                "Outlier",
                "Quantization (signal processing)",
                "Fine-tuning",
                "Computer science",
                "Artificial intelligence",
                "Algorithm",
                "Physics",
                "Particle physics"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Artificial Intelligence",
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language",
                "FOS: Computer and information sciences",
                "Machine Learning (cs.LG)",
                "Computer Science - Machine Learning",
                "Artificial Intelligence (cs.AI)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "HotaQ: Hardware Oriented Token Adaptive Quantization for Large Language Models",
        "doi": "10.1109/tcad.2024.3487781",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Security token",
                "Computer science",
                "Quantization (signal processing)",
                "Computer hardware",
                "Programming language",
                "Computer network",
                "Algorithm"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Random Contrastive Interaction for Particle Swarm Optimization in High-Dimensional Environment",
        "doi": "10.1109/tevc.2023.3277501",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Metaheuristic Optimization Algorithms Research"
            ],
            "topics": [
                "Metaheuristic Optimization Algorithms Research",
                "Advanced Multi-Objective Optimization Algorithms",
                "Vehicle Routing Optimization Methods"
            ],
            "concepts": [
                "Particle swarm optimization",
                "Scalability",
                "Computer science",
                "Convergence (economics)",
                "Network topology",
                "Multi-swarm optimization",
                "Mathematical optimization",
                "Evolutionary algorithm",
                "Swarm behaviour",
                "Evolutionary computation",
                "Topology (electrical circuits)",
                "Fitness landscape",
                "Artificial intelligence",
                "Mathematics",
                "Machine learning",
                "Population",
                "Demography",
                "Combinatorics",
                "Database",
                "Sociology",
                "Economics",
                "Economic growth",
                "Operating system"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Human resource management in the age of generative artificial intelligence: Perspectives and research directions on ChatGPT",
        "doi": "10.1111/1748-8583.12524",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Artificial Intelligence in Healthcare and Education"
            ],
            "topics": [
                "Artificial Intelligence in Healthcare and Education",
                "AI and HR Technologies",
                "Ethics and Social Impacts of AI"
            ],
            "concepts": [
                "Generative grammar",
                "Context (archaeology)",
                "Stakeholder",
                "Scholarship",
                "Realm",
                "Knowledge management",
                "Sociology",
                "Engineering ethics",
                "Political science",
                "Artificial intelligence",
                "Public relations",
                "Computer science",
                "Engineering",
                "Law",
                "Paleontology",
                "Biology"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Augmenting the Author: Exploring the Potential of AI Collaboration in\n  Academic Writing",
        "doi": "10.48550/arxiv.2404.16071",
        "abstract": "5 pages, workshop paper, CHI 2024 conference GENAI",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Artificial Intelligence in Healthcare and Education"
            ],
            "topics": [
                "Artificial Intelligence in Healthcare and Education"
            ],
            "concepts": [
                "Computer science",
                "Political science",
                "Sociology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "HCI Research Tools",
                "Human-Computer Interaction (cs.HC)",
                "Generative AI",
                "FOS: Computer and information sciences",
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Human-Computer Interaction",
                "Natural Language Processing",
                "Computer Science - Artificial Intelligence",
                "AI"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers",
        "doi": "10.48550/arxiv.2406.17343",
        "abstract": "Recent advancements in diffusion models, particularly the architectural transformation from UNet-based models to Diffusion Transformers (DiTs), significantly improve the quality and scalability of image and video generation. However, despite their impressive capabilities, the substantial computational costs of these large-scale models pose significant challenges for real-world deployment. Post-Training Quantization (PTQ) emerges as a promising solution, enabling model compression and accelerated inference for pretrained models, without the costly retraining. However, research on DiT quantization remains sparse, and existing PTQ frameworks, primarily designed for traditional diffusion models, tend to suffer from biased quantization, leading to notable performance degradation. In this work, we identify that DiTs typically exhibit significant spatial variance in both weights and activations, along with temporal variance in activations. To address these issues, we propose Q-DiT, a novel approach that seamlessly integrates two key techniques: automatic quantization granularity allocation to handle the significant variance of weights and activations across input channels, and sample-wise dynamic activation quantization to adaptively capture activation changes across both timesteps and samples. Extensive experiments conducted on ImageNet and VBench demonstrate the effectiveness of the proposed Q-DiT. Specifically, when quantizing DiT-XL/2 to W6A8 on ImageNet ($256 \\times 256$), Q-DiT achieves a remarkable reduction in FID by 1.09 compared to the baseline. Under the more challenging W4A8 setting, it maintains high fidelity in image and video generation, establishing a new benchmark for efficient, high-quality quantization in DiTs. Code is available at \\href{https://github.com/Juanerx/Q-DiT}{https://github.com/Juanerx/Q-DiT}.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Image Generation",
                "Model Compression",
                "Quantization",
                "Video Generation"
            ],
            "methods": [
                "Dense Connections",
                "Label Smoothing",
                "Transformer",
                "Softmax",
                "Layer Normalization",
                "Attention",
                "BPE",
                "Linear Layer",
                "Absolute Position Encodings",
                "Multi-Head Attention",
                "Adam",
                "Dropout",
                "Position-Wise Feed-Forward Layer",
                "Diffusion",
                "Residual Connection"
            ],
            "main_collection_name": [
                "Position Embeddings",
                "Feedforward Networks",
                "Attention Mechanisms",
                "Transformers",
                "Stochastic Optimization",
                "Skip Connections",
                "Attention Modules",
                "Image Generation Models",
                "Normalization",
                "Output Functions",
                "Subword Segmentation",
                "Regularization"
            ],
            "main_collection_area": [
                "General",
                "Natural Language Processing",
                "Computer Vision"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Advanced Memory and Neural Computing"
            ],
            "topics": [
                "Advanced Memory and Neural Computing",
                "Neural Networks and Reservoir Computing",
                "Neural Networks and Applications"
            ],
            "concepts": [
                "Transformer",
                "Quantization (signal processing)",
                "Computer science",
                "Electrical engineering",
                "Engineering",
                "Algorithm",
                "Voltage"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Vision and Pattern Recognition (cs.CV)",
                "Computer Science - Artificial Intelligence",
                "FOS: Computer and information sciences",
                "Computer Science - Computer Vision and Pattern Recognition",
                "Artificial Intelligence (cs.AI)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
        "doi": "10.48550/arxiv.2401.18079",
        "abstract": "LLMs are seeing growing use for applications which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in sub-4-bit precision. Our work, KVQuant, facilitates low precision KV cache quantization by incorporating several novel methods: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral models, we achieve &lt; 0.1 perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for KVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B model.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "KV-Cache Quantization (KVQ) methods for LLMs"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Quantization"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Natural Language Processing Techniques"
            ],
            "topics": [
                "Natural Language Processing Techniques",
                "Algorithms and Data Compression",
                "Web Data Mining and Analysis"
            ],
            "concepts": [
                "Cache",
                "Inference",
                "Quantization (signal processing)",
                "Context (archaeology)",
                "Computer science",
                "Parallel computing",
                "Algorithm",
                "Artificial intelligence",
                "History",
                "Archaeology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "0209 industrial biotechnology",
                "0202 electrical engineering, electronic engineering, information engineering",
                "Computer Science - Machine Learning",
                "Machine Learning (cs.LG)",
                "FOS: Computer and information sciences",
                "02 engineering and technology"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "WKVQuant: Quantizing Weight and Key/Value Cache for Large Language\n  Models Gains More",
        "doi": "10.48550/arxiv.2402.12065",
        "abstract": "Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for parameter optimization. Experiments show that WKVQuant achieves almost comparable memory savings to weight-activation quantization, while also approaching the performance of weight-only quantization.",
        "orkg categories": {
            "domains": [],
            "methods": [
                "WKVQuant"
            ],
            "research problems": [
                "KV-Cache Quantization (KVQ) methods for LLMs"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Quantization",
                "Text Generation"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Key (lock)",
                "Cache",
                "Value (mathematics)",
                "Computer science",
                "Parallel computing",
                "Computer security",
                "Machine learning"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers",
        "doi": "10.48550/arxiv.2407.09413",
        "abstract": "Seeking answers to questions within long scientific research articles is a crucial area of study that aids readers in quickly addressing their inquiries. However, existing question-answering (QA) datasets based on scientific papers are limited in scale and focus solely on textual content. We introduce SPIQA (Scientific Paper Image Question Answering), the first large-scale QA dataset specifically designed to interpret complex figures and tables within the context of scientific research articles across various domains of computer science. Leveraging the breadth of expertise and ability of multimodal large language models (MLLMs) to understand figures, we employ automatic and manual curation to create the dataset. We craft an information-seeking task on interleaved images and text that involves multiple images covering plots, charts, tables, schematic diagrams, and result visualizations. SPIQA comprises 270K questions divided into training, validation, and three different evaluation splits. Through extensive experiments with 12 prominent foundational models, we evaluate the ability of current multimodal systems to comprehend the nuanced aspects of research articles. Additionally, we propose a Chain-of-Thought (CoT) evaluation strategy with in-context retrieval that allows fine-grained, step-by-step assessment and improves model performance. We further explore the upper bounds of performance enhancement with additional textual information, highlighting its promising potential for future research and the dataset's impact on revolutionizing how we interact with scientific literature.",
        "orkg categories": {
            "domains": [
                "Computer science"
            ],
            "methods": [],
            "research problems": [
                "scientific question answering"
            ],
            "tasks": [
                "Figure/table understanding"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Articles",
                "Question Answering",
                "Visual Question Answering (VQA)"
            ],
            "methods": [
                "Focus"
            ],
            "main_collection_name": [
                "Transformers"
            ],
            "main_collection_area": [
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Advanced Text Analysis Techniques",
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Question answering",
                "Computer science",
                "Information retrieval"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Vision and Pattern Recognition (cs.CV)",
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Artificial Intelligence",
                "FOS: Computer and information sciences",
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language",
                "Computer Science - Computer Vision and Pattern Recognition"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Artificial intelligence for literature reviews: opportunities and challenges",
        "doi": "10.1007/s10462-024-10902-3",
        "abstract": "I have added all the three tables to the review",
        "orkg categories": {
            "domains": [
                "Any"
            ],
            "methods": [
                "Support Vector Machine"
            ],
            "research problems": [
                "AI in systematic literature review"
            ],
            "tasks": [
                "Paper classification"
            ]
        },
        "papers with code categories": {
            "tasks": [],
            "methods": [
                "SLR"
            ],
            "main_collection_name": [
                "Optimization"
            ],
            "main_collection_area": [
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Explainable Artificial Intelligence (XAI)"
            ],
            "topics": [
                "Explainable Artificial Intelligence (XAI)",
                "Topic Modeling",
                "Meta-analysis and systematic reviews"
            ],
            "concepts": [
                "Computer science",
                "Usability",
                "Leverage (statistics)",
                "Systematic review",
                "Field (mathematics)",
                "Automation",
                "Data science",
                "Transparency (behavior)",
                "Artificial intelligence",
                "Best practice",
                "Software engineering",
                "Management science",
                "Human–computer interaction",
                "Engineering",
                "Mechanical engineering",
                "Mathematics",
                "Computer security",
                "MEDLINE",
                "Management",
                "Political science",
                "Pure mathematics",
                "Law",
                "Economics"
            ]
        },
        "openaire categories": {
            "subjects": [
                "mental health technology",
                "Human-Computer Interaction (cs.HC)",
                "0503 education",
                "sensors",
                "artificial intelligence",
                "Personalization",
                "Medical technology",
                "Systematic review",
                "Social Sciences",
                "Systematic Literature Review",
                "RC435-571",
                "02 engineering and technology",
                "psychiatric diagnosis",
                "Computer Science - Information Retrieval",
                "Medical Technology",
                "Psychiatry",
                "Artificial Intelligence",
                "Biomedical",
                "R855-855.5",
                "Electronic computers. Computer science",
                "Assessment",
                "0302 clinical medicine",
                "L",
                "elementary education",
                "Education",
                "0501 psychology and cognitive sciences",
                "Artificial intelligence; Evaluation framework; Large language models; Literature review; Natural anguage processing; Systematic literature reviews; Usability;",
                "Learning",
                "Artificial intelligence in education",
                "AI",
                "design",
                "0202 electrical engineering, electronic engineering, information engineering",
                "Computer Science - Artificial Intelligence",
                "05 social sciences",
                "Science & Technology",
                "Engineering",
                "E-Government",
                "Humans",
                "H",
                "Government Services",
                "Challenges",
                "QA75.5-76.95",
                "Information Retrieval (cs.IR)",
                "Voice Disorders",
                "Emergency Department",
                "Opportunities",
                "Technology",
                "machine learning",
                "Teaching",
                "Computer Science - Human-Computer Interaction",
                "FOS: Computer and information sciences",
                "Artificial Intelligence, Cloud Engineering, Resource Allocation, Decision Systems, Optimization",
                "03 medical and health sciences",
                "Artificial intelligence",
                "personalized psychiatry",
                "0502 economics and business",
                "Artificial Intelligence (cs.AI)",
                "literature review"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "SciDQA: A Deep Reading Comprehension Dataset over Scientific Papers",
        "doi": "10.18653/v1/2024.emnlp-main.1163",
        "abstract": "18 pages, Accepted to EMNLP 2024",
        "orkg categories": {
            "domains": [
                "Scientific papers"
            ],
            "methods": [],
            "research problems": [
                "scientific question answering"
            ],
            "tasks": [
                "Deep reading and multi-document QA"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Articles",
                "Question Answering",
                "Reading Comprehension"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Computer science",
                "Reading (process)",
                "Artificial intelligence",
                "Reading comprehension",
                "Comprehension",
                "Natural language processing",
                "Data science",
                "Information retrieval",
                "Linguistics",
                "Programming language",
                "Philosophy"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "FOS: Computer and information sciences",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "SciQAG: A Framework for Auto-Generated Science Question Answering Dataset with Fine-grained Evaluation",
        "doi": "",
        "abstract": "We introduce SciQAG, a novel framework for automatically generating high-quality science question-answer pairs from a large corpus of scientific literature based on large language models (LLMs). SciQAG consists of a QA generator and a QA evaluator, which work together to extract diverse and research-level questions and answers from scientific papers. Utilizing this framework, we construct a large-scale, high-quality, open-ended science QA dataset containing 188,042 QA pairs extracted from 22,743 scientific papers across 24 scientific domains. We also introduce SciQAG-24D, a new benchmark task designed to evaluate the science question-answering ability of LLMs. Extensive experiments demonstrate that fine-tuning LLMs on the SciQAG dataset significantly improves their performance on both open-ended question answering and scientific tasks. To foster research and collaboration, we make the datasets, models, and evaluation codes publicly available, contributing to the advancement of science question answering and developing more interpretable and reasoning-capable AI systems.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Open-Ended Question Answering",
                "Question Answering",
                "Science Question Answering"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [],
            "topics": [],
            "concepts": []
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "The WEKA data mining software",
        "doi": "10.1145/1656274.1656278",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Data Mining Algorithms and Applications"
            ],
            "topics": [
                "Data Mining Algorithms and Applications",
                "Advanced Database Systems and Queries",
                "Data Management and Algorithms"
            ],
            "concepts": [
                "Computer science",
                "Workbench",
                "Data science",
                "Software",
                "Data mining",
                "Open source software",
                "World Wide Web",
                "Software engineering",
                "Operating system",
                "Visualization"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Integrating Large Language Models and Knowledge Graphs for Extraction and Validation of Textual Test Data",
        "doi": "",
        "abstract": "Aerospace manufacturing companies, such as Thales Alenia Space, design, develop, integrate, verify, and validate products characterized by high complexity and low volume. They carefully document all phases for each product but analyses across products are challenging due to the heterogeneity and unstructured nature of the data in documents. In this paper, we propose a hybrid methodology that leverages Knowledge Graphs (KGs) in conjunction with Large Language Models (LLMs) to extract and validate data contained in these documents. We consider a case study focused on test data related to electronic boards for satellites. To do so, we extend the Semantic Sensor Network ontology. We store the metadata of the reports in a KG, while the actual test results are stored in parquet accessible via a Virtual Knowledge Graph. The validation process is managed using an LLM-based approach. We also conduct a benchmarking study to evaluate the performance of state-of-the-art LLMs in executing this task. Finally, we analyze the costs and benefits of automating preexisting processes of manual data extraction and validation for subsequent cross-report analyses.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Benchmarking",
                "Knowledge Graphs"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [],
            "topics": [],
            "concepts": []
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Artificial Intelligence (cs.AI)",
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language",
                "Computer Science - Artificial Intelligence"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Leveraging LLM based Retrieval-Augmented Generation for Legal Knowledge Graph Completion",
        "doi": "10.1109/dsc63484.2024.00033",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Artificial Intelligence in Law"
            ],
            "topics": [
                "Artificial Intelligence in Law",
                "Topic Modeling",
                "Semantic Web and Ontologies"
            ],
            "concepts": [
                "Computer science",
                "Knowledge graph",
                "Graph",
                "Information retrieval",
                "Artificial intelligence",
                "Theoretical computer science"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "A Genome-wide CRISPR Screen in Toxoplasma Identifies Essential Apicomplexan Genes",
        "doi": "10.1016/j.cell.2016.08.019",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Toxoplasma gondii Research Studies"
            ],
            "topics": [
                "Toxoplasma gondii Research Studies",
                "Mosquito-borne diseases and control",
                "Herpesvirus Infections and Treatments"
            ],
            "concepts": [
                "Biology",
                "Microneme",
                "CRISPR",
                "Gene",
                "Genome",
                "Toxoplasma gondii",
                "Genetics",
                "Genetic screen",
                "Plasmodium falciparum",
                "Apicomplexa",
                "Malaria",
                "Phenotype",
                "Antibody",
                "Immunology"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
        "doi": "10.48550/arxiv.2304.01373",
        "abstract": "Code at https://github.com/EleutherAI/pythia",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "transformer model",
                "Large Language Models (LLMs)"
            ],
            "tasks": [
                "Causal language modeling"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Common Sense Reasoning",
                "Coreference Resolution",
                "Language Modelling",
                "Memorization",
                "Question Answering"
            ],
            "methods": [
                "Pythia"
            ],
            "main_collection_name": [
                "Language Models"
            ],
            "main_collection_area": [
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Suite",
                "Computer science",
                "Memorization",
                "Code (set theory)",
                "Training (meteorology)",
                "Test suite",
                "Scaling",
                "Scale (ratio)",
                "Training set",
                "Data science",
                "Artificial intelligence",
                "Machine learning",
                "Programming language",
                "Mathematics education",
                "Set (abstract data type)",
                "Psychology",
                "Regression analysis",
                "Geometry",
                "Archaeology",
                "Mathematics",
                "Test case",
                "Meteorology",
                "Physics",
                "Quantum mechanics",
                "History"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language",
                "004"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "UL2: Unifying Language Learning Paradigms",
        "doi": "10.48550/arxiv.2205.05131",
        "abstract": "Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized &amp; unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 &amp; GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B &amp; Flan-UL2 20B.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Large Language Models (LLMs)",
                "transformer model"
            ],
            "tasks": [
                "Mixture-of-Denoisers, which combines diverse pretraining paradigms together"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Arithmetic Reasoning",
                "Common Sense Reasoning",
                "Coreference Resolution",
                "In-Context Learning",
                "Information Retrieval",
                "Long-range modeling",
                "MMLU",
                "Multi-task Language Understanding",
                "Natural Language Inference",
                "Question Answering",
                "Retrieval",
                "Text Classification",
                "Text Generation",
                "Word Sense Disambiguation"
            ],
            "methods": [
                "Multi-Head Attention",
                "Inverse Square Root Schedule",
                "Layer Normalization",
                "Adam",
                "Linear Layer",
                "BPE",
                "Attention Dropout",
                "Residual Connection",
                "Cosine Annealing",
                "15 Ways to Contact How can i speak to someone at Delta Airlines",
                "SentencePiece",
                "Dropout",
                "GPT-3",
                "Fixed Factorized Attention",
                "GELU",
                "GLU",
                "Softmax",
                "Weight Decay",
                "Adafactor",
                "Dense Connections",
                "T5",
                "Linear Warmup With Cosine Annealing",
                "Attention",
                "UL2"
            ],
            "main_collection_name": [
                "Activation Functions",
                "Learning Rate Schedules",
                "Subword Segmentation",
                "Normalization",
                "Language Models",
                "Stochastic Optimization",
                "Output Functions",
                "Attention Modules",
                "Transformers",
                "Attention Patterns",
                "Feedforward Networks",
                "Skip Connections",
                "Attention Mechanisms",
                "Tokenizers",
                "Regularization"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Multimodal Machine Learning Applications"
            ],
            "concepts": [
                "Computer science",
                "Artificial intelligence",
                "Automatic summarization",
                "Context (archaeology)",
                "Perspective (graphical)",
                "Machine learning",
                "Class (philosophy)",
                "Scalability",
                "Language model",
                "Pareto principle",
                "Natural language processing",
                "Paleontology",
                "Operations management",
                "Database",
                "Economics",
                "Biology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "02 engineering and technology",
                "0105 earth and related environmental sciences",
                "FOS: Computer and information sciences",
                "01 natural sciences",
                "0202 electrical engineering, electronic engineering, information engineering",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Generative AI and the future of education: Ragnarök or reformation? A paradoxical perspective from management educators",
        "doi": "10.1016/j.ijme.2023.100790",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "generative artificial intelligence",
                "pedagogy"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Ethics and Social Impacts of AI"
            ],
            "topics": [
                "Ethics and Social Impacts of AI",
                "AI in Service Interactions",
                "Digital Transformation in Industry"
            ],
            "concepts": [
                "Perspective (graphical)",
                "Generative grammar",
                "Sociology",
                "Epistemology",
                "Environmental ethics",
                "Psychology",
                "Philosophy",
                "Linguistics",
                "Artificial intelligence",
                "Computer science"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
        "doi": "10.48550/arxiv.2307.09288",
        "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Large Language Models (LLMs)",
                "transformer model"
            ],
            "tasks": [
                "Self-supervized Learning"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "",
                "Arithmetic Reasoning",
                "Code Generation",
                "Math Word Problem Solving",
                "Multiple Choice Question Answering (MCQA)",
                "Multi-task Language Understanding",
                "Question Answering",
                "Sentence Completion"
            ],
            "methods": [
                "Dropout",
                "Entropy Regularization",
                "RMSNorm",
                "Grouped-query attention",
                "Label Smoothing",
                "SwiGLU",
                "AdamW",
                "Absolute Position Encodings",
                "PPO",
                "Dense Connections",
                "Feedforward Network",
                "Attention",
                "BPE",
                "Rotary Embeddings",
                "Transformer",
                "Softmax",
                "Residual Connection"
            ],
            "main_collection_name": [
                "Output Functions",
                "Subword Segmentation",
                "Normalization",
                "Policy Gradient Methods",
                "Activation Functions",
                "Stochastic Optimization",
                "Regularization",
                "Transformers",
                "Attention",
                "Skip Connections",
                "Attention Mechanisms",
                "Position Embeddings",
                "Feedforward Networks"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "Reinforcement Learning",
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Speech and dialogue systems"
            ],
            "concepts": [
                "Helpfulness",
                "Computer science",
                "Work (physics)",
                "Foundation (evidence)",
                "Scale (ratio)",
                "Competition (biology)",
                "Psychology",
                "Engineering",
                "Ecology",
                "Geography",
                "Archaeology",
                "Social psychology",
                "Biology",
                "Cartography",
                "Mechanical engineering"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computer Science - Computation and Language",
                "Computation and Language (cs.CL)",
                "Computer Science - Artificial Intelligence",
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Machine Learning",
                "Machine Learning (cs.LG)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models",
        "doi": "10.18653/v1/2023.conll-1.21",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Machine Learning in Healthcare",
                "Artificial Intelligence in Healthcare and Education"
            ],
            "concepts": [
                "Context (archaeology)",
                "Benchmark (surveying)",
                "Computer science",
                "Domain (mathematical analysis)",
                "Health care",
                "Transparency (behavior)",
                "Test (biology)",
                "SAFER",
                "Multinational corporation",
                "Readability",
                "Language model",
                "Modalities",
                "Data science",
                "Natural language processing",
                "Artificial intelligence",
                "Cognitive psychology",
                "Psychology",
                "Computer security",
                "Political science",
                "Sociology",
                "Mathematical analysis",
                "Paleontology",
                "Social science",
                "Mathematics",
                "Geodesy",
                "Law",
                "Biology",
                "Programming language",
                "Geography"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents",
        "doi": "10.18653/v1/2023.emnlp-main.689",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Text Readability and Simplification"
            ],
            "concepts": [
                "Situated",
                "Computer science",
                "Set (abstract data type)",
                "Quality (philosophy)",
                "Work (physics)",
                "Human–computer interaction",
                "Value (mathematics)",
                "Artificial intelligence",
                "Machine learning",
                "Engineering",
                "Programming language",
                "Mechanical engineering",
                "Philosophy",
                "Epistemology"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Can ChatGPT Pass High School Exams on English Language Comprehension?",
        "doi": "10.1007/s40593-023-00372-z",
        "abstract": "<jats:title>Abstract</jats:title><jats:p>Launched in late November 2022, ChatGPT, a large language model chatbot, has garnered considerable attention. However, ongoing questions remain regarding its capabilities. In this study, ChatGPT was used to complete national high school exams in the Netherlands on the topic of English reading comprehension. In late December 2022, we submitted the exam questions through the ChatGPT web interface (GPT-3.5). According to official norms, ChatGPT achieved a mean grade of 7.3 on the Dutch scale of 1 to 10—comparable to the mean grade of all students who took the exam in the Netherlands, 6.99. However, ChatGPT occasionally required re-prompting to arrive at an explicit answer; without these nudges, the overall grade was 6.5. In March 2023, API access was made available, and a new version of ChatGPT, GPT-4, was released. We submitted the same exams to the API, and GPT-4 achieved a score of 8.3 without a need for re-prompting. Additionally, employing a bootstrapping method that incorporated randomness through ChatGPT’s ‘temperature’ parameter proved effective in self-identifying potentially incorrect answers. Finally, a re-assessment conducted with the GPT-4 model updated as of June 2023 showed no substantial change in the overall score. The present findings highlight significant opportunities but also raise concerns about the impact of ChatGPT and similar large language models on educational assessment.</jats:p>",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploring Language Models in Education"
            ],
            "tasks": [
                "Exam Oriented"
            ]
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Artificial Intelligence in Healthcare and Education"
            ],
            "topics": [
                "Artificial Intelligence in Healthcare and Education",
                "Topic Modeling",
                "Text Readability and Simplification"
            ],
            "concepts": [
                "Computer science",
                "Mathematics education",
                "Reading comprehension",
                "Comprehension",
                "Reading (process)",
                "English language",
                "Psychology",
                "Medical education",
                "Medicine",
                "Linguistics",
                "Programming language",
                "Philosophy"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Large language model",
                "4. Education",
                "Educational assessment",
                "GPT-4",
                "Reading comprehension",
                "GPT-3.5"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
        "doi": "10.18653/v1/2023.emnlp-main.468",
        "abstract": "The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Large Language models in general science"
            ],
            "tasks": [
                "scientific problem solving benchmark to evaluate the problem solving abilities of Large-Language-Models"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Overall - Test"
            ],
            "methods": [
                "Transformer",
                "Linear Warmup With Cosine Annealing",
                "GPT-4",
                "Attention",
                "Label Smoothing",
                "Dense Connections",
                "Discriminative Fine-Tuning",
                "Residual Connection",
                "Position-Wise Feed-Forward Layer",
                "Attention Dropout",
                "Adam",
                "GELU",
                "Cosine Annealing",
                "Absolute Position Encodings",
                "Dropout",
                "Layer Normalization",
                "Linear Layer",
                "BPE",
                "GPT",
                "Softmax",
                "Multi-Head Attention",
                "Weight Decay"
            ],
            "main_collection_name": [
                "Skip Connections",
                "Activation Functions",
                "Subword Segmentation",
                "Position Embeddings",
                "Fine-Tuning",
                "Learning Rate Schedules",
                "Output Functions",
                "Stochastic Optimization",
                "Transformers",
                "Normalization",
                "Language Models",
                "Feedforward Networks",
                "Attention Mechanisms",
                "Regularization",
                "Attention Modules"
            ],
            "main_collection_area": [
                "General",
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Software Engineering Research"
            ],
            "concepts": [
                "Benchmark (surveying)",
                "Consistency (knowledge bases)",
                "Computer science",
                "Domain (mathematical analysis)",
                "Artificial intelligence",
                "Management science",
                "Mathematics",
                "Engineering",
                "Mathematical analysis",
                "Geodesy",
                "Geography"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Artificial Intelligence",
                "Computer Science - Computation and Language",
                "FOS: Computer and information sciences",
                "Artificial Intelligence (cs.AI)",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Do Large Language Models Understand Chemistry? A Conversation with ChatGPT",
        "doi": "10.1021/acs.jcim.3c00285",
        "abstract": "Large language models (LLMs) have promised a revolution in answering complex questions using the ChatGPT model. Its application in chemistry is still in its infancy. This viewpoint addresses the question of how well ChatGPT understands chemistry by posing five simple tasks in different subareas of chemistry.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Large Language models in general science"
            ],
            "tasks": [
                "how well LLM understands Chemistry by posing five simple tasks in different subareas of Chemistry"
            ]
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Machine Learning in Materials Science"
            ],
            "topics": [
                "Machine Learning in Materials Science",
                "Topic Modeling"
            ],
            "concepts": [
                "Conversation",
                "Simple (philosophy)",
                "Computer science",
                "Chemistry",
                "Linguistics",
                "Epistemology",
                "Philosophy"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Communication",
                "Language"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "A Survey on Evaluation of Large Language Models",
        "doi": "10.1145/3641289",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Artificial Intelligence in Healthcare and Education"
            ],
            "concepts": [
                "Popularity",
                "Computer science",
                "Psychology",
                "Social psychology"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "ChemCrow: Augmenting large-language models with chemistry tools",
        "doi": "10.48550/arxiv.2304.05376",
        "abstract": "<jats:title>Abstract</jats:title><jats:p>Large language models (LLMs) have shown strong performance in tasks across domains but struggle with chemistry-related problems. These models also lack access to external knowledge sources, limiting their usefulness in scientific applications. We introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery and materials design. By integrating 18 expert-designed tools and using GPT-4 as the LLM, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our agent autonomously planned and executed the syntheses of an insect repellent and three organocatalysts and guided the discovery of a novel chromophore. Our evaluation, including both LLM and expert assessments, demonstrates ChemCrow’s effectiveness in automating a diverse set of chemical tasks. Our work not only aids expert chemists and lowers barriers for non-experts but also fosters scientific advancement by bridging the gap between experimental and computational chemistry.</jats:p>",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Large Language models in general science"
            ],
            "tasks": [
                "a novel LLM-powered method for integrating computational tools in chemistry"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Computational chemistry",
                "Drug Discovery"
            ],
            "methods": [
                "Multi-Head Attention",
                "Attention",
                "Absolute Position Encodings",
                "Position-Wise Feed-Forward Layer",
                "Transformer",
                "Dense Connections",
                "Label Smoothing",
                "Linear Layer",
                "BPE",
                "Layer Normalization",
                "Dropout",
                "Adam",
                "Residual Connection",
                "Softmax"
            ],
            "main_collection_name": [
                "Subword Segmentation",
                "Normalization",
                "Output Functions",
                "Attention Modules",
                "Feedforward Networks",
                "Transformers",
                "Regularization",
                "Skip Connections",
                "Position Embeddings",
                "Attention Mechanisms",
                "Stochastic Optimization"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Machine Learning in Materials Science"
            ],
            "topics": [
                "Machine Learning in Materials Science"
            ],
            "concepts": [
                "Limiting",
                "Bridging (networking)",
                "Computer science",
                "Set (abstract data type)",
                "Drug discovery",
                "Chemistry",
                "Data science",
                "Nanotechnology",
                "Artificial intelligence",
                "Engineering",
                "Programming language",
                "Mechanical engineering",
                "Computer network",
                "Biochemistry",
                "Materials science"
            ]
        },
        "openaire categories": {
            "subjects": [
                "03 medical and health sciences",
                "Machine Learning (stat.ML)",
                "Chemical Physics (physics.chem-ph)",
                "FOS: Physical sciences",
                "0303 health sciences",
                "Article",
                "Statistics - Machine Learning",
                "0301 basic medicine",
                "FOS: Computer and information sciences",
                "Physics - Chemical Physics"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Assessment of chemistry knowledge in large language models that generate code",
        "doi": "10.1039/d2dd00087c",
        "abstract": "<jats:p>In this work, we investigate the question: do code-generating large language models know chemistry? Our results indicate, mostly yes.</jats:p>",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Machine Learning in Materials Science"
            ],
            "topics": [
                "Machine Learning in Materials Science",
                "Topic Modeling",
                "Chemical Synthesis and Analysis"
            ],
            "concepts": [
                "Computer science",
                "Code (set theory)",
                "Programming language",
                "Set (abstract data type)"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Chemistry"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
        "doi": "10.48550/arxiv.2306.08568",
        "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Large Language Models (LLMs)",
                "a>>aaaaaaaaaaatransformer model"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Code Generation",
                "HumanEval",
                "mbpp"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Software Engineering Research"
            ],
            "concepts": [
                "Code (set theory)",
                "Computer science",
                "Programming language",
                "Margin (machine learning)",
                "Set (abstract data type)",
                "Machine learning"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Computation and Language",
                "FOS: Computer and information sciences",
                "Computer Science - Artificial Intelligence",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "StarCoder: may the source be with you!",
        "doi": "10.48550/arxiv.2305.06161",
        "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Large Language Models (LLMs)",
                "transformer model"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "8k",
                "Code Generation",
                "HumanEval",
                "PII Redaction"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Software Engineering Research",
                "Machine Learning and Data Classification"
            ],
            "concepts": [
                "Python (programming language)",
                "Computer science",
                "Open source",
                "Programming language",
                "Source code",
                "Tracing",
                "Context (archaeology)",
                "License",
                "Inference",
                "MIT License",
                "World Wide Web",
                "Software engineering",
                "Artificial intelligence",
                "Software",
                "Operating system",
                "Biology",
                "Paleontology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Artificial Intelligence",
                "Computer Science - Software Engineering",
                "Computer Science - Programming Languages",
                "FOS: Computer and information sciences",
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language",
                "Programming Languages (cs.PL)",
                "Software Engineering (cs.SE)",
                "Artificial Intelligence (cs.AI)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
        "doi": "10.48550/arxiv.2304.12244",
        "abstract": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Large Language Models (LLMs)",
                "transformer model"
            ],
            "tasks": [
                "supervized open-domain complex instruction finetuning"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Instruction Following"
            ],
            "methods": [
                "Attention",
                "Dropout",
                "BPE",
                "Softmax",
                "Absolute Position Encodings",
                "Adam",
                "Multi-Head Attention",
                "Dense Connections",
                "Residual Connection",
                "Transformer",
                "Linear Layer",
                "GPT-4",
                "Test",
                "Layer Normalization",
                "Position-Wise Feed-Forward Layer",
                "Label Smoothing"
            ],
            "main_collection_name": [
                "Regularization",
                "Stochastic Optimization",
                "Attention Modules",
                "Position Embeddings",
                "Normalization",
                "Feedforward Networks",
                "Subword Segmentation",
                "Output Functions",
                "Active Learning",
                "Attention Mechanisms",
                "Skip Connections",
                "Language Models",
                "Transformers"
            ],
            "main_collection_area": [
                "General",
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Machine Learning and Data Classification"
            ],
            "concepts": [
                "Computer science",
                "Code (set theory)",
                "Set (abstract data type)",
                "Domain (mathematical analysis)",
                "Factor (programming language)",
                "Language model",
                "Programming language",
                "Test (biology)",
                "Human–computer interaction",
                "Artificial intelligence",
                "Paleontology",
                "Biology",
                "Mathematical analysis",
                "Mathematics"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computer Science - Computation and Language",
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Artificial Intelligence",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct",
        "doi": "10.48550/arxiv.2308.09583",
        "abstract": "Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. For more details refer to https://github.com/nlpxucan/WizardLM",
        "orkg categories": {
            "domains": [],
            "methods": [
                "Reinforcement Learning from Evol-Instruct Feedback (RLEIF)"
            ],
            "research problems": [
                "AI in mathematics"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Arithmetic Reasoning",
                "GSM8K",
                "Math",
                "Mathematical Reasoning",
                "Math Word Problem Solving"
            ],
            "methods": [
                "15 Ways to Contact How can i speak to someone at Delta Airlines",
                "Weight Decay",
                "Linear Layer",
                "Position-Wise Feed-Forward Layer",
                "Fixed Factorized Attention",
                "Linear Warmup With Cosine Annealing",
                "Softmax",
                "Transformer",
                "Attention Dropout",
                "GELU",
                "Layer Normalization",
                "Attention",
                "Cosine Annealing",
                "Dropout",
                "Absolute Position Encodings",
                "BPE",
                "Multi-Head Attention",
                "GPT-4",
                "Dense Connections",
                "Residual Connection",
                "GPT-3",
                "Label Smoothing",
                "Adam"
            ],
            "main_collection_name": [
                "Stochastic Optimization",
                "Attention Modules",
                "Normalization",
                "Output Functions",
                "Regularization",
                "Transformers",
                "Learning Rate Schedules",
                "Position Embeddings",
                "Activation Functions",
                "Subword Segmentation",
                "Skip Connections",
                "Attention Patterns",
                "Attention Mechanisms",
                "Language Models",
                "Feedforward Networks"
            ],
            "main_collection_area": [
                "General",
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Machine Learning and Data Classification"
            ],
            "concepts": [
                "Margin (machine learning)",
                "Computer science",
                "Open source",
                "Domain (mathematical analysis)",
                "Language of mathematics",
                "Artificial intelligence",
                "The Internet",
                "Mathematics education",
                "Programming language",
                "Mathematics",
                "World Wide Web",
                "Machine learning",
                "Software",
                "Mathematical analysis"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computation and Language (cs.CL)",
                "Computer Science - Computation and Language",
                "Computer Science - Artificial Intelligence",
                "Computer Science - Machine Learning",
                "Artificial Intelligence (cs.AI)",
                "Machine Learning (cs.LG)",
                "FOS: Computer and information sciences"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models",
        "doi": "10.48550/arxiv.2308.16149",
        "abstract": "Arabic-centric, foundation model, large-language model, LLM, generative model, instruction-tuned, Jais, Jais-chat",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Large Language Models (LLMs)",
                "transformer model"
            ],
            "tasks": [
                "Instruction Tuning",
                "Causal language modeling"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Decoder",
                "Safety Alignment"
            ],
            "methods": [
                "Linear Warmup With Cosine Annealing",
                "GELU",
                "Attention Dropout",
                "Dense Connections",
                "Dropout",
                "Adam",
                "Layer Normalization",
                "Residual Connection",
                "BPE",
                "Attention",
                "GPT-3",
                "15 Ways to Contact How can i speak to someone at Delta Airlines",
                "Multi-Head Attention",
                "Cosine Annealing",
                "Linear Layer",
                "Fixed Factorized Attention",
                "Softmax",
                "Weight Decay"
            ],
            "main_collection_name": [
                "Subword Segmentation",
                "Stochastic Optimization",
                "Feedforward Networks",
                "Attention Mechanisms",
                "Normalization",
                "Output Functions",
                "Transformers",
                "Attention Patterns",
                "Skip Connections",
                "Learning Rate Schedules",
                "Activation Functions",
                "Attention Modules",
                "Regularization"
            ],
            "main_collection_area": [
                "General",
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Machine Learning in Healthcare"
            ],
            "concepts": [
                "Computer science",
                "Foundation (evidence)",
                "Generative grammar",
                "Arabic",
                "Code (set theory)",
                "Margin (machine learning)",
                "Artificial intelligence",
                "Natural language processing",
                "Open source",
                "Programming language",
                "Linguistics",
                "Software",
                "Machine learning",
                "History",
                "Philosophy",
                "Archaeology",
                "Set (abstract data type)"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Machine Learning (cs.LG)",
                "Artificial Intelligence (cs.AI)",
                "F.2.2",
                "Computer Science - Computation and Language",
                "F.2.2; I.2.7",
                "FOS: Computer and information sciences",
                "I.2.7",
                "Computer Science - Artificial Intelligence",
                "Computer Science - Machine Learning",
                "68T50",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
        "doi": "10.48550/arxiv.2306.02707",
        "abstract": "Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMA's release policy to be published at https://aka.ms/orca-lm), a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Large Language Models (LLMs)",
                "transformer model"
            ],
            "tasks": [
                "Explanation tuning"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Imitation Learning",
                "Knowledge Distillation"
            ],
            "methods": [
                "Linear Layer",
                "Dense Connections",
                "Absolute Position Encodings",
                "Position-Wise Feed-Forward Layer",
                "Label Smoothing",
                "Multi-Head Attention",
                "Dropout",
                "BPE",
                "GPT-4",
                "Residual Connection",
                "Softmax",
                "Adam",
                "Attention",
                "Transformer",
                "Layer Normalization"
            ],
            "main_collection_name": [
                "Normalization",
                "Transformers",
                "Attention Mechanisms",
                "Feedforward Networks",
                "Output Functions",
                "Regularization",
                "Position Embeddings",
                "Attention Modules",
                "Stochastic Optimization",
                "Subword Segmentation",
                "Skip Connections",
                "Language Models"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Explainable Artificial Intelligence (XAI)"
            ],
            "topics": [
                "Explainable Artificial Intelligence (XAI)",
                "Machine Learning in Healthcare",
                "Topic Modeling"
            ],
            "concepts": [
                "Imitation",
                "Computer science",
                "Benchmark (surveying)",
                "Scale (ratio)",
                "Process (computing)",
                "Artificial intelligence",
                "Machine learning",
                "Psychology",
                "Social psychology",
                "Physics",
                "Geodesy",
                "Quantum mechanics",
                "Geography",
                "Operating system"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "FOS: Computer and information sciences",
                "Computer Science - Machine Learning",
                "Computation and Language (cs.CL)",
                "Machine Learning (cs.LG)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?",
        "doi": "10.48550/arxiv.2306.16636",
        "abstract": "We present the Chinese Elementary School Math Word Problems (CMATH) dataset, comprising 1.7k elementary school-level math word problems with detailed annotations, source from actual Chinese workbooks and exams. This dataset aims to provide a benchmark tool for assessing the following question: to what grade level of elementary school math do the abilities of popular large language models (LLMs) correspond? We evaluate a variety of popular LLMs, including both commercial and open-source options, and discover that only GPT-4 achieves success (accuracy $\\geq$ 60\\%) across all six elementary school grades, while other models falter at different grade levels. Furthermore, we assess the robustness of several top-performing LLMs by augmenting the original problems in the CMATH dataset with distracting information. Our findings reveal that GPT-4 is able to maintains robustness, while other model fail. We anticipate that our study will expose limitations in LLMs' arithmetic and reasoning capabilities, and promote their ongoing development and advancement.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploring Language Models in Mathematics"
            ],
            "tasks": [
                "assess the robustness of several top-performing LLMs by augmenting the original problems in the CMATH dataset with distracting information"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Language Modeling",
                "Language Modelling",
                "Math",
                "Math Word Problem Solving"
            ],
            "methods": [
                "Absolute Position Encodings",
                "Softmax",
                "Layer Normalization",
                "Residual Connection",
                "Adam",
                "BPE",
                "GPT-4",
                "Dropout",
                "Transformer",
                "Dense Connections",
                "Label Smoothing",
                "fail",
                "Linear Layer",
                "Attention",
                "Multi-Head Attention",
                "Position-Wise Feed-Forward Layer"
            ],
            "main_collection_name": [
                "Normalization",
                "Position Embeddings",
                "Regularization",
                "Subword Segmentation",
                "Language Models",
                "Attention Modules",
                "Stochastic Optimization",
                "Skip Connections",
                "Attention Mechanisms",
                "Feedforward Networks",
                "Output Functions",
                "Transformers"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Text Readability and Simplification"
            ],
            "topics": [
                "Text Readability and Simplification",
                "Mathematics, Computing, and Information Processing",
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Robustness (evolution)",
                "Mathematics education",
                "Benchmark (surveying)",
                "Open source",
                "Elementary mathematics",
                "Computer science",
                "Psychology",
                "Geography",
                "Programming language",
                "Cartography",
                "Biochemistry",
                "Chemistry",
                "Software",
                "Gene"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Artificial Intelligence (cs.AI)",
                "4. Education",
                "Computer Science - Machine Learning",
                "Computer Science - Computation and Language",
                "Machine Learning (cs.LG)",
                "Computation and Language (cs.CL)",
                "Computer Science - Artificial Intelligence",
                "FOS: Computer and information sciences"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Benchmarking Arabic AI with Large Language Models",
        "doi": "10.48550/arxiv.2305.14982",
        "abstract": "Foundation Models, Large Language Models, Arabic NLP, Arabic Speech, Arabic AI, GPT3.5 Evaluation, USM Evaluation, Whisper Evaluation, GPT-4, BLOOMZ, Jais13b",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Text Readability and Simplification"
            ],
            "concepts": [
                "Benchmarking",
                "Computer science",
                "Natural language processing",
                "Artificial intelligence",
                "Arabic",
                "Language model",
                "Spoken language",
                "Speech recognition",
                "Linguistics",
                "Philosophy",
                "Marketing",
                "Business"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Artificial Intelligence",
                "Perplexity",
                "Grok",
                "Cancer enquiries",
                "Human-Computer Interaction (cs.HC)",
                "Co-Pilot",
                "ChatGPT",
                "Large Language Models",
                "Humans",
                "Health enquiries",
                "Language",
                "68T50",
                "Artificial Intelligence (cs.AI)",
                "F.2.2; I.2.7",
                "MetaAI",
                "I.2.7",
                "Internet",
                "Generative Artificial Intelligence",
                "Computer Science - Human-Computer Interaction",
                "Large language model",
                "Consumer Health Information",
                "Artificial intelligence",
                "Computer Science - Computer Vision and Pattern Recognition",
                "FOS: Computer and information sciences",
                "Gemini",
                "Computer Science - Computation and Language",
                "Computer Science - Artificial Intelligence",
                "Computation and Language (cs.CL)",
                "Multilingualism",
                "Neoplasms",
                "Claude",
                "Computer Vision and Pattern Recognition (cs.CV)",
                "F.2.2"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "MEGA: Multilingual Evaluation of Generative AI",
        "doi": "10.18653/v1/2023.emnlp-main.258",
        "abstract": "Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploring Large Language Models for Multilingual Tasks"
            ],
            "tasks": [
                "benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks",
                "Sequence-labelling with BLSTM",
                "Summarization",
                "Question Answering",
                "commonsense reasoning",
                "Natural Language Inference"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Benchmarking"
            ],
            "methods": [
                "Dropout",
                "Attention",
                "Transformer",
                "BPE",
                "Multi-Head Attention",
                "Residual Connection",
                "Softmax",
                "Absolute Position Encodings",
                "Linear Layer",
                "Adam",
                "Label Smoothing",
                "GPT-4",
                "Position-Wise Feed-Forward Layer",
                "Dense Connections",
                "Layer Normalization"
            ],
            "main_collection_name": [
                "Skip Connections",
                "Regularization",
                "Position Embeddings",
                "Transformers",
                "Output Functions",
                "Attention Mechanisms",
                "Feedforward Networks",
                "Attention Modules",
                "Stochastic Optimization",
                "Language Models",
                "Subword Segmentation",
                "Normalization"
            ],
            "main_collection_area": [
                "General",
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Natural Language Processing Techniques"
            ],
            "topics": [
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Mega-",
                "Computer science",
                "Generative grammar",
                "Artificial intelligence",
                "Natural language processing",
                "Physics",
                "Astronomy"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "FOS: Computer and information sciences",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning",
        "doi": "10.18653/v1/2023.findings-emnlp.878",
        "abstract": "Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field. ChatGPT represents one of the most exciting LLM systems developed recently to showcase impressive skills for language generation and highly attract public attention. Among various exciting applications discovered for ChatGPT in English, the model can process and generate texts for multiple languages due to its multilingual training data. Given the broad adoption of ChatGPT for English in different problems and areas, a natural question is whether ChatGPT can also be applied effectively for other languages or it is necessary to develop more language-specific technologies. The answer to this question requires a thorough evaluation of ChatGPT over multiple tasks with diverse languages and large datasets (i.e., beyond reported anecdotes), which is still missing or limited in current research. Our work aims to fill this gap for the evaluation of ChatGPT and similar LLMs to provide more comprehensive information for multilingual NLP applications. While this work will be an ongoing effort to include additional experiments in the future, our current paper evaluates ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium, low, and extremely low resources. We also focus on the zero-shot learning setting for ChatGPT to improve reproducibility and better simulate the interactions of general users. Compared to the performance of previous models, our extensive experimental results demonstrate a worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploring Large Language Models for Multilingual Tasks"
            ],
            "tasks": [
                "Summarization",
                "Common Sense Reasoning (CSR)",
                "Question answering (QA)",
                "Natural Language Inference (NLI)",
                "Relation Classification",
                "Part of speech (POS) tagging",
                "Named Entity Recognition (NER)",
                "to evaluate the performance of ChatGPT and LLMs for NLP tasks in different languages"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Multilingual NLP",
                "Text Generation",
                "Zero-Shot Learning"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Artificial Intelligence in Healthcare and Education",
                "Text Readability and Simplification"
            ],
            "concepts": [
                "Computer science",
                "Artificial intelligence",
                "Natural language processing",
                "Field (mathematics)",
                "Process (computing)",
                "Natural language",
                "Data science",
                "Mathematics",
                "Pure mathematics",
                "Operating system"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Artificial Intelligence (cs.AI)",
                "Computation and Language (cs.CL)",
                "Computer Science - Artificial Intelligence",
                "FOS: Computer and information sciences",
                "Computer Science - Computation and Language"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",
        "doi": "10.48550/arxiv.2306.05179",
        "abstract": "Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different levels. In total, M3Exam contains 12,317 questions in 9 diverse languages with three educational levels, where about 23\\% of the questions require processing images for successful solving. We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with complex multimodal questions. We believe that M3Exam can be a valuable resource for comprehensively evaluating LLMs by examining their multilingual and multimodal abilities and tracking their development. Data and evaluation code is available at \\url{https://github.com/DAMO-NLP-SG/M3Exam}.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploring Evaluation Benchmarks for Large Language Models"
            ],
            "tasks": [
                "Specific downstream task",
                "proposed a novel benchmark dataset for evaluating LLMs by offering a multilingual, multimodal, and multi-level assessment"
            ]
        },
        "papers with code categories": {
            "tasks": [],
            "methods": [
                "Layer Normalization",
                "Residual Connection",
                "Position-Wise Feed-Forward Layer",
                "BPE",
                "Softmax",
                "Label Smoothing",
                "Transformer",
                "Dropout",
                "Adam",
                "Dense Connections",
                "GPT-4",
                "Absolute Position Encodings",
                "Linear Layer",
                "Attention",
                "Multi-Head Attention"
            ],
            "main_collection_name": [
                "Feedforward Networks",
                "Attention Mechanisms",
                "Stochastic Optimization",
                "Transformers",
                "Language Models",
                "Regularization",
                "Skip Connections",
                "Output Functions",
                "Position Embeddings",
                "Attention Modules",
                "Normalization",
                "Subword Segmentation"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Text Readability and Simplification"
            ],
            "concepts": [
                "Benchmark (surveying)",
                "Computer science",
                "Context (archaeology)",
                "Multimodality",
                "Multilingualism",
                "Natural language processing",
                "Artificial intelligence",
                "Psychology",
                "World Wide Web",
                "Geography",
                "Pedagogy",
                "Archaeology",
                "Geodesy"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computer Science - Computer Vision and Pattern Recognition",
                "Computation and Language (cs.CL)",
                "Computer Vision and Pattern Recognition (cs.CV)",
                "4. Education",
                "Computer Science - Computation and Language"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Measuring Massive Multitask Chinese Understanding",
        "doi": "10.48550/arxiv.2304.12986",
        "abstract": "The development of large-scale Chinese language models is flourishing, yet there is a lack of corresponding capability assessments. Therefore, we propose a test to measure the multitask accuracy of large Chinese language models. This test encompasses four major domains, including medicine, law, psychology, and education, with 15 subtasks in medicine and 8 subtasks in education. We found that the best-performing models in the zero-shot setting outperformed the worst-performing models by nearly 18.6 percentage points on average. Across the four major domains, the highest average zero-shot accuracy of all models is 0.512. In the subdomains, only the GPT-3.5-turbo model achieved a zero-shot accuracy of 0.693 in clinical medicine, which was the highest accuracy among all models across all subtasks. All models performed poorly in the legal domain, with the highest zero-shot accuracy reaching only 0.239. By comprehensively evaluating the breadth and depth of knowledge across multiple disciplines, this test can more accurately identify the shortcomings of the models.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploring Large Language Models for Multilingual Tasks"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "All"
            ],
            "methods": [
                "Attention",
                "BPE",
                "Test",
                "Adam",
                "GPT-3",
                "Linear Layer",
                "Weight Decay",
                "Fixed Factorized Attention",
                "Attention Dropout",
                "15 Ways to Contact How can i speak to someone at Delta Airlines",
                "GELU",
                "Softmax",
                "Residual Connection",
                "Dropout",
                "Multi-Head Attention",
                "Linear Warmup With Cosine Annealing",
                "Cosine Annealing",
                "Dense Connections",
                "Layer Normalization"
            ],
            "main_collection_name": [
                "Active Learning",
                "Attention Mechanisms",
                "Transformers",
                "Feedforward Networks",
                "Skip Connections",
                "Attention Modules",
                "Activation Functions",
                "Learning Rate Schedules",
                "Output Functions",
                "Normalization",
                "Attention Patterns",
                "Stochastic Optimization",
                "Regularization",
                "Subword Segmentation"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Machine Learning in Healthcare"
            ],
            "topics": [
                "Machine Learning in Healthcare",
                "Radiomics and Machine Learning in Medical Imaging",
                "Explainable Artificial Intelligence (XAI)"
            ],
            "concepts": [
                "Flourishing",
                "Computer science",
                "Zero (linguistics)",
                "Artificial intelligence",
                "Test (biology)",
                "Machine learning",
                "Shot (pellet)",
                "Scale (ratio)",
                "Natural language processing",
                "Psychology",
                "Linguistics",
                "Geography",
                "Social psychology",
                "Paleontology",
                "Philosophy",
                "Chemistry",
                "Organic chemistry",
                "Biology",
                "Cartography"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "Computation and Language (cs.CL)",
                "Computer Science - Artificial Intelligence",
                "Artificial Intelligence (cs.AI)",
                "3. Good health",
                "I.2.7",
                "FOS: Computer and information sciences"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Evaluating language models for mathematics through interactions",
        "doi": "10.1073/pnas.2318124121",
        "abstract": "<jats:p>There is much excitement about the opportunity to harness the power of large language models (LLMs) when building problem-solving assistants. However, the standard methodology of evaluating LLMs relies on static pairs of inputs and outputs; this is insufficient for making an informed decision about which LLMs are best to use in an interactive setting, and how that varies by setting. Static assessment therefore limits how we understand language model capabilities. We introduce CheckMate, an adaptable prototype platform for humans to interact with and evaluate LLMs. We conduct a study with CheckMate to evaluate three language models (InstructGPT, ChatGPT, and GPT-4) as assistants in proving undergraduate-level mathematics, with a mixed cohort of participants from undergraduate students to professors of mathematics. We release the resulting interaction and rating dataset, MathConverse. By analyzing MathConverse, we derive a taxonomy of human query behaviors and uncover that despite a generally positive correlation, there are notable instances of divergence between correctness and perceived helpfulness in LLM generations, among other findings. Further, we garner a more granular understanding of GPT-4 mathematical problem-solving through a series of case studies, contributed by experienced mathematicians. We conclude with actionable takeaways for ML practitioners and mathematicians: models that communicate uncertainty, respond well to user corrections, and can provide a concise rationale for their recommendations, may constitute better assistants. Humans should inspect LLM output carefully given their current shortcomings and potential for surprising fallibility.</jats:p>",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploring Language Models in Mathematics"
            ],
            "tasks": [
                "interactive and dynamic evaluation of LLMs in mathematics"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Language Modelling",
                "Mathematical Problem-Solving",
                "Mathematical Reasoning",
                "Navigate"
            ],
            "methods": [
                "Softmax",
                "BPE",
                "Adam",
                "Position-Wise Feed-Forward Layer",
                "Multi-Head Attention",
                "Linear Layer",
                "Dense Connections",
                "Label Smoothing",
                "Residual Connection",
                "Absolute Position Encodings",
                "Attention",
                "Dropout",
                "Transformer",
                "Layer Normalization",
                "GPT-4",
                "AWARE"
            ],
            "main_collection_name": [
                "Attention Modules",
                "Skip Connections",
                "Feedforward Networks",
                "Stochastic Optimization",
                "Position Embeddings",
                "Graph Representation Learning",
                "Output Functions",
                "Attention Mechanisms",
                "Subword Segmentation",
                "Language Models",
                "Transformers",
                "Regularization",
                "Normalization"
            ],
            "main_collection_area": [
                "General",
                "Graphs",
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Explainable Artificial Intelligence (XAI)",
                "Software Engineering Research"
            ],
            "concepts": [
                "Helpfulness",
                "Correctness",
                "Readability",
                "Computer science",
                "Divergence (linguistics)",
                "Management science",
                "Mathematics education",
                "Psychology",
                "Social psychology",
                "Linguistics",
                "Programming language",
                "Engineering",
                "Philosophy"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Public policy",
                "Crimen",
                "Mathematics",
                "Computation and Language (cs.CL)",
                "AI",
                "Administración de proyectos culturales",
                "Language",
                "Computer Science - Computation and Language",
                "Human-Computer Interaction (cs.HC)",
                "FOS: Computer and information sciences",
                "collaborative",
                "Política cultural-Impacto social",
                "Educación y crimen",
                "Educación",
                "Computer Science - Human-Computer Interaction",
                "Bienes públicos",
                "Education",
                "Crime",
                "Problem Solving",
                "Public goods",
                "Política pública-Aspectos sociales",
                "Humans",
                "Machine Learning (cs.LG)",
                "language models",
                "Physical Sciences",
                "bilingual",
                "FOS: Mathematics",
                "practices",
                "cooperative learning classrooms",
                "Students",
                "human–computer interaction",
                "Administración pública",
                "Computer Science - Machine Learning",
                "theorem proving",
                "Prevención del delito",
                "Política pública"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and Problem Solving: Evidence from the Vietnamese National High School Graduation Examination",
        "doi": "10.48550/arxiv.2306.06331",
        "abstract": "This study offers a complete analysis of ChatGPT's mathematics abilities in responding to multiple-choice questions for the Vietnamese National High School Graduation Examination (VNHSGE) on a range of subjects and difficulty levels. The dataset included 250 questions divided into four levels: knowledge (K), comprehension (C), application (A), and high application (H), and it included ten themes that covered diverse mathematical concepts. The outcomes demonstrate that ChatGPT's performance varies depending on the difficulty level and subject. It performed best on questions at Level (K), with an accuracy rate of $83\\%$; but, as the difficulty level rose, it scored poorly, with an accuracy rate of $10\\%$. The study has also shown that ChatGPT significantly succeeds in providing responses to questions on subjects including exponential and logarithmic functions, geometric progression, and arithmetic progression. The study found that ChatGPT had difficulty correctly answering questions on topics including derivatives and applications, spatial geometry, and Oxyz spatial calculus. Additionally, this study contrasted ChatGPT outcomes with Vietnamese students in VNHSGE and in other math competitions. ChatGPT dominated in the SAT Math competition with a success rate of $70\\%$, followed by VNHSGE mathematics ($58.8\\%)$. However, its success rates were lower on other exams, such as AP Statistics, the GRE Quantitative, AMC 10, AMC 12, and AP Calculus BC. These results suggest that ChatGPT has the potential to be an effective teaching tool for mathematics, but more work is needed to enhance its handling of graphical data and address the challenges presented by questions that are getting more challenging.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Math",
                "Mathematical Reasoning",
                "Multiple-choice"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Online Learning and Analytics"
            ],
            "topics": [
                "Online Learning and Analytics"
            ],
            "concepts": [
                "Graduation (instrument)",
                "Vietnamese",
                "Mathematics education",
                "Comprehension",
                "Subject (documents)",
                "Mathematics",
                "Calculus (dental)",
                "Computer science",
                "Medicine",
                "Geometry",
                "Linguistics",
                "Philosophy",
                "Dentistry",
                "Library science",
                "Programming language"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "An Empirical Study on Challenging Math Problem Solving with GPT-4",
        "doi": "10.48550/arxiv.2306.01337",
        "abstract": "",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Intelligent Tutoring Systems and Adaptive Learning",
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Frontier",
                "Work (physics)",
                "Mathematics education",
                "Competition (biology)",
                "Computer science",
                "Management science",
                "Mathematics",
                "Ecology",
                "Engineering",
                "Political science",
                "Biology",
                "Mechanical engineering",
                "Law"
            ]
        },
        "openaire categories": {
            "subjects": []
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "How well do Large Language Models perform in Arithmetic tasks?",
        "doi": "10.48550/arxiv.2304.02015",
        "abstract": "Large language models have emerged abilities including chain-of-thought to answer math word problems step by step. Solving math word problems not only requires abilities to disassemble problems via chain-of-thought but also needs to calculate arithmetic expressions correctly for each step. To the best of our knowledge, there is no work to focus on evaluating the arithmetic ability of large language models. In this work, we propose an arithmetic dataset MATH 401 to test the latest large language models including GPT-4, ChatGPT, InstrctGPT, Galactica, and LLaMA with various arithmetic expressions and provide a detailed analysis of the ability of large language models. MATH 401 and evaluation codes are released at \\url{https://github.com/GanjinZero/math401-llm}.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploring Language Models in Mathematics"
            ],
            "tasks": [
                "propose an arithmetic dataset MATH 401 to test latest large language models"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Math"
            ],
            "methods": [
                "BPE",
                "Softmax",
                "Test",
                "Galactica",
                "Multi-Head Attention",
                "Dense Connections",
                "Attention",
                "Layer Normalization",
                "Adam",
                "Linear Layer",
                "Label Smoothing",
                "Absolute Position Encodings",
                "Dropout",
                "GPT-4",
                "Transformer",
                "Position-Wise Feed-Forward Layer",
                "Residual Connection"
            ],
            "main_collection_name": [
                "Feedforward Networks",
                "Subword Segmentation",
                "Output Functions",
                "Normalization",
                "Transformers",
                "Active Learning",
                "Language Models",
                "Skip Connections",
                "Attention Mechanisms",
                "Position Embeddings",
                "Regularization",
                "Attention Modules",
                "Stochastic Optimization"
            ],
            "main_collection_area": [
                "Natural Language Processing",
                "General"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling"
            ],
            "concepts": [
                "Arithmetic",
                "Computer science",
                "Focus (optics)",
                "Word (group theory)",
                "Natural language processing",
                "Theoretical computer science",
                "Linguistics",
                "Mathematics",
                "Physics",
                "Optics",
                "Philosophy"
            ]
        },
        "openaire categories": {
            "subjects": [
                "language localization",
                "FOS: Computer and information sciences",
                "Computer Simulation",
                "03 medical and health sciences",
                "Computer Science - Computation and Language",
                "0302 clinical medicine",
                "Reproducibility of Results",
                "Humans",
                "Pharmacology, Clinical",
                "Nonlinear Dynamics",
                "Computation and Language (cs.CL)",
                "0501 psychology and cognitive sciences",
                "Computer Science - Artificial Intelligence",
                "executive function",
                "Software",
                "Artificial Intelligence (cs.AI)",
                "Psychology",
                "Pharmacokinetics",
                "functional architecture",
                "language localizer",
                "Bayes Theorem",
                "05 social sciences",
                "Models, Biological",
                "modularity",
                "2. Zero hunger"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data",
        "doi": "10.18653/v1/2023.emnlp-main.574",
        "abstract": "In this paper, we study how to improve the zero-shot reasoning ability of large language models~(LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an \\emph{Iterative Reading-then-Reasoning~(IRR)} approach for solving question answering tasks based on structured data, called \\textbf{StructGPT}. In our approach, we construct the specialized function to collect relevant evidence from structured data (\\ie \\emph{reading}), and let LLMs concentrate the reasoning task based on the collected information (\\ie \\emph{reasoning}). Specially, we propose an \\emph{invoking-linearization-generation} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at~\\url{https://github.com/RUCAIBox/StructGPT}.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Development And Evaluation of a SOTA LLM-based NL-To-SQL translation interface"
            ],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [
                "Language Modeling",
                "Language Modelling",
                "Large Language Model",
                "Question Answering"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Text and Document Classification Technologies"
            ],
            "concepts": [
                "Construct (python library)",
                "Computer science",
                "Task (project management)",
                "Case-based reasoning",
                "Reading (process)",
                "Deductive reasoning",
                "Natural language processing",
                "Artificial intelligence",
                "Programming language",
                "Linguistics",
                "Philosophy",
                "Management",
                "Economics"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computer Science - Computation and Language",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "MMBench: Is Your Multi-modal Model an All-Around Player?",
        "doi": "10.1007/978-3-031-72658-3_13",
        "abstract": "Accepted in ECCV2024 as Oral Presentation",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploring Evaluation Benchmarks for Large Language Models"
            ],
            "tasks": [
                "General language task",
                "develops a comprehensive evaluation pipeline, primarily comprised of two elements",
                "Multimodal LLMs"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "All",
                "Instruction Following",
                "Multiple-choice",
                "Visual Question Answering"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Multimodal Machine Learning Applications"
            ],
            "topics": [
                "Multimodal Machine Learning Applications",
                "Natural Language Processing Techniques",
                "Topic Modeling"
            ],
            "concepts": [
                "Computer science",
                "Modal",
                "Composite material",
                "Materials science"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computation and Language (cs.CL)",
                "Computer Science - Computer Vision and Pattern Recognition",
                "Computer Science - Computation and Language",
                "FOS: Computer and information sciences",
                "Computer Vision and Pattern Recognition (cs.CV)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark",
        "doi": "10.18653/v1/2023.emnlp-main.699",
        "abstract": "Large language models (LLMs) have been shown to perform well at a variety of syntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed in many forms including conversational agents that interact with humans, we lack a grounded benchmark to measure how well LLMs understand \\textit{social} language. Here, we introduce a new theory-driven benchmark, SocKET, that contains 58 NLP tasks testing social knowledge which we group into five categories: humor &amp; sarcasm, offensiveness, sentiment &amp; emotion, and trustworthiness. In tests on the benchmark, we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks, which were predicted from theory. Through zero-shot evaluations, we show that pretrained models already possess some innate but limited capabilities of social language understanding and training on one category of tasks can improve zero-shot testing on others. Our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware LLMs. The associated resources are released at https://github.com/minjechoi/SOCKET.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploring Evaluation Benchmarks for Large Language Models"
            ],
            "tasks": [
                "Specific downstream task",
                "Social knowledge",
                "introduce a new easyto-use benchmark, SOCKET, that systematically organizes 58 tasks"
            ]
        },
        "papers with code categories": {
            "tasks": [],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Hate Speech and Cyberbullying Detection"
            ],
            "topics": [
                "Hate Speech and Cyberbullying Detection",
                "Topic Modeling",
                "Natural Language Processing Techniques"
            ],
            "concepts": [
                "Benchmark (surveying)",
                "Sarcasm",
                "Task (project management)",
                "Computer science",
                "Natural language processing",
                "Dimension (graph theory)",
                "Artificial intelligence",
                "Cognitive psychology",
                "Psychology",
                "Linguistics",
                "Irony",
                "Geodesy",
                "Geography",
                "Philosophy",
                "Mathematics",
                "Management",
                "Pure mathematics",
                "Economics"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computation and Language (cs.CL)",
                "FOS: Computer and information sciences",
                "Computer Science - Artificial Intelligence",
                "Computer Science - Computation and Language",
                "Artificial Intelligence (cs.AI)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models",
        "doi": "10.48550/arxiv.2306.13394",
        "abstract": "Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.",
        "orkg categories": {
            "domains": [],
            "methods": [
                "Text Translation",
                "Numerical Calculation",
                "Optical Character Recognition",
                "Fine-Grained Recognition",
                "Coarse-Grained Recognition"
            ],
            "research problems": [
                "Exploring Evaluation Benchmarks for Large Language Models"
            ],
            "tasks": [
                "General language task",
                "Multimodal LLMs",
                "measures both perception and cognition abilities on a total of 14 subtasks"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Benchmarking",
                "Language Modeling",
                "Language Modelling",
                "Large Language Model",
                "MME",
                "Model Optimization",
                "Multimodal Large Language Model",
                "Prompt Engineering"
            ],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Multimodal Machine Learning Applications"
            ],
            "concepts": [
                "Computer science",
                "Benchmark (surveying)",
                "Artificial intelligence",
                "Perception",
                "Selection (genetic algorithm)",
                "Natural language processing",
                "Machine learning",
                "Geography",
                "Geodesy",
                "Neuroscience",
                "Biology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computer Vision and Pattern Recognition",
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Computation and Language",
                "FOS: Computer and information sciences",
                "Computer Science - Artificial Intelligence",
                "Computation and Language (cs.CL)",
                "Computer Vision and Pattern Recognition (cs.CV)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation",
        "doi": "10.1609/aaai.v38i16.29767",
        "abstract": "<jats:p>New Natural Langauge Process~(NLP) benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present Xiezhi, the most comprehensive evaluation suite designed to assess holistic domain knowledge.Xiezhi comprises multiple-choice questions across 516 diverse disciplines ranging from 13 different subjects with 249,587 questions and accompanied by Xiezhi-Specialty with 14,041 questions and Xiezhi-Interdiscipline with 10,746 questions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results indicate that LLMs exceed average performance of humans in science, engineering, agronomy, medicine, and art, but fall short in economics, jurisprudence, pedagogy, literature, history, and management. All the evaluation code and data are open sourced in https://github.com/MikeGu721/XiezhiBenchmark</jats:p>",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploring Evaluation Benchmarks for Large Language Models"
            ],
            "tasks": [
                "General language task",
                "Comprehensive domain knowledge",
                "designed to assess holistic domain knowledge"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Jurisprudence",
                "Management",
                "Multiple-choice"
            ],
            "methods": [
                "ALIGN"
            ],
            "main_collection_name": [
                "Vision and Language Pre-Trained Models"
            ],
            "main_collection_area": [
                "Computer Vision"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Semantic Web and Ontologies"
            ],
            "topics": [
                "Semantic Web and Ontologies"
            ],
            "concepts": [
                "Benchmark (surveying)",
                "Domain (mathematical analysis)",
                "Computer science",
                "Domain knowledge",
                "Knowledge management",
                "Artificial intelligence",
                "Mathematics",
                "Geography",
                "Cartography",
                "Mathematical analysis"
            ]
        },
        "openaire categories": {
            "subjects": [
                "FOS: Computer and information sciences",
                "Computer Science - Computation and Language",
                "Computation and Language (cs.CL)"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models",
        "doi": "10.48550/arxiv.2306.11507",
        "abstract": "Large Language Models (LLMs) such as ChatGPT, have gained significant attention due to their impressive natural language processing capabilities. It is crucial to prioritize human-centered principles when utilizing these models. Safeguarding the ethical and moral compliance of LLMs is of utmost importance. However, individual ethical issues have not been well studied on the latest LLMs. Therefore, this study aims to address these gaps by introducing a new benchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs in three crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPT examines toxicity in language models by employing toxic prompt templates derived from social norms. It then quantifies the extent of bias in models by measuring quantifiable toxicity values across different groups. Lastly, TrustGPT assesses the value of conversation generation models from both active value-alignment and passive value-alignment tasks. Through the implementation of TrustGPT, this research aims to enhance our understanding of the performance of conversation generation models and promote the development of language models that are more ethical and socially responsible.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploring Evaluation Benchmarks for Large Language Models"
            ],
            "tasks": [
                "Specific downstream task",
                "TRUSTGPT assesses the ethical dimensions of eight latest LLMs from three perspectives: toxicity, bias, and value-alignment"
            ]
        },
        "papers with code categories": {
            "tasks": [],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Artificial Intelligence in Healthcare and Education"
            ],
            "topics": [
                "Artificial Intelligence in Healthcare and Education",
                "Topic Modeling"
            ],
            "concepts": [
                "Conversation",
                "Safeguarding",
                "Value (mathematics)",
                "Benchmark (surveying)",
                "Trustworthiness",
                "Computer science",
                "Psychology",
                "Computer security",
                "Medicine",
                "Geography",
                "Nursing",
                "Communication",
                "Geodesy",
                "Machine learning"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "Computation and Language (cs.CL)",
                "FOS: Computer and information sciences",
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Artificial Intelligence"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Can Large Language Models Understand Real-World Complex Instructions?",
        "doi": "10.1609/aaai.v38i16.29777",
        "abstract": "Large language models (LLMs) can understand human instructions, showing their potential for pragmatic applications beyond traditional NLP tasks. However, they still struggle with complex instructions, which can be either complex task descriptions that require multiple tasks and constraints, or complex input that contains long context, noise, heterogeneous information and multi-turn format. Due to these features, LLMs often ignore semantic constraints from task descriptions, generate incorrect formats, violate length or sample count constraints, and be unfaithful to the input text. Existing benchmarks are insufficient to assess LLMs' ability to understand complex instructions, as they are close-ended and simple. To bridge this gap, we propose CELLO, a benchmark for evaluating LLMs' ability to follow complex instructions systematically. We design eight features for complex instructions and construct a comprehensive evaluation dataset from real-world scenarios. We also establish four criteria and develop corresponding metrics, as current ones are inadequate, biased or too strict and coarse-grained. We compare the performance of representative Chinese-oriented and English-oriented models in following complex instructions through extensive experiments. Resources of CELLO are publicly available at https://github.com/Abbey4799/CELLO.",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [
                "Exploring Evaluation Benchmarks for Large Language Models"
            ],
            "tasks": [
                "Specific downstream task",
                "evaluating LLMs’ ability to follow complex instructions systematically"
            ]
        },
        "papers with code categories": {
            "tasks": [],
            "methods": [],
            "main_collection_name": [],
            "main_collection_area": []
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Multimodal Machine Learning Applications"
            ],
            "concepts": [
                "Computer science",
                "Natural language processing",
                "Programming language",
                "Linguistics",
                "Philosophy"
            ]
        },
        "openaire categories": {
            "subjects": [
                "cognitive modeling",
                "incremental learning",
                "Robotics (cs.RO)",
                "Electronic computers. Computer science",
                "Information Retrieval (cs.IR)",
                "humanoid robots",
                "resource-constrained environments",
                "Robotics and AI",
                "sentiment analysis",
                "TK1-9971",
                "Computer Vision and Pattern Recognition (cs.CV)",
                "Computer Science - Computation and Language",
                "knowledge representation for robots",
                "Computation and Language (cs.CL)",
                "Computer Science - Computer Vision and Pattern Recognition",
                "human–robot interaction",
                "large language models",
                "Software Engineering (cs.SE)",
                "data privacy",
                "Mechanical engineering and machinery",
                "Machine Learning (cs.LG)",
                "Artificial Intelligence (cs.AI)",
                "Computer Science - Machine Learning",
                "FOS: Computer and information sciences",
                "Computer Science - Artificial Intelligence",
                "TJ1-1570",
                "Computer Science - Software Engineering",
                "Electrical engineering. Electronics. Nuclear engineering",
                "Small large language model",
                "QA75.5-76.95",
                "aspect-based sentiment analysis",
                "natural language processing",
                "Computer Science - Information Retrieval",
                "Computer Science - Robotics"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
        "doi": "10.48550/arxiv.2305.08322",
        "abstract": "New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users.",
        "orkg categories": {
            "domains": [],
            "methods": [
                "Five shot",
                "Zero shot"
            ],
            "research problems": [
                "Exploring Evaluation Benchmarks for Large Language Models"
            ],
            "tasks": [
                "General language task",
                "designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context",
                "Chinese evaluation"
            ]
        },
        "papers with code categories": {
            "tasks": [
                "Multiple-choice"
            ],
            "methods": [
                "Linear Layer",
                "BPE",
                "Softmax",
                "ALIGN",
                "Dense Connections",
                "Position-Wise Feed-Forward Layer",
                "GPT-4",
                "Multi-Head Attention",
                "Label Smoothing",
                "Dropout",
                "Residual Connection",
                "Attention",
                "Layer Normalization",
                "Adam",
                "Absolute Position Encodings",
                "Transformer"
            ],
            "main_collection_name": [
                "Regularization",
                "Stochastic Optimization",
                "Attention Mechanisms",
                "Attention Modules",
                "Transformers",
                "Normalization",
                "Position Embeddings",
                "Feedforward Networks",
                "Skip Connections",
                "Subword Segmentation",
                "Language Models",
                "Output Functions",
                "Vision and Language Pre-Trained Models"
            ],
            "main_collection_area": [
                "General",
                "Natural Language Processing",
                "Computer Vision"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Natural Language Processing Techniques",
                "Text Readability and Simplification"
            ],
            "concepts": [
                "Suite",
                "Context (archaeology)",
                "Foundation (evidence)",
                "Computer science",
                "Mathematics education",
                "Artificial intelligence",
                "Psychology",
                "Geography",
                "Archaeology"
            ]
        },
        "openaire categories": {
            "subjects": [
                "4. Education",
                "Computation and Language (cs.CL)",
                "FOS: Computer and information sciences",
                "Computer Science - Computation and Language"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    },
    {
        "title": "Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench",
        "doi": "10.48550/arxiv.2308.03656",
        "abstract": "NeurIPS 2024; 10 pages of main text; 14 pages of appendices",
        "orkg categories": {
            "domains": [],
            "methods": [],
            "research problems": [],
            "tasks": []
        },
        "papers with code categories": {
            "tasks": [],
            "methods": [
                "Adam",
                "Residual Connection",
                "Multi-Head Attention",
                "Linear Layer",
                "Dropout",
                "Position-Wise Feed-Forward Layer",
                "Dense Connections",
                "Softmax",
                "BPE",
                "GPT-4",
                "Label Smoothing",
                "Absolute Position Encodings",
                "Attention",
                "Transformer",
                "Layer Normalization"
            ],
            "main_collection_name": [
                "Feedforward Networks",
                "Transformers",
                "Stochastic Optimization",
                "Regularization",
                "Subword Segmentation",
                "Position Embeddings",
                "Skip Connections",
                "Language Models",
                "Attention Mechanisms",
                "Normalization",
                "Attention Modules",
                "Output Functions"
            ],
            "main_collection_area": [
                "General",
                "Natural Language Processing"
            ]
        },
        "openalex categories": {
            "primary topics": [
                "Topic Modeling"
            ],
            "topics": [
                "Topic Modeling",
                "Machine Learning in Healthcare"
            ],
            "concepts": [
                "Feeling",
                "Empathy",
                "Psychology",
                "Social psychology",
                "Code (set theory)",
                "Cognitive psychology",
                "Computer science",
                "Set (abstract data type)",
                "Programming language"
            ]
        },
        "openaire categories": {
            "subjects": [
                "Computer Science - Computation and Language",
                "Computation and Language (cs.CL)",
                "FOS: Computer and information sciences"
            ]
        },
        "crossref categories": {
            "subjects": []
        }
    }
]