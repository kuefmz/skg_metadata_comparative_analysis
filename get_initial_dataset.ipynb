{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "328ff3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbad9046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'MiniCPM: Unveiling the Potential of Small Language Models with Scalable\\n  Training Strategies',\n",
       " 'doi': '10.48550/arxiv.2404.06395',\n",
       " 'abstract': \"The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .\",\n",
       " 'orkg categories': {'domains': ['Generic'],\n",
       "  'methods': [],\n",
       "  'research problems': ['Small Language Models (SLMs) survey'],\n",
       "  'tasks': []},\n",
       " 'papers with code categories': {'tasks': ['Domain Adaptation'],\n",
       "  'methods': ['Chinchilla'],\n",
       "  'main_collection_name': ['Language Models'],\n",
       "  'main_collection_area': ['Natural Language Processing']},\n",
       " 'openalex categories': {'primary topics': ['Topic Modeling'],\n",
       "  'topics': ['Topic Modeling', 'Natural Language Processing Techniques'],\n",
       "  'concepts': ['Training (meteorology)',\n",
       "   'Scalability',\n",
       "   'Computer science',\n",
       "   'Natural language processing',\n",
       "   'Artificial intelligence',\n",
       "   'Geography',\n",
       "   'Database',\n",
       "   'Meteorology']},\n",
       " 'openaire categories': {'subjects': ['FOS: Computer and information sciences',\n",
       "   'Computer Science - Machine Learning',\n",
       "   'Machine Learning (cs.LG)',\n",
       "   'Computation and Language (cs.CL)',\n",
       "   'Computer Science - Computation and Language']}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Preview the first entry\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f7c03fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_fields = [\n",
    "    \"orkg categories\",\n",
    "    \"papers with code categories\",\n",
    "    \"openalex categories\",\n",
    "    \"openaire categories\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07482997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1f56ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid papers: 70\n"
     ]
    }
   ],
   "source": [
    "count_all_present = 0\n",
    "valid_papers = []\n",
    "\n",
    "for paper in data:\n",
    "    present_fields = 0\n",
    "    for field in required_fields:\n",
    "        if any(len(paper[field].get(sub, [])) > 0 for sub in paper[field]):\n",
    "            present_fields += 1\n",
    "    if present_fields == len(required_fields):\n",
    "        count_all_present += 1\n",
    "        valid_papers.append(paper)\n",
    "\n",
    "print(f\"Valid papers: {count_all_present}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c403f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_fields = {\n",
    "    \"orkg categories\": [\"domains\", \"methods\", \"research problems\", \"tasks\"],\n",
    "    \"papers with code categories\": [\"tasks\", \"methods\", \"main_collection_name\", \"main_collection_area\"],\n",
    "    \"openalex categories\": [\"primary topics\", \"topics\", \"concepts\"],\n",
    "    \"openaire categories\": [\"subjects\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ddeb6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in valid_papers:\n",
    "    for field, subfields in category_fields.items():\n",
    "        flat_list = []\n",
    "        if field in entry:\n",
    "            for sub in subfields:\n",
    "                cleaned = [s.lower() for s in entry[field].get(sub, []) if isinstance(s, str)]\n",
    "                entry[field][sub] = cleaned\n",
    "                flat_list.extend(cleaned)\n",
    "        entry[f\"{field.replace(' ', '_')}_flat\"] = list(set(flat_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "895108c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'MiniCPM: Unveiling the Potential of Small Language Models with Scalable\\n  Training Strategies',\n",
       " 'doi': '10.48550/arxiv.2404.06395',\n",
       " 'abstract': \"The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .\",\n",
       " 'orkg categories': {'domains': ['generic'],\n",
       "  'methods': [],\n",
       "  'research problems': ['small language models (slms) survey'],\n",
       "  'tasks': []},\n",
       " 'papers with code categories': {'tasks': ['domain adaptation'],\n",
       "  'methods': ['chinchilla'],\n",
       "  'main_collection_name': ['language models'],\n",
       "  'main_collection_area': ['natural language processing']},\n",
       " 'openalex categories': {'primary topics': ['topic modeling'],\n",
       "  'topics': ['topic modeling', 'natural language processing techniques'],\n",
       "  'concepts': ['training (meteorology)',\n",
       "   'scalability',\n",
       "   'computer science',\n",
       "   'natural language processing',\n",
       "   'artificial intelligence',\n",
       "   'geography',\n",
       "   'database',\n",
       "   'meteorology']},\n",
       " 'openaire categories': {'subjects': ['fos: computer and information sciences',\n",
       "   'computer science - machine learning',\n",
       "   'machine learning (cs.lg)',\n",
       "   'computation and language (cs.cl)',\n",
       "   'computer science - computation and language']},\n",
       " 'orkg_categories_flat': ['generic', 'small language models (slms) survey'],\n",
       " 'papers_with_code_categories_flat': ['chinchilla',\n",
       "  'natural language processing',\n",
       "  'domain adaptation',\n",
       "  'language models'],\n",
       " 'openalex_categories_flat': ['geography',\n",
       "  'computer science',\n",
       "  'scalability',\n",
       "  'database',\n",
       "  'natural language processing',\n",
       "  'artificial intelligence',\n",
       "  'natural language processing techniques',\n",
       "  'meteorology',\n",
       "  'training (meteorology)',\n",
       "  'topic modeling'],\n",
       " 'openaire_categories_flat': ['fos: computer and information sciences',\n",
       "  'computer science - computation and language',\n",
       "  'machine learning (cs.lg)',\n",
       "  'computation and language (cs.cl)',\n",
       "  'computer science - machine learning']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a459a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_fields(entry):\n",
    "    return {\n",
    "        \"title\": entry.get(\"title\", \"\"),\n",
    "        \"doi\": entry.get(\"doi\", \"\"),\n",
    "        \"abstract\": entry.get(\"abstract\", \"\"),\n",
    "        \"orkg_categories_flat\": sorted(entry.get(\"orkg_categories_flat\", [])),\n",
    "        \"papers_with_code_categories_flat\": sorted(entry.get(\"papers_with_code_categories_flat\", [])),\n",
    "        \"openalex_categories_flat\": sorted(entry.get(\"openalex_categories_flat\", [])),\n",
    "        \"openaire_categories_flat\": sorted(entry.get(\"openaire_categories_flat\", [])),\n",
    "    }\n",
    "\n",
    "initial_data = [extract_clean_fields(p) for p in valid_papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c535fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'MiniCPM: Unveiling the Potential of Small Language Models with Scalable\\n  Training Strategies',\n",
       " 'doi': '10.48550/arxiv.2404.06395',\n",
       " 'abstract': \"The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .\",\n",
       " 'orkg_categories_flat': ['generic', 'small language models (slms) survey'],\n",
       " 'papers_with_code_categories_flat': ['chinchilla',\n",
       "  'domain adaptation',\n",
       "  'language models',\n",
       "  'natural language processing'],\n",
       " 'openalex_categories_flat': ['artificial intelligence',\n",
       "  'computer science',\n",
       "  'database',\n",
       "  'geography',\n",
       "  'meteorology',\n",
       "  'natural language processing',\n",
       "  'natural language processing techniques',\n",
       "  'scalability',\n",
       "  'topic modeling',\n",
       "  'training (meteorology)'],\n",
       " 'openaire_categories_flat': ['computation and language (cs.cl)',\n",
       "  'computer science - computation and language',\n",
       "  'computer science - machine learning',\n",
       "  'fos: computer and information sciences',\n",
       "  'machine learning (cs.lg)']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3522d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_arxiv_taxonomy():\n",
    "    url = \"https://arxiv.org/category_taxonomy\"\n",
    "    response = requests.get(url)\n",
    "    matches = re.findall(r'<span class=\"descriptor\">([^<]+)</span>\\s+\\(([^)]+)\\)', response.text)\n",
    "    return {code.lower(): desc.lower() for desc, code in matches}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ffbe903",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_map = fetch_arxiv_taxonomy()\n",
    "\n",
    "acm_ccs_map = {\n",
    "    \"i.4\": \"image processing and computer vision\",\n",
    "    \"h.2.8\": \"database applications\",\n",
    "    \"h.2\": \"database management\",\n",
    "    \"k.4\": \"computers and society\",\n",
    "    \"d.2\": \"software engineering\",\n",
    "    \"c.2\": \"computer-communication networks\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62a81a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_openaire_categories(categories, arxiv_map, acm_map):\n",
    "    decoded_labels = set()\n",
    "    cleaned = set()\n",
    "\n",
    "    for cat in categories:\n",
    "        cat = cat.strip().lower()\n",
    "        if cat.startswith(\"fos:\"):\n",
    "            cat = cat[4:]\n",
    "\n",
    "        for match in re.findall(r\"\\(([\\w\\.\\-]+)\\)\", cat):\n",
    "            if match in arxiv_map:\n",
    "                decoded_labels.add(arxiv_map[match])\n",
    "            elif match in acm_map:\n",
    "                decoded_labels.add(acm_map[match])\n",
    "        cat = re.sub(r\"\\([\\w\\.\\-]+\\)\", \"\", cat).strip()\n",
    "\n",
    "        if cat in arxiv_map:\n",
    "            decoded_labels.add(arxiv_map[cat])\n",
    "            continue\n",
    "        if cat in acm_map:\n",
    "            decoded_labels.add(acm_map[cat])\n",
    "            continue\n",
    "        if not re.fullmatch(r\"[a-z]\\.\\d+(\\.\\d+)?\", cat):\n",
    "            cleaned.add(cat)\n",
    "\n",
    "    return sorted(cleaned.union(decoded_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a841f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in initial_data:\n",
    "    entry[\"openaire_categories_flat\"] = clean_openaire_categories(\n",
    "        entry.get(\"openaire_categories_flat\", []),\n",
    "        arxiv_map,\n",
    "        acm_ccs_map\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7359c116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'OLMo: Accelerating the Science of Language Models',\n",
       " 'doi': '10.18653/v1/2024.acl-long.841',\n",
       " 'abstract': 'Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.',\n",
       " 'orkg_categories_flat': ['generic', 'small language models (slms) survey'],\n",
       " 'papers_with_code_categories_flat': ['language modeling',\n",
       "  'language modelling'],\n",
       " 'openalex_categories_flat': ['computer science', 'topic modeling'],\n",
       " 'openaire_categories_flat': ['computation and language',\n",
       "  'computer and information sciences',\n",
       "  'computer science - computation and language']}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1761f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"initial_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(initial_data, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
